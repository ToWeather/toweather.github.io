<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[凸优化基础：拉格朗日乘数法、对偶与 KKT 条件]]></title>
    <url>%2F%E5%87%B8%E4%BC%98%E5%8C%96%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E3%80%81%E5%AF%B9%E5%81%B6%E4%B8%8E-KKT-%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[数学优化概述所谓数学优化，就是在给定约束的情况下对某个目标函数求最小值或最大值。信号处理领域和通信领域的很多问题，最终的求解形式都可以归结为数学优化问题。 凸优化问题是数学优化问题的一个子类，该问题的目标函数是凸函数，且可行域是一个凸集。对于很多数学优化问题，科学界并没有一套通用且可行的解法，但对于解决凸优化问题，其理论和方法都已经很成熟了。因而现在对于非凸问题的解法通常都是先将原问题松弛至一个凸问题，然后再求解该凸问题。 本文先讲述用于求解局部最优化问题的拉格朗日乘数法（Lagrange Multipliers）以及局部最优解需满足的 KKT 条件；然后再讲述对偶（duality）—— 一种将原优化问题简化的方法；最后再对凸优化问题、对偶、KKT 条件三者之间的关系做一个简单的讲解。 在讲述前先对数学优化问题给出一个正式且通用的定义： 定义 1： 函数 $f,g_1,\cdots,g_m$ 以及 $h_1,\cdots,h_l$ 均被定义在自变量空间 $\Omega = {\Bbb R}^n $ 上，则一般的优化问题具有如下形式 \min_{ {\bf x} \in \Omega} f({\bf x}) \quad {\text {s.t.} } \quad g_j({\bf x}) \leq 0 \ \ \forall j \ \ {\text {and} } \ \ h_i({\bf x}) = 0 \ \ \forall i \tag{1} 后面都会遵循这个定义及对应的符号。比如后面我们都会假定优化问题为求目标函数在可行域内的最小值（或极小值），并且假定不等式约束的方向是“约束函数小于等于0”。这样的假设是合理的，因为任何一个其他优化问题都可以转换为上述形式。 局部最优化问题及其解法局部最优即是函数的极小值（或极大值）。对于无约束条件的目标函数（假设其连续可导），极值处的点 ${\bf x}^{\ast}$ 必然满足导数或梯度为 ${\bf 0}$ 的性质： \nabla_{\bf x} f({\bf x}^{\ast}) = {\bf 0} \tag{2}但满足式（2）的点并不一定是极值点，还有可能是鞍点（saddle point）。为此，我们进一步考虑目标函数的二阶特性，若在满足式（2）的地方同时满足如下二阶正定的特性（式（3）），则该点必然是目标函数的一个极小值点。 {\bf v}^t \nabla^2 f({\bf x}^{\ast}) {\bf v} > {\bf 0},\ \forall {\bf v} \in {\Bbb R}^{n} \backslash \lbrace {\bf 0} \rbrace \tag{3}其中 $ \nabla^2 f({\bf x}) $ 为 Hessian 矩阵，体现的是目标函数 $f({\bf x})$ 的二阶特性。在某一点处的 Hessian 矩阵是正定矩阵说明在该点的邻域内目标函数是一个凸函数，这也符合直觉。 这样，式（2）和式（3）构成了无约束局部最小化问题的解的充要条件。类似地，式（2）和 “Hessian 矩阵为负定矩阵”则是无约束局部最大化问题的解的充要条件，后面除非特殊说明，我们不再讨论求最大值或极大值的问题。 等式约束优化问题我们通过一个例子来说明等式约束优化问题的解法。 问题 1: 求解等式约束优化问题 $\min_{ {\bf x} \in {\Bbb R}^2 } f({\bf x}) \ \ {\text {s.t.} } \ \ h({\bf x}) = 0$ ，其中 $ f({\bf x}) = x_1 + x_2 ,\ h({\bf x}) = x_{1}^2 + x_{2}^2 - 2 $。 我们可以看到该例中的自变量 $ {\bf x} $ 为一个二维向量，$ x_1 $ 和 $x_2$ 分别是 ${\bf x} $ 的两个分量，这样的设定也非常利于我们作可视化的展示。 我们可以在一个二维坐标系中画出目标函数 $ f({\bf x}) = x_1 + x_2$ 在给定函数值下的图像，变化函数值我们可以得出一组类似等高线的图像（如下图中蓝色曲线）；同时我们还可以画出由等式约束 $h({\bf x}) = 0 $ 所构成的可行域的图像（如下图中红色曲线）。 给定可行域上的某个点 ${\bf x}_{\text F}$，若我们想要向问题的最优解逼近（假设将自变量变化 $\delta {\bf x}$ ），须满足以下两个条件： h({\bf x}_{\text F} + \alpha \delta {\bf x}) = 0 \tag{4} f({\bf x}_{\text F} + \alpha \delta {\bf x}) \lt f({\bf x}_{\text F}) \tag{5}要想满足条件（4），$\delta {\bf x}$ 的方向必须与等式约束所构成的曲面 $h({\bf x}) = 0$ 的切向量平行，亦即与约束函数的梯度方向 $\nabla_{\bf x} h({\bf x}_{\text F}) $ 正交，如下图所示。 要想满足条件（5），则 $\delta {\bf x}$ 须有与目标函数的梯度反方向 $-\nabla_{\bf x} f({\bf x}_{\text F})$ 方向一致的分量，即须满足 \delta {\bf x} \cdot (-\nabla_{\bf x} f({\bf x}_{\text F})) \gt 0 \tag{6}如下图所示。 将以上两点结合起来看，我们很快就可以找到求解等式约束下的优化问题的最优解须满足的条件 \nabla_{\bf x} f({\bf x}_{\text F}) = \mu \nabla_{\bf x} h({\bf x}_{\text F}) \tag{7}其中 $\mu$ 为任意标量。在该场景下，由于 $\delta {\bf x}$ 需要正交于 $h({\bf x}_{\text F})$ ，即有 \delta {\bf x} \cdot (-\nabla_{\bf x} f({\bf x}_{\text F})) = -\delta {\bf x} \cdot \mu \nabla_{\bf x} h({\bf x}_{\text F}) = 0即在满足式（7）的 ${\bf x}_{\text F}$ 处，再也找不到一个变化自变量的方向，使得变化后的 $ {\bf x} $ 仍然在可行域内的同时使得目标函数减小，因而式（7）构成了求解等式约束问题的一个必要条件。 下图展示了问题 1 中满足式（7）的两个点（critical points）。 但仅仅满足式（7）并不能保证 ${\bf x}_{\text F}$ 是带等式约束的局部最小值问题的解，它同样有可能是局部最大值问题的解或者鞍点，因而我们仍然需要再加上二阶正定的条件。综上所述，式（7）和二阶正定特性一起构成了带等式约束的局部最小问题的解，我们通过如下定理对上面的结论做一个正式的阐述。 定理 1: 对于等式约束优化问题 \min_{ {\bf x} \in {\Bbb R}^n} f({\bf x}) \ \ \ \text {s.t.} \ \ \ h({\bf x}) = 0\tag{8}定义拉格朗日乘数 {\cal L}({\bf x}, \mu) = f({\bf x}) + \mu h({\bf x})\tag{9}则若 ${\bf x}^{\ast}$ 为该问题的解，则必存在 $\mu^{\ast}$，满足如下条件 \nabla_{\bf x} {\cal L}({\bf x}^{\ast}, \mu^{\ast}) = {\bf 0}\tag{10} \nabla_{\mu} {\cal L}({\bf x}^{\ast}, \mu^{\ast}) = 0\tag{11} {\bf y}^t(\nabla_{\bf x\bf x} {\cal L}({\bf x}^{\ast}, \mu^{\ast})){\bf y} \geq 0 \ \ \ \forall {\bf y} \ \ \text {s.t.} \ \ \nabla_{\bf x}h({\bf x}^{\ast})^t {\bf y} = 0\tag{12}反过来，若式（10）～（12）成立，则 ${\bf x}^{\ast}$ 必然为原优化问题的一个局部最小解。 即式（10）～（12）是该等式约束局部优化问题有解的充要条件。注意到 ${\cal L}({\bf x}^{\ast}, \mu^{\ast}) = f({\bf x}^{\ast})$ ，式（10）等价于式（7）；式（11）强调了最优解一定满足等式约束条件；式（12）即为二阶正定性条件。 不等式约束优化问题我们仍然通过一个例子来说明不等式约束优化问题的解法。 问题 2：求解不等式约束优化问题 $\min_{ {\bf x} \in {\Bbb R}^2 } f({\bf x}) \ \ {\text {s.t.} } \ \ g({\bf x}) \leq 0$ ，其中 $f({\bf x}) = x_1^2 + x_2^2, \ \ g({\bf x}) = x_1^2 + x_2^2 - 1$. 函数 $f({\bf x}) = x_1^2 + x_2^2$ 的等高线如下图所示。 不等式约束 $g({\bf x}) \leq 0$ 所产生的可行域如下图所示。 我们可以看到在这个例子中，无约束条件下优化问题的解 ${\bf x}^{\ast}$ 正好落在可行域内，即 $g({\bf x}^{\ast}) &lt; 0$，如下图所示。此时不等式约束优化问题的解即等价于无约束优化问题的解，即式（2）和式（3）是该问题的解的充要条件。 再考虑无约束条件下优化问题的解落在可行域之外的情形，考虑如下例子。 问题 3：求解不等式约束优化问题 $\min_{ {\bf x} \in {\Bbb R}^2 } f({\bf x}) \ \ {\text {s.t.} } \ \ g({\bf x}) \leq 0$ ，其中 $f({\bf x}) = (x_1 - 1.1)^2 + (x_2 - 1.1)^2$, $g({\bf x}) = x_1^2 + x_2^2 - 1$. 此时目标函数的等高线和可行域如下图所示，此时无约束优化问题的解落在可行域之外。我们可以推断，该情形下不等式约束优化问题的解必然在可行域的边界上，即此时问题退化为等式约束优化问题的求解。 回忆之前等式约束优化问题须满足的条件： -\nabla_{\bf x} f({\bf x}) = \lambda \nabla_{\bf x} g({\bf x}) \tag{13}即需要目标函数的梯度的反方向（目标函数下降最快方向）与约束函数的梯度方向平行。但考虑到此时可行域是由曲面所包裹的空间而不仅仅是曲面，目标函数的梯度的反方向需要与约束函数的梯度方向一致，即 $\lambda &gt; 0$，直观来理解就是在可行域的表面上，目标函数的下降方向与可行域的扩张方向一致，导致可行域无法再扩张（注意当前在可行域的表面），因而目标函数的值将降无可降。如下图所示。 因此，不等式约束优化问题的解法可以由以上两种情形所概括，用如下定理作一个总结，后面也会对该定理作进一步阐释。 定理 2： 对于不等式约束优化问题 \min_{ {\bf x} \in {\Bbb R}^n} f({\bf x}) \ \ \ \text {s.t.} \ \ \ g({\bf x}) \leq 0\tag{14}定义拉格朗日乘数 {\cal L}({\bf x}, \lambda) = f({\bf x}) + \lambda g({\bf x})\tag{15}则若 ${\bf x}^{\ast}$ 为该问题的解，则必存在 $\lambda^{\ast}$，满足如下条件 \nabla_{\bf x} {\cal L}({\bf x}^{\ast}, \lambda^{\ast}) = {\bf 0} \tag{16} \lambda^{\ast} \geq 0 \tag{17} \lambda^{\ast}g({\bf x}^{\ast}) = 0 \tag{18} g({\bf x}^{\ast}) \leq 0 \tag{19} {\bf y}^t \nabla_{\bf x \bf x} {\cal L}({\bf x}^{\ast}, \lambda^{\ast}){\bf y} \geq 0 \ \ \ \text {for certain} \ \ {\bf y} \tag{20}反过来，若式（16）～（20）成立，则 ${\bf x}^{\ast}$ 必然是原优化问题的一个局部最优解。 上面定理中式（20）为二阶正定性条件，当不等式约束没有起到实质作用时（即无约束优化问题的解在可行域内），${\bf y} \in {\Bbb R}^n \backslash \lbrace {\bf 0} \rbrace$；当不等式约束起到实质作用时，${\bf y}$ 为与 $\nabla_{\bf x} g({\bf x}^{\ast})$ 正交的任意向量。后面不再对拉格朗日乘数的二阶正定性约束条件进行说明。 我们可以看到，在不等式约束没有起到实质性作用时，由于 $g({\bf x}^{\ast}) &lt; 0$，由式（18）可知， $\lambda^{\ast}$ 取值为 0，式（16）退化为 $\nabla_{\bf x} f({\bf x}^{\ast}) = {\bf 0}$；在不等式约束起到实质作用时，$g({\bf x}^{\ast}) = 0$，则 $\lambda^{\ast} &gt; 0$，故 ${\cal L}({\bf x}^{\ast}, \lambda^{\ast}) = f({\bf x}^{\ast}) + \lambda^{\ast}g({\bf x}^{\ast})$，式（16）即为式（13）。 KKT 条件分别考虑了等式约束优化问题和不等式约束优化问题之后，我们可以将一般优化问题（1）的解的充要条件总结成如下定理。 定理 3： 对于一般优化问题（1），定义拉格朗日乘数为 {\cal L}({\bf x}, {\bf \mu}, {\bf \lambda}) = f({\bf x}) + {\bf \mu}^t {\bf h}({\bf x}) + {\bf \lambda}^t {\bf g} ({\bf x}) \tag{21}其中 ${\bf \mu} = (\mu_1, \cdots,\mu_l)$，${\bf h}({\bf x}) = (h_1({\bf x}),\cdots,h_l({\bf x}))$，${\bf \lambda} = (\lambda_1,\cdots,\lambda_m)$，${\bf g}({\bf x}) = (g_1({\bf x}), \cdots, g_m({\bf x}))$ 均为向量。 则优化问题（1）的局部最优解的充要条件为 \nabla_{\bf x} {\cal L}({\bf x}^{\ast}, {\bf \mu}^{\ast}, {\bf \lambda}^{\ast}) = {\bf 0} \tag{22} \lambda_j^{\ast} \geq 0 \ \ \ \text {for} \ \ j = 1,\cdots,m \tag{23} \lambda_j^{\ast} g_j({\bf x}^{\ast}) = 0 \ \ \ \text {for} \ \ j = 1, \cdots, m \tag{24} g_j({\bf x}^{\ast}) \leq 0 \ \ \ \text {for} \ \ j = 1, \cdots, m \tag{25} h_i({\bf x}^{\ast}) = 0 \ \ \ \text {for} \ \ i = 1, \cdots, l \tag{26} {\bf y}^t \nabla_{\bf x \bf x} {\cal L}({\bf x}^{\ast}, {\bf \mu}^{\ast}, {\bf \lambda}^{\ast}){\bf y} \geq 0 \ \ \ \text {for certain} \ \ {\bf y} \tag{27} 我们称式（22）～（27）为一般优化问题（1）的 Karush-Kuhn-Tucker（KKT） 条件。 拉格朗日对偶对偶问题形式仍然考虑一般优化问题（1），拉格朗日函数为式（21），定义 p({\bf x}) = \max_{ {\bf \mu}, {\bf \lambda}: \ \lambda_j \geq 0} {\cal L}({\bf x}, {\bf \mu}, {\bf \lambda}) \tag{28}很容易发现 p({\bf x}) = \begin{cases} f({\bf x}), & \text {当} \ {\bf x } \ \text {在可行域内} \\ +\infty, & 其他 \end{cases} \tag{29}因为当 ${\bf x}$ 在可行域内时，有 $g_j({\bf x}) \leq 0$，$h_i({\bf x}) = 0$，又要求 $\lambda_j \geq 0$，因此要对 ${\cal L}({\bf x}, {\bf \mu}, {\bf \lambda})$ 关于 $\bf \mu$ 和 $\bf \lambda$ 取最大值，只能将 $\lambda_j$ 都置为 0，因此此时拉格朗日函数的最大取值为 $f({\bf x})$；当 $\bf x$ 在可行域之外时，我们可以将项 $\sum_j \lambda_j g_j(\bf x)$ 和 $\sum_i \mu_ih_i({\bf x})$ 设置的任意大，因此此时拉格朗日函数的最大取值为 $+\infty$. 则一般优化问题（1）等价于以下问题： \min_{\bf x} p({\bf x}) = \min_{\bf x} \max_{ {\bf \mu}, {\bf \lambda}: \ \lambda_j \geq 0} {\cal L}({\bf x}, {\bf \mu}, {\bf \lambda}) \tag{30}我们称该问题或一般优化问题（1）为主问题（primal problem），记 $p^{\ast} = \min_{\bf x} p({\bf x})$ 为主问题的值。 定义对偶函数为 q({\bf \mu, \bf \lambda}) = \min_{\bf x} {\cal L}({\bf x}, {\bf \mu}, {\bf \lambda}) \tag{31}则对偶问题（dual problem）为 \max_{\mu, \lambda: \ \lambda_j \geq 0} q({\bf \mu}, {\bf \lambda}) = \max_{\mu, \lambda: \ \lambda_j \geq 0} \min_{\bf x} {\cal L}({\bf x}, {\bf \mu}, {\bf \lambda}) \tag{32}对偶问题（32）与主问题（30）相比交换了对不同变量的优化顺序。记 $q^{\ast} = \max_{\mu, \lambda: \lambda_j \geq 0} {\cal L}({\bf x}, \mu, \lambda)$ 为对偶问题的值。 对偶问题有一些比较好的性质，使其相对来说比较容易求解。不管原始优化问题是否为凸问题，对偶函数 $q(\mu, \lambda)$ 都是一个凹函数，因而对偶问题是一个凸问题，可以简单证明如下。 为表述方便，$\beta = (\mu_1, \cdots, \mu_l, \lambda_1, \cdots, \lambda_m)$，对偶函数的定义域为 $D_q = \lbrace \beta \ | \ q(\beta) &gt; -\infty \rbrace$，则对任意 ${\bf x} \in {\Bbb R}^n$， $\beta_a,\beta_b \in D_q$ 以及 $\alpha \in (0, 1)$，有 \begin{align} {\cal L}({\bf x}, \alpha\beta_a + (1 - \alpha)\beta_b) &= f({\bf x}) + (\alpha\mu_a + (1-\alpha)\mu_b)^t h({\bf x}) + (\alpha\lambda_a + (1 - \alpha)\lambda_b)^t g({\bf x}) \\ &= \alpha [f({\bf x}) + \mu_a^th({\bf x}) + \lambda_a^tg({\bf x})] + (1 - \alpha)[f({\bf x}) + \mu_b^th({\bf x}) + \lambda_b^tg({\bf x})] \\ &= \alpha {\cal L}({\bf x}, \beta_a) + (1 - \alpha) {\cal L} ({\bf x}, \beta_b) \end{align} \tag{33}对上式两边同时对 $\bf x$ 取最小值，有 \begin{align} \min_{\bf x}{\cal L}({\bf x}, \alpha\beta_a + (1 - \alpha)\beta_b) &= \min_{\bf x}[\alpha {\cal L}({\bf x}, \beta_a) + (1 - \alpha) {\cal L} ({\bf x}, \beta_b)] \\ &\geq \alpha \min_{\bf x} {\cal L}({\bf x}, \beta_a) + (1 - \alpha)\min_{\bf x}{\cal L}({\bf x}, \beta_b) \end{align} \tag{34}上式说明了对偶函数 $q(\mu, \lambda)$ 为凹函数。 引入对偶问题的目的就是想通过它来求解主问题，但二者并不一定等价，后面我们会讨论对偶问题与主问题之间的关系（弱对偶定理），以及满足什么条件的情况下二者等价（强对偶定理）。在此之前，我们先来看一下对偶问题的几何阐释，以便我们对问题本质认识得更加深刻。 对偶问题的几何阐释我们还是通过例子来阐释对偶问题。 问题 4：求解以下不等式约束优化问题 $\min_{ {\bf x} \in {\Bbb R}^2} f({\bf x}) \ \ \text {s.t.} \ \ g({\bf x}) \leq 0$，其中 $ f({\bf x}) = 0.4(x_1^2 + x_2^2)$，$g({\bf x}) = 2 - x_1 - x_2$. 函数 $f({\bf x})$ 的等高线和由不等式约束 $g({\bf x}) \leq 0$ 所决定的可行域如下图所示。 我们将该坐标系下的每个点 ${\bf x} \in {\Bbb R}^2$ 映射至空间 $(g({\bf x}), f({\bf x})) \in {\Bbb R}^2$，定义该映射产生的新的点集为 G = \lbrace (y, z) \ | \ y = g({\bf x}), z = f({\bf x}) \ \ \text {for} \ \ {\bf x} \in {\Bbb R}^2 \rbrace \tag{35}该映射产生的点集在坐标系 $y-z$ 下如下图所示 上图中蓝色部分为所有 ${\bf x} \in {\Bbb R}^2$ 按照 (35) 映射之后所构成的点集，可行域为 $y \leq 0$，如上图中红色部分所示。可以很容易地看到，问题 4 要求解的“在 $y \leq 0$ 的条件下 $z$ 最小”这个问题的解即为上图中的红色点 $(y^{\ast}, z^{\ast})$。 我们再来分析问题 4 的拉格朗日对偶问题，其对应的拉格朗日函数为 {\cal L}({\bf x}, \lambda) = f({\bf x}) + \lambda g({\bf x}) = z + \lambda y, \ \ \ \lambda \geq 0 \tag{36}我们考虑直线 $z + \lambda y = \alpha$ 的特点，该直线的斜率为 $-\lambda$，与 $z$ 轴的截距为 $\alpha$，如下图所示 固定 $\lambda$ 时（即固定直线的斜率），我们在 $G$ 内平移直线（即需要直线经过 $G$ 内至少一个点），此时我们根据 $G$ 内所有斜率为 $-\lambda$ 的直线与 $z$ 轴的最小截距来求得拉格朗日对偶函数 $ q(\lambda) = \min_{\bf x} {\cal L}({\bf x}, \lambda) = \min_{(y, z) \in G} (z + \lambda y) $ 的值，如下图所示 我们再来变化 $\lambda$，来求解对偶问题 $\max_{\lambda: \lambda \geq0} q(\lambda)$，在几何意义上，就是变化上述直线的斜率，找到使得以斜率来划分的可行直线族中的最小 $z$ 截距 $q(\lambda)$ 最大的 $\lambda^{\ast}$. 我们可以看到，在这个问题中，对偶问题的解与主问题的解一致（我们称该现象为强对偶），如下图所示。 弱对偶定理弱对偶定理给出了一般情形下，对偶问题与主问题的最优值之间的关系，如下所述。 定理 4：令 ${\bf x} \in {\Bbb R}^n$ 为一般优化问题（1）的主问题（30）的一个可行解； 令 $({\bf \mu}, {\bf \lambda})$ 为对偶问题（32）的一个可行解，即满足 $\lambda_j \geq {\bf 0}$，则有 p({\bf x}) \geq q({\bf \mu, \bf \lambda}) \tag{37}更进一步，令主问题的最优值为 $p^{\ast} = \min_{\bf x} p({\bf x})$，令对偶问题的最优值为 $q^{\ast} = \max_{\mu, \lambda: \lambda_j \geq 0} q(\mu, \lambda)$，有 p^{\ast} \geq q^{\ast} \tag{38} 证明过程非常简单，有 p({\bf x}) = \max_{\mu, \lambda: \lambda_j \geq 0} {\cal L}({\bf x}, \mu, \lambda) \geq {\cal L}({\bf x}, \mu, \lambda) \geq \min_{\bf x} {\cal L}({\bf x, \mu, \lambda}) = q({\mu, \lambda}) \tag{39}上式即证明了（37），再对（39）左边关于 $\bf x$ 求极小值，右边关于 $(\mu, \lambda)$ 求极大值，可证明（38）： p^{\ast} = \min_{\bf x} p({\bf x}) \geq \max_{\mu, \lambda: \lambda_j \geq 0} q(\mu, \lambda) = q^{\ast} \tag{40}弱对偶定理说明了对一般优化问题 （1），对偶问题的最优值 $p^{\ast}$ 是主问题的最优值 $q^{\ast}$ 的一个下限，我们称 $p^{\ast}$ 与 $q^{\ast}$ 之间的差异为对偶差（duality gap）。 举个对偶差的简单例子，考虑下图所示的一维优化问题 可以看到目标函数 $f(x)$ 是非凸的，不等式约束函数 $g(x) = -x - m$，按照式（35）的做法，将 $x \in {\Bbb R}$ 映射至 $z-y$ 平面中，如下图所示 可以看到这个问题中，主问题的最优值和对偶问题的最优值并不一致，两者之间存在对偶差。 那么在什么情形下，主问题的解与对偶问题的解一致呢？以下的强对偶定理给出回答。 强对偶定理 定理 5： 对于一般优化问题（1），若 $f({\bf x})$ 和 $g_j({\bf x}), \ j = 1,\cdots,m$ 均为凸函数，且 $h_i({\bf x}), \ i = 1,\cdots,l$ 均为仿射函数，且约束 $g_j({\bf x}) \leq 0$ 是严格可行的，则存在 ${\bf x}^{\ast}$，${\mu}^{\ast}$，${\lambda}^{\ast}$ ，使得 ${\bf x}^{\ast}$ 是主问题（30）的解，$(\mu^{\ast}, \lambda^{\ast})$ 是对偶问题（32）的解，有 p^{\ast} = q^{\ast} = {\cal L}({\bf x}^{\ast}, \mu^{\ast}, \lambda^{\ast}) \tag{41} 定理 5 说明了在一定的条件下（该条件为 Slater 条件) ），强对偶成立，我们可以通过解决对偶问题来解决主问题，对强对偶定理的证明详见 Slater 强对偶条件证明。另外，当 Slater 条件满足时，优化问题的最优解满足式（22）～（27）所列出的 KKT 条件。]]></content>
      <categories>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
        <tag>数学优化</tag>
        <tag>对偶</tag>
        <tag>拉格朗日乘数法</tag>
        <tag>KKT 条件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述经典监督学习算法的第二篇文章，其它相关文章请参见 经典监督学习算法系列文章。 逻辑回归逻辑回归（logistic regression）模型是一种非常简单、但又非常具有启发性的线性分类算法。所谓线性分类算法，是指经过训练产出特征空间中的超平面，并将超平面分隔出来的不同的子空间归于不同的类别的一类分类算法。逻辑回归模型在产出用于分类的超平面上又多了一层概率解释，其给出的并不只是简单的非此即彼的分类信息，而是某个样本属于某一类的置信度，这也是其在经典分类算法中具有代表性的原因。 我们先从生成模型的角度来推导逻辑回归模型。考虑二类分类问题，类别分别用 $ {\cal C}_1 $ 和 $ {\cal C}_2 $ 表示。假设输入的样本为 $ {\bf x} $，类别的先验概率为 $ p({\cal C}_k) $，指定类别下产生 $ {\bf x} $ 的概率为 $ p({\bf x} | {\cal C}_k) $。则我们可根据贝叶斯公式计算出类别 $ {\cal C}_1 $ 的后验概率为 \begin{aligned} p({\cal C}_1 | {\bf x}) &= \frac{p({\bf x} | {\cal C}_1) p({\cal C}_1)}{p({\bf x} | {\cal C}_1) p({\cal C}_1)) + p({\bf x} | {\cal C}_2) p({\cal C}_2)} \\ &= \frac{1}{1 + \exp(-a) } = \sigma(a) \end{aligned} \tag{1}其中 $ a $ 被定义为 a = \ln \frac{p({\bf x} | {\cal C}_1) p({\cal C}_1)} {p({\bf x} | {\cal C}_2) p({\cal C}_2)} \tag{2}它衡量的是给定样本 $ {\bf x} $ 下类 $ {\cal C}_1 $ 的后验概率相对于类 $ {\cal C}_2 $ 的后验概率的增益。另外，式（1）中的函数 $ \sigma(a) $ 被称为 sigmoid 函数，定义为 \sigma(a) = \frac {1} {1 + \exp(-a) } \tag{3}sigmoid 函数有具有以下非常好的计算性质： \sigma(-a) = 1 - \sigma(a) \tag{4} \sigma^{\prime}(a) = \sigma(a) (1 - \sigma(a)) \tag{5}sigmoid 函数的图像如下所示 从上图中可以看出，sigmoid 函数将自变量压缩在了 $ [0, 1] $ 区间，因而很适合作为产出概率的函数；另外，由于其非线性特性以及易于计算等特点，sigmoid 函数及其变种也经常被用于神经网络（neural networks）的激活层（activation layer）。 再回到我们求解类别 $ {\cal C}_1 $ 的后验概率的问题。由式（1）可知，问题可以转换为将 $ a $ 表征为含有参数的表达式，然后再在某个最优准则下求解这些参数。 当我们假设在给定类别下的样本 $ {\bf x } $ 的条件概率分布为具有相同协方差矩阵（这一假设很重要，正是由于假设不同类别的条件概率分布具有相同的协方差矩阵才使得不同高斯密度函数中的二次项能够抵消）的高斯分布时，$ a $ 正好是 $ {\bf x} $ 的线性函数。具体地，假设 p({\bf x} | {\cal C}_k) = \frac{1} {(2 \pi)^{D/2} } \frac {1} {|{\bf \Sigma}| ^ {1 / 2} } \exp \lbrace - \frac{1} {2} ({\bf x} - {\bf \mu}_k)^{\text T} {\bf \Sigma}^{-1} ({\bf x} - {\bf \mu}_k) \rbrace \tag{6}将上式代入式（2）中，可得 a = {\bf w}^{\text T} {\bf x} + w_0 \tag{7}其中 {\bf w} = {\bf \Sigma}^{-1} ({\bf \mu}_1 - {\bf \mu}_2) \tag{8} w_0 = -\frac{1} {2} {\bf \mu}_{1}^{\text T} {\bf \Sigma}^{-1} {\bf \mu}_1 + \frac{1} {2} {\bf \mu}_{2}^{\text T} {\bf \Sigma}^{-1} {\bf \mu}_2 + \ln {\frac {p({\cal C}_1)} {p({\cal C}_2)} } \tag{9}由于我们要求解的是各个类别的后验概率 $ p({\cal C}_k | {\bf x}) $，而它具有以下形式（以二分类模型中的类别 $ {\cal C}_1 $ 为例）： p({\cal C}_1 | {\bf x}) = \sigma({\bf w}^{\text T} {\bf x} + w_0) \tag{10}因此假设中的与高斯分布有关的参数都对我们来说都不重要，此后我们专注于求解 $ {\bf w} $ 和 $ w_0 $ 就行了。公式（10）即是逻辑回归模型的核心假设，而求解参数 $ {\bf w} $ 和 $ w_0 $ 则是逻辑回归模型的核心问题。 最大似然参数估计和线性回归问题一样，我们仍然会预先将样本 $ {\bf x} $ 做特征变换得到 $ \phi = \phi({\bf x}) $；另外为便于处理，我们还假设二类分类中的类别分别为 $ {\cal C}_1 = +1 $，$ {\cal C}_2 = -1 $。那么对于某对样本 $ ({\phi}_n, t_n) $，我们可以写出其在逻辑回归模型下关于参数 $ {\bf w} $ （这里的 $ {\bf w} $ 已经涵盖了偏置量 $ w_0 $ ）的似然函数 p(t_n|{\bf w}) = y_n = \sigma(t_n{\bf w}^{\text T} {\phi}_n) \tag{11}将上述似然函数求对数再取负，便可得到交叉熵（cross entropy） \begin{aligned} E_n({\bf w}) &= -\ln p(t_n|{\bf w}) \\ &= \ln(1 + \exp(-t_n a_n) \\ &= \ln(1 + \exp(-t_n {\bf w}^{\text T} \phi_n)) \end{aligned} \tag{12}式（12）也被称作 logistic 损失函数，它与 0-1 损失函数的对比如下图所示（其中自变量为估计量 $ a $ 与实际量 $ t $ 的乘积）。 0-1 损失函数是对于二分类问题最优的一个衡量准则，在该准则下，只要估计量与实际量符号相同就认为估计器估计正确，从而没有损失；反之则认为估计器估计失败，给出一个常量损失值。虽然 0-1 损失函数是最优的，但实际场景中一般都不采用该损失函数，一是因为它不可导，不方便计算；二是它对所有错误情形一视同仁，不利于算法学习到有用且详尽的关于样本集的信息。 而 logistic 损失函数则是对 0-1 损失函数的一个近似，它不仅希望估计量与实际量符号相同，它还希望估计量与实际量的乘积 $ ta $ 越大越好，这一点也是逻辑回归模型容易对线性可分（linear separable）的数据集造成严重的过拟合现象的主要原因：对于线性可分的数据集，采用最大似然方法求解参数的逻辑回归模型会产出一个分割不同类别的超平面 $ {\bf w}^{\text T} \phi = 0 $，且会将 $ {\bf w} $ 的幅度设为无穷大，从而使得训练集里面的每一个样本点所属类别的后验概率 $ p({\cal C}_k | {\bf x}) $ 都为 1；而这样的超平面不止一个，但最大似然方法没有机制来选取一个合理的超平面，最终产出的超平面完全取决于优化方法和初始值的选取。不过该过拟合问题仍然可以通过采用 Bayesian 的方法或正则化得到缓解。 给定数据集 $ \lbrace \phi_n, t_n \rbrace $, 其中 $ t_n \in \lbrace -1, +1 \rbrace $，$ \phi_n = \phi({\bf x}_n) $，$ n = 1, …, N $。可以写出该数据集关于 $ {\bf w} $ 的交叉熵（或损失函数）为 E({\bf w}) = -\sum_{n=1}^{N} \ln y_n = -\sum_{n=1}^{N} \ln \sigma(t_n{\bf w}^{\text T} {\phi}_n) \tag{13}下面讲述对参数 $ {\bf w} $ 的求解方法。 梯度下降法由于逻辑回归模型中的 sigmoid 函数的非线性特性，导致最大似然方法得不到关于参数 $ {\bf w} $ 的一个闭式解。梯度下降法可以提供对参数 $ {\bf w}$ 的一个简单的迭代求解过程。损失函数 $ E({\bf w}) $ 的一阶导数为 \nabla E({\bf w}) = \sum_{n = 1}^{N} (y_n - 1)t_n \phi_n \tag{14}上式用到了式（5）。梯度下降法会将 $ {\bf w} $ 朝着 $ E({\bf w}) $ 下降最快的方向逼近，其迭代公式为 {\bf w}^{({\text {new} })} = {\bf w}^{({\text {old} })} - \eta \nabla E({\bf w}) \tag{15}其中 $ \eta $ 为学习率（learning rate），是一个超参数。从式（14）与（15）中可以看出，每一个训练样本都会对 $ {\bf w}$ 的更新有所贡献，具体来说，某样本计算出的输出类别后验概率值 $ y $ 与 1 的差距越大，其贡献越大。 牛顿法牛顿法是一种比梯度下降法更高效、且更容易求得最优解的迭代方法。该方法用到了对于参数 $ {\bf w} $ 的二阶导数。其迭代公式（推导放在下一小节）为 {\bf w}^{({\text {new} })} = {\bf w}^{({\text {old} })} - {\bf H}^{-1} \nabla E({\bf w}) \tag{16}其中 $ {\bf H} $ 为 Hessian 矩阵，其元素由函数 $ E({\bf w}) $ 对 $ {\bf w} $ 的二阶导数构成，具体地， {\bf H}_{ij} = \frac{\partial^{2} E({\bf w})} {\partial w_i \partial w_j} \tag{17}现在我们来推导牛顿法对于参数 $ {\bf w} $ 的更新。首先将 $ \nabla E({\bf w}) $ 以矩阵的形式表示 \nabla E({\bf w}) = \sum_{n = 1}^{N} (y_n - 1)t_n \phi_n = {\bf \Phi}^{\text T} [ ({\bf y} - {\bf 1}) \circ {\bf t} ] \tag{18}其中 $ {\bf \Phi}^{\text T} = [\phi_1, \cdots, \phi_N ] $，$ {\bf y} = (y_1, \cdots, y_N) $，$ {\bf t} = (t_1, \cdots, t_N) $，$ {\bf 1} = (1, \cdots, 1) $，$ \circ $ 为元素层面（element-wise）的乘法。 然后可以求得 Hessian 矩阵 {\bf H} = \nabla \nabla E({\bf w}) = \sum_{n=1}^{N} (1 - y_n)y_n \phi_n \phi_n^{\text T} = {\bf \Phi}^{\text T} {\bf R} {\bf \Phi} \tag{19}其中 $ {\bf R} $ 为一 $ N \times N $ 的对角矩阵，其对角元素为 R_{nn} = (1 - y_n)y_n \tag{20}可以看出，该 Hessian 矩阵依赖于参数 $ {\bf w} $，根本原因是损失函数（13）不是一个二次函数，而牛顿法的本质是将目标函数近似为二次函数，这就导致近似值与真实值之间存在差距，需要通过多次迭代来逼近。另一方面，由于 $ y_{n} \in (0, 1) $，从而使得 $ {\bf H} $ 为一个正定（positive-definite）矩阵（对任意非零向量 $ {\bf u} $，都有 $ {\bf u}^{\text T} {\bf H} {\bf u} > 0 $ ）。$ {\bf H} $ 为正定矩阵则意味着损失函数（13）是一个凸函数，存在一个极小值（或一片连续的极小值区域），且该极小值就是全局最优解。 我们将式（18）与式（19）代入式（16）中，可以得到 $ {\bf w} $ 在牛顿法下的更新方程为 \begin{aligned} {\bf w}^{({\text {new} })} &= {\bf w}^{({\text {old} })} - ({\bf \Phi}^{\text T} {\bf R} {\bf \Phi})^{-1} {\bf \Phi}^{\text T} [ ({\bf y} - {\bf 1}) \circ {\bf t} ] \\ &= ({\bf \Phi}^{\text T} {\bf R} {\bf \Phi})^{-1} \lbrace {\bf \Phi}^{\text T} {\bf R} {\bf \Phi} {\bf w}^{({\text {old} })} - {\bf \Phi}^{\text T} [ ({\bf y} - {\bf 1}) \circ {\bf t} ] \rbrace \\ &= ({\bf \Phi}^{\text T} {\bf R} {\bf \Phi})^{-1} {\bf \Phi}^{\text T} {\bf R} {\bf z} \end{aligned} \tag{21}其中 $ {\bf z} $ 为一 $ N $ 维的列向量 {\bf z} = {\bf \Phi} {\bf w}^{({\text {old} })} - {\bf R}^{-1} [ ({\bf y} - {\bf 1}) \circ {\bf t} ] \tag{22}式（21）的形式和线性回归中求解 $ {\bf w} $ 的正规方程的形式很像，只是多了一个权值矩阵 $ {\bf R} $，因此该方法又被称为 iterative reweighted least squares （IRLS）。可以理解为该方法对每一个样本点进行了加权，加权值的大小与 $ R_{nn} $ 成比例，而 $ R_{nn} $ 的大小又与 $ t_n $ 在逻辑回归模型假设中的输出分布情况下的方差 $ {\text {var} } [t_n] $ 成正比；从直觉上来讲，IRLS 方法是对当前参数下变数更大（对给定输入的输出类别的不确定性很大）的训练样本赋予更重的权重，以提高模型对这些样本的预测能力。 梯度下降法与牛顿法比较梯度下降法与牛顿法均为可以用来求解最优化问题的迭代算法。二者有一些共同点：1）都为贪心算法；2）都只能获得局部最优解（若证明目标函数是凸函数，例如 logistic 损失函数（13），则可获得全局最优解）。假设目标函数为 $ f({\bf x}) $，下面分别给出梯度下降法和牛顿法的推导。 梯度下降法的推导对函数值 $ f({\bf x} + {\bf v}) $ 在 $ {\bf x} $ 处作一阶泰勒级数展开得 f({\bf x} + {\bf v}) \approx f({\bf x}) + (\nabla f({\bf x}))^{\text T} {\bf v} \tag{23}假设 $ || {\bf v} || = 1 $（固定更新步长），想要 $ f({\bf x} + {\bf v}) $ 相比 $ f({\bf x}) $ 下降最多，则要使 $ (\nabla f({\bf x}))^{\text T} {\bf v} $ 最小，即 $ {\bf v} $ 的方向要与 $ \nabla f({\bf x}) $ 的方向相反，即有 {\bf v}_s = -\frac{\nabla f({\bf x})} {|| \nabla f({\bf x}) ||} \tag{24}又由于我们希望更新的步长与梯度的大小成正比（因为梯度越小，越接近最优点，更新的步长要减小），可以写出 $ {\bf x} $ 的更新公式为 {\bf x}^{({\text {new} })} = {\bf x}^{({\text {old} })} + \eta || \nabla f({\bf x}) || {\bf v}_s = {\bf x}^{({\text {old} })} - \eta \nabla f({\bf x}) \tag{25}其中 $ \eta $ 即为学习率，需要根据情况调整，既不可设置的太大（容易跳出最佳区域），也不可设置的太小（迭代次数变多，降低算法效率）。 牛顿法的推导对函数值 $ f({\bf x} + {\bf v}) $ 在 $ {\bf x} $ 处作二阶泰勒级数展开得 f({\bf x} + {\bf v}) \approx f({\bf x}) + (\nabla f({\bf x}))^{\text T} {\bf v} + \frac{1}{2} {\bf v}^{\text T} \nabla^{2} f({\bf x}) {\bf v} \tag{26}要想使 $ f({\bf x} + {\bf v}) $ 最小（给定 $ {\bf x} $），即要求使 $ (\nabla f({\bf x}))^{\text T} {\bf v} + \frac{1}{2} {\bf v}^{\text T} \nabla^{2} f({\bf x}) {\bf v} $ 最小的 $ {\bf v}_s $，很容易求得 {\bf v}_s = -(\nabla^{2} f({\bf x}))^{-1} \nabla f({\bf x}) \tag{27}因而 $ {\bf x} $ 的更新公式为 {\bf x}^{({\text {new} })} = {\bf x}^{({\text {old} })} + {\bf v}_s = {\bf x}^{({\text {old} })} - (\nabla^{2} f({\bf x}))^{-1} \nabla f({\bf x}) \tag{28}上式与式（15）一致。 完成上面的推导之后，我们很容易看到牛顿法与梯度下降法之间的一些显著的差异。 首先，梯度下降法需要设置一个超参数——学习率；而牛顿法则不需要。避免设置超参数可以省掉很多事。 其次，牛顿法一般比梯度下降法更快获得收敛，但其需要更多的计算开销和内存开销（需计算和保存 Hessian 矩阵）。牛顿法是二阶近似，而梯度下降法是一阶近似。通俗来讲，假如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所选的位置选一个坡度最大的方向走一步；而牛顿法会在选择方向时，不仅会考虑坡度是否足够大，还会考虑你走了一步之后坡度是否会变得更大。所以，牛顿法比梯度下降法利用的信息更多一些，看得更远一些，从而能更快地到达盆地底部。从几何上来讲，牛顿法就是用一个二次曲面去拟合当前所处位置的局部曲面，而梯度下降法则是用一个平面去拟合当前的局部曲面，很显然，二次曲面要比平面更接近于当前的局部曲面，因而牛顿法会比梯度下降法收敛得更快。 softmax 回归将逻辑回归模型被拓展到多类别的情形时的模型被称为 softmax 回归模型。具体地，softmax 回归假定类别 $ {\cal C}_k $ 的后验概率为 p({\cal C}_k | \phi) = y_k (\phi) = \frac {\exp(a_k)} {\sum_i \exp(a_i) } \tag{29}其中 $ a_k $ 被定义为 a_k = {\bf w}_k^{\text T} \phi \tag{30}softmax 回归的核心问题即是估计参数 $ \lbrace {\bf w}_k \rbrace $，$ k = 1, …, K $。为方便推导，假设输出类别由一个 $ K $ 维向量 $ {\bf t} $ 表征，该向量有且仅有一个元素的值为 1，其他元素的值均为 0，非零元素的序位代表输出的类别（例如，输出类别 $ {\cal C}_k $ 对应的向量的第 $ k $ 个元素为非零）。则可以写出给定数据集 $ \lbrace \phi_n, {\bf t}_n \rbrace $ 下参数 $ \lbrace {\bf w}_k \rbrace $ 的似然函数为 p({\bf T} | {\bf w}_1, \cdots, {\bf w}_K) = \prod_{n=1}^{N} \prod_{k=1}^{K} p({\cal C}_k | \phi_n)^{t_{nk} } = \prod_{n=1}^{N} \prod_{k=1}^{K} y_{nk} ^ {t_{nk} } \tag{31}其中 $ y_{nk} = y_k(\phi_n) $，$ {\bf T} \in \lbrace 0, 1 \rbrace^{N \times K } $ 由输出的类别向量所组成。对似然函数求对数再取负可得交叉熵为 E({\bf w}_1, \cdots, {\bf w}_K) = -\ln p({\bf T} | {\bf w}_1, \cdots, {\bf w}_K) = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk} \tag{32}对 $ E({\bf w}_1, \cdots, {\bf w}_K) $ 求一阶导数可得 \nabla_{\bf {w}_j} E({\bf w}_1, \cdots, {\bf w}_K) = \sum_{n = 1}^{N} (y_{nj} - t_{nj}) \phi_n \tag{33}上式用到了以下导数公式 \frac {\partial y_k} {\partial a_j} = y_k (I_{kj} - y_j) \tag{34}其中 $ I_{kj} $ 是单位矩阵 $ {\bf I}_{KK} $ 中位置为 $ (k, j) $ 的元素。 有了式（33）之后，我们就可以对参数 $ \lbrace {\bf w}_k \rbrace $ 按照梯度下降方法进行更新了。利用牛顿法更新参数的方法和前面类似，只需求出交叉熵关于参数 $ \lbrace {\bf w}_k \rbrace $ 的 Hessian 矩阵即可，这里不加赘述。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>线性分类算法</tag>
        <tag>监督学习</tag>
        <tag>判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图模型与和积算法]]></title>
    <url>%2F%E5%9B%BE%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[图模型综述概率在信号处理领域中扮演着重要的角色，很多问题的建模都需要引入概率（如贝叶斯模型），因而很多问题的求解也表现为对一些概率分布的计算（如计算参数的后验概率）。再复杂的概率计算都是对以下两个规则的重复应用：贝叶斯公式（$ P(A, B) = P(A)P(B | A) $）和边缘概率求取公式（$ P(A) = \sum_{B} P(A, B) $ ）。但当问题变得复杂之后，涉及到的变量会变多、概率模型会变得复杂，因而寻找一种简便直观的分析概率分布的方法就变得重要起来。 图模型（Graphical Models）就是一种用来刻画变量之间的概率依赖关系的方法，它由节点和边组成，节点代表的是各个随机变量，边则代表不同随机变量之间的关系。多变量的概率分布的一些局部特征可以很直观地在图模型中显示出来，同时复杂的概率运算也可以通过一些与图有关的操作来完成，因而我们可以借助图模型来设计新的模型以及推导出更高效的算法。 图模型一般分为两种，一种是贝叶斯网络（Bayesian Networks），其边是有向的，它可以用来表示各随机变量之间的因果关系；另一种是马尔可夫随机场（Markov Random Fields），其边是无向的，它一般用来表示多个随机变量之间所满足的约束关系。下面我们将具体讲述这两种图模型。 贝叶斯网络考虑一个简单的关于三个变量的联合概率分布 $ p(a, b, c) $，根据贝叶斯公式，我们可以写出 p(a, b, c) = p(c | a, b)p(b | a)p(a) \tag{1}我们可以将该概率分解以一个有向图来表示，如下图所示。其中每个节点代表一个随机变量，有向箭头则表示该概率分解中的条件依赖关系，即如果随机变量 $ A $ 依赖于某个随机变量集合 $ {\cal S} $，则 $ {\cal S} $ 中的每一个随机变量对应的节点均会发射出一条有向箭头指向随机变量 $ A $ 所对应的节点（如下图中，$ a $ 指向 $ b $，$ a $、$ b $ 指向 $ c $）。 上面的这种对概率分布的表示方法即为贝叶斯网络。对于更一般的概率分布，例如 $ K $ 个随机变量的联合分布 $ p(x_1, \cdots, x_K) $， 我们总可以根据链式法则将其分解为一系列条件概率的乘积： p(x_1, \cdots, x_K) = p(x_K | x_1, \cdots, x_{K - 1}) \cdots p(x_2 | x_1)p(x_1) \tag{2}将上述分解用贝叶斯网络来表示，可以得到一个具有全连接（fully connected）的有向图，即每个节点都有来自编号比其小的节点的进入边，使得任意两个节点之间都有边相连。 式（2）中的分解代表了一种最一般的形式，对节点之间的关系没有任何假设，因而其对应的贝叶斯网络也是最复杂的。而在实际的建模中，我们或多或少会有一些关于不同随机变量之间的先验信息，因而概率分解会较式（2）简单，反映在贝叶斯网络中即是有向边的减少，有向边的减少又可以进一步促进我们对模型的认识以及简化问题的求解过程。 例如上图表示了随机变量 $ (x_1, \cdots, x_7) $ 的联合分布对应的贝叶斯网络，可以看到，该有向图不是全连接的，我们可以根据该图写出这些变量的联合概率分布 p(x_1)p(x_2)p(x_3 | x_1)p(x_4 | x_1, x_2)p(x_5 | x_2)p(x_6|x_3, x_4)p(x_7 | x_3, x_5) \tag{3}更一般地，我们可以根据任意一个贝叶斯网络写出所有节点对应的随机变量的联合概率分布为 p({\bf x}) = \prod_{k = 1}^{K} p(x_k | \text {pa}_k) \tag{4}其中 $ \text {pa}_k $ 为节点 $ x_k $ 的所有父节点构成的集合（某个节点的父节点指其入边所连接的节点）。 注意到，贝叶斯网络都是有向无环图（Directed Acyclic Graphs，DAG），因为我们总可以按照某种方式将各随机变量排序，使得序位高的随机变量仅仅只“直接”依赖于序位低的随机变量。 条件独立条件独立是多变量概率分布中的一个非常重要的概念，其在分析概率模型、简化模型结构、降低图模型中的推断问题的计算复杂度等方面均扮演着重要的角色。考虑三个随机变量 $ a $，$ b $ 和 $ c $，假设给定 $ b $ 和 $ c $ 的值之后的 $ a $ 的条件概率分布为 p(a | b, c) = p(a | c) \tag{5}即给定 $ b $ 和 $ c $ 之后，$ a $ 并不依赖于 $ b $ 的值，我们称在给定 $ c $ 的值的情况下，$ a $ 和 $ b $ 条件独立，并且我们将这条性质简记为 a \perp b | c \tag{6}给定式（6），我们还可以写出 p(a, b | c) = p(a | b, c)p(b | c) = p(a | c)p(b | c) \tag{7}在知道了某个联合概率分布所对应的贝叶斯网络之后，我们希望能找到任意随机变量集合之间的条件独立关系，d-separation 定理可以帮助我们从贝叶斯网络中很快地找到上述条件独立关系。在介绍 d-separation 定理之前，我们先来看一下最简单的可以表征随机变量间的条件独立关系的三个基本贝叶斯网络单元。 三个基本的贝叶斯网络单元 第一个贝叶斯网络单元如上图所示，我们关心的是在给定 $ c $ 的值的情况下 $ a $ 和 $ b $ 的条件独立性，因而我们根据 $ c $ 所处的地位将该网络称为 tail-to-tail 网络单元（因为连接它的两条边皆为发射边，对应有向箭头的尾部）。可以根据上图写出 $ p(a, b, c) $ 的形式为 p(a, b, c) = p(a | c)p(b | c)p(c) \tag{8}我们首先看一下随机变量 $ a $ 和 $ b $ 是否是独立的，为此，可以写出 $ a $ 和 $ b $ 的联合分布为 p(a, b) = \sum_{c} p(a | c)p(b | c)p(c) \not= p(a)p(b) \tag{9}可见，$ a $ 和 $ b $ 并不独立，我们可以记为 $ a \not\perp b | \emptyset $ （$ \emptyset $ 代表空集）。再来看给定 $ c $ 的值的情况下 $ a $ 和 $ b $ 的条件独立性，写出 $ p(a, b | c) $ 的形式为 p(a, b | c) = \frac{p(a, b, c)}{p(c)} = p(a | c)p(b | c) \tag{10}上式的形式符合条件独立的定义，因而有 $ a \perp b | c $。 我们再来用一种直观的方式来从上图中来阐释各随机变量之间的关系，$ a $ 由 $ c $ 生成，$ b $ 也由 $ c $ 生成，即 $ a $ 和 $ b $ 是“同源”的，因而它们必然是相关的；另一方面，在给定 $ c $ 的值之后，生成的 $ b $ 仅与 $ c $ 相关，生成的 $ a $ 也仅与 $ c $ 相关，这就说明了 $ a $ 和 $ b $ 在给定 $ c $ 的情况下条件独立。我们还可以将上图这样理解，在没有观察到 $ c $ 的值时，节点 $ a $ 和节点 $ b $ 之间是有路径“相通”的（如上图左所示），从而 $ a $ 和 $ b $ 相关；但一旦观察到 $ c $ 的值，$ a $ 和 $ b $ 之间的通路被“阻塞”（如上图右所示，带阴影的节点表示该节点被观察到，下同），从而有 $ a $ 和 $ b $ 对于 $ c $ 条件独立。 第二个贝叶斯网络如下图所示，是一个链式的形式，我们根据 $ c $ 所处的地位将该网络称为 head-to-tail 网络单元。 根据上图可分别写出 $ p(a, b, c) $，$ p(a, b) $ 和 $ p(a, b | c) $ 的形式为 p(a, b, c) = p(a)p(c | a)p(b | c) \tag{11} p(a, b) = p(a) \sum_{c} p(c | a)p(b | c) = p(a)p(b | a) \not= p(a)p(b) \tag{12} p(a, b | c) = \frac{p(a, b, c)}{p(c)} = \frac{p(a)p(c | a)p(b | c)}{p(c)} = p(a | c)p(b | c) \tag{13}可以得出 $ a \not\perp b | \emptyset $，$ a \perp b | c $。这种形式的贝叶斯网络所代表的概率模型在涉及到时间序列的建模的时候应用广泛，例如一阶马尔可夫模型，它比无记忆系统传达的信息更多，又保留了简单易处理的特性。和第一个网络单元类似，我们称在没有观察到 $ c $ 的值时，节点 $ a $ 和节点 $ b $ 之间是“相通”的（如上图左所示），从而 $ a $ 和 $ b $ 相关；但一旦观察到 $ c $ 的值，$ a $ 和 $ b $ 之间的通路即被“阻塞”（如上图右所示），从而有 $ a $ 和 $ b $ 对于 $ c $ 条件独立。 最后来看第三个贝叶斯网络单元，如下图所示，我们称其为 head-to-head 网络单元。 我们还是根据上图分别写出 $ p(a, b, c) $，$ p(a, b) $ 和 $ p(a, b | c) $ 的形式 p(a, b, c) = p(a)p(b)p(c | a, b) \tag{14} p(a, b) = p(a)p(b)\sum_{c} p(c | a, b) = p(a)p(b) \tag{15} p(a, b | c) = \frac{p(a, b, c)}{p(c)} = \frac{p(a)p(b)p(c | a, b)}{p(c)} \not= p(a | c)p(b | c) \tag{16}可以得出 $ a \perp b | \emptyset $，$ a \not\perp b | c $。这个性质正好和上面两个贝叶斯网络单元的性质相反。为方便理解，举一个不恰当的例子，假设 $ a $ 是某次测验的难度，$ b $ 是某学生的能力，$ c $ 是该学生在该次测验的成绩，随机变量 $ a $ 和 $ b $ 一般来讲是独立的，它们共同决定了 $ c $ 的分布，因而我们可以用上图来表示 $ a $，$ b $，$ c $ 之间的关系；当我们观测到 $ c $ 的值之后，例如该学生在本次测验中的成绩很高，我们又以某种方式得知了 $ a $ 的值，例如该次测验的难度非常高，那么 $ b $ 的取值显然是与 $ a $ 有关的，在这里该学生的能力必须非常强才可以。同前面一样，我们可以定义一套从图中就可以得出的条件独立性质：当没有观察到 $ c $ 的值时，节点 $ a $ 和节点 $ b $ 之间是“阻塞” 的（如上图左所示），从而 $ a $ 和 $ b $ 独立；而当观察到 $ c $ 的值后，节点 $ a $ 和节点 $ b $ 之间的路径被“打通”（如上图右所示），因而 $ a $ 和 $ b $ 对于 $ c $ 条件相关。 d-separation 定理假设 $ A $，$ B $ 和 $ C $ 是某一个贝叶斯网络中的三个非相交的节点集合，那么 d-separation 定理可以用来判定下列条件独立性质是否成立：$ A \perp B | C $。在一个贝叶斯网络中，任何一个局部单元都符合我们前面所讲述的三个基本的贝叶斯网络单元的形式，而通过这些基本的贝叶斯网络单元，我们可以看到哪些路径是“阻塞”的或“连通”的。d-separation 定理则基于以上特点来判定任意两个节点之间的所有可能的路径是否是“阻塞”的，若集合 $ A $ 中的任意节点到集合 $ B $ 中的任意节点之间的路径均是“阻塞”的，则称 $ A $ 与 $ B $ 相对于 $ C $ 是 d-separated 的，同时有 $ A \perp B | C $。 我们称集合 $ A $ 中的某节点到集合 $ B $ 中的某节点之间的某条路径是“阻塞”的，如果该路径满足以下两个条件之一： 路径中的有向箭头在某个节点处符合 head-to-tail 或者 tail-to-tail 的形式，且该节点在集合 $ C $ 中； 路径中的有向箭头在某个节点处符合 head-to-head 的形式，且该节点以及该节点的所有子孙节点均不在集合 $ C $ 中。 为方便理解，考虑下图中的例子。 先看上图左，我们要判定 $ a \perp b | c $ 是否成立，即需要判定 $ a $ 到 $ b $ 之间的路径是否是“阻塞”的，$ a $ 到 $ b $ 需要经过 $ e $ 和 $ f $。可以看到有向箭头在节点 $ e $ 处符合 head-to-head 的形式，而该节点的后代 $ c $ 被观测到，因而 $ a $ 到 $ b $ 之间的路径在节点 $ e $ 处是“相通”的，再看有向箭头在节点 $ f $ 处符合 tail-to-tail 的形式，而该节点 $ f $ 并不在观测集合中，因此 $ a $ 到 $ b $ 之间的路径在节点 $ f $ 处也是“相通”的，综合可知，$ a $ 到 $ b $ 之间的路径是“相通”的，故可以得出 $ a \not\perp b | c $ 。 再来看上图右，此时要判定 $ a \perp b | f $ 是否成立，判定方式同上。有向箭头在节点 $ e $ 处符合 head-to-head 的形式，且该节点及其后代 $ c $ 均不在观测集合中，因此 $ a $ 到 $ b $ 的路径在节点 $ e $ 处被“阻断”，事实上 $ a $ 到 $ b $ 的路径在节点 $ f $ 处也是“阻断”的，但判断某条路径是否被“阻断”仅需判断一处“阻断”即可。故可以得出 $ a \perp b | f $ 。 马尔可夫随机场另一种可以用来表示多个变量的概率分布的模型为马尔可夫随机场，同贝叶斯网络不同，它是一个无向图，且它对于表示不同随机变量集合之间的条件独立性质更加直观。在马尔可夫随机场中，若随机变量集合 $ A $ 中的任意节点到随机变量集合 $ B $ 中的任意节点之间的通路都需要经过随机变量集合 $ C $ 中的某一节点，则有 $ A \perp B | C $，如下图所示，可直观理解为 $ C $ 中观测到的变量会坍缩到一个确定的状态，从而会阻塞图中 $ A $ 到 $ B $ 的通路，导致 $ A $ 和 $ B $ 变得不相干。 如何根据马尔可夫场写出对应的随机变量集合的联合分布呢？我们首先定义一下 clique 的概念：若一个节点集合中的所有节点之间均有边连接，则称该节点集合为一个 clique。根据此性质，我们可以得出一个 clique 中不含任何条件独立的性质，即在一个 clique 中找不出三个不相交的子集 $ A $，$ B $，$ C $，使得 $ A \perp B | C $ 成立。因而，我们可以认为每个 clique 都对应一个关于该 clique 中所有随机变量的函数，而最终的所有变量的概率分布可以写成这些函数的乘积。不失一般性，我们一般选取一个无向图中所有的最大 clique （所谓最大 clique，是指对于某个 clique，不可能再向其中加入新的节点，而使得到的新的节点集合保持 clique 全连接的性质），因为其他较小的 clique 所对应的函数总是可以被某个最大 clique 所对应的函数所吸收。我们将某个最大 clique $ C $ 所对应的函数称为势能函数（potential function），并以 $ \psi_C({\bf x}_C) $ 来表示。则所有变量的联合概率分布可以写为 p({\bf x}) = \frac{1}{Z} \prod_C \psi_C({\bf x}_C) \tag{17}其中，$ Z $ 为归一化常量。由于两个不相连的节点必然在两个不同的 clique 中，因此式（17）总是可以将该两个节点对应的随机变量分布在两个不同的因子中，从而在某种程度上完美地反映了马尔可夫随机场所表征的变量之间的条件独立性。 我们可以将贝叶斯网络转换成马尔可夫随机场，对于只包含 tail-to-tail 或 head-to-tail 节点（即每个节点仅有一个父节点）的贝叶斯网络，直接将有向边替换成无向边即可，这一点可以从 tail-to-tail 单元和 head-to-tail 单元的概率表达式推断出来；而对于含有 head-to-head 节点的贝叶斯网络，我们除了需要把该网络中的有向边替换成无向边之外，还需要将该节点的所有父节点两两相连，因为该单元的概率表达式为 $ p(x_k | \text {pa}_k) $，该函数包含了该节点和其所有的父节点，且一般不可再分解，因而该节点和其所有的父节点构成一个 clique。 贝叶斯网络和马尔可夫随机场各具优势，但无论是贝叶斯网络还是马尔可夫随机场，都不能完美的表征所有可能的概率分布，贝叶斯网络擅长处理结构层次比较明显的条件概率性质；而马尔可夫随机场可以处理稍微复杂点的条件概率性质，但其对于一些简单的条件概率性质却不能表征出来（例如贝叶斯网络中的 head-to-head 节点单元，在对应的马尔可夫随机场中，这些节点表现为全连接，丧失了所有条件独立性质）。我们应当根据具体的问题场景来选用合适的图模型。 和积算法与最大和算法和积算法（sum-product algorithm）是基于图模型的一类非常强大且普适的算法，它可以高效地根据一个由多个因式构成的关于多个变量的函数来计算出一些局部函数（譬如求取某些随机变量的边缘分布）。信号处理领域的很多经典算法都可以归结为和积算法（或最大和算法）的特例，例如前向-后向算法（forward/backward algorithm），Kalman 滤波，Viterbi 算法，FFT 算法等等。 在介绍和积算法之前，我们要先介绍因子图（factor graphs），其是推演和积算法的关键。 因子图不管是在贝叶斯网络中还是在马尔可夫随机场中，一个概率分布函数都可表示为如下式所示的一系列因式的乘积 p({\bf x}) = \prod_{\cal S} f_{\cal S}({\bf x}_{\cal S}) \tag{18}其中 $ {\bf x}_{\cal S} $ 为所有随机变量集合的一个子集，每个因子 $ f_{\cal S} $ 为随机变量子集 $ {\bf x}_{\cal S} $ 的函数。我们可以将变量（variables）和因子（factors）均作为节点表示在一张图中，并以无向边连接以上两类节点，用来表征因子和变量的对应关系（若某个因子是关于某个变量的函数，则该变量对应的节点与该因子对应的节点之间应有一条无向边连接），这种图被称为因子图，有时候也被称为二分图（bipartite graphs），因为它包含两类节点且无向边仅存在于两类节点之间。 例如，某个概率分布可以写成如下因子的分解形式 p({\bf x}) = f_a(x_1, x_2, x_3)f_b(x_2, x_3, x_4)f_c(x_4, x_5) \tag{19}其因子图如下图所示 该因子图中实际上包含了一个“环路”（$ f_a \to x_2 \to f_b \to x_3 \to f_a $），而要在一个因子图中精确地推断某些边缘分布或其他有意义的量是需要因子图中是不包括“环路”的（cycle-free），我们可以通过重组某些因子的方式来“去环”，例如在上图中，我们可以将 $ f_a $ 和 $ f_b $ 合并成一个新的因子，但这样会损失一些有用的信息。 大多数情况下，我们都可以根据贝叶斯网络或者马尔可夫随机场得到一个无环的因子图，这样我们就可以在该因子上运行和积算法或者最大和算法以得到我们想要求得的量。 和积算法给定一个关于多变量的概率分布以及该概率分布的一个因式分解，和积算法可以用来高效地求解某个变量子集的边缘概率分布。和积算法的本质就是充分地利用原始概率分布的因式分解（因子图），以及合理地交换求和与求积的顺序（消息传递），来尽可能地减少计算开销。 不失一般性，假设要求解随机变量 $ x $ 的边缘分布 $ p(x) $，可以写为 p(x) = \sum_{ {\bf x} \backslash x} p({\bf x}) \tag{20}其中 $ {\bf x} \backslash x $ 表示除了 $ x $ 之外的其他所有变量，下图显示了无环因子图中包含 $ x $ 的部分。 我们根据上图可以将概率分布 $ p({\bf x}) $ 写为 p({\bf x}) = \prod_{s \in \text {ne}(x)} F_s(x, X_s) \tag{21}其中 $\text {ne}(x) $ 是因子图中所有与变量节点 $ x $ 相邻的因子节点，而 $ X_s $ 是与 $ x $ 相邻的某个因子节点 $ f_s $ 除了 $ x $ 以外的其他变量节点及它们的后代所构成的集合，$ F_s(x, X_s) $ 则表示排除 $ f_s \to x $ 的通路外与因子 $ f_s $ 有关的所有因子所构成的函数。将式（21）代入式（20）中，并交换求和与求积的顺序，可得 p(x) = \prod_{s \in \text {ne}(x)} \left [\sum_{X_s} F_s(x, X_s) \right ] = \prod _{s \in \text {ne}(x)} \mu_{f_s \to x}(x) \tag{22}这里我们引入了函数 $ \mu_{f_s \to x}(x) $，定义为 \mu_{f_s \to x}(x) \equiv \sum_{X_s} F_s(x, X_s) \tag{23}我们可以将 $ \mu_{f_s \to x}(x) $ 视作因子节点 $ f_s $ 向变量节点 $ x $ 传递的消息（message），而 $ p(x) $ 等于变量节点 $ x $ 的所有相邻因子节点向其传递的消息的乘积。因为这个原因，和积算法也被称为消息传递算法（message-passing algorithms）。 下面，我们再来将 $ F_s(x, X_s) $ 做进一步地分解。 如上图所示，它可以被分解为 F_s(x, X_s) = f_s(x, x_1, \cdots, x_M)G_1(x_1, X_{s1}) \cdots G_M(x_M, X_{sM}) \tag{24}$ x_1, \cdots, x_M $ 是因子节点 $ f_s $ 除去 $ x $ 之外的其他所有相邻变量节点，$ G_m(x_m, X_{sm}) $ 表示上图中阴影部分所代表的函数，即排除 $ x_m \to f_s $ 通路外与 $ x_m $ 有关（有简单路径可以由 $ x_m $ 出发到达的变量）的所有变量所组成的函数。将式（24）代入式（23）可得 \begin{aligned} \mu_{f_s \to x}(x) &= \sum_{x_1} \cdots \sum_{x_M} f_s(x, x_1, \cdots, x_M) \prod_{m \in \text{ne}(f_s) \backslash x} \left [ \sum_{X_{sm} }G_m(x_m, X_{sm}) \right ] \\ &= \sum_{x_1} \cdots \sum_{x_M} f_s(x, x_1, \cdots, x_M) \prod_{m \in \text{ne}(f_s) \backslash x} \mu_{x_m \to f_s}(x_m) \end{aligned} \tag{25}其中 $ \text {ne} (f_s) \backslash x $ 表示除开 $ x $ 外与因子节点 $ f_s $ 相邻的变量节点；$ \mu_{x_m \to f_s } $ 为另一类消息，是由变量节点传递至因子节点的消息，定义为 \mu_{x_m \to f_s } \equiv \sum_{X_{sm} }G_m(x_m, X_{sm}) \tag{26}可以注意到，我们定义的在因子图的某一条边上传递的这两类消息（因子节点传递至变量节点的消息 $ \mu_{f \to x}(x) $ 以及变量节点传递至因子节点的消息 $ \mu_{x \to f}(x) $）都只是关于这条边所连接的变量节点 $ x $ 的函数。另一方面，由式（25）可知，某一因子节点想要向某一变量节点传递消息前，须接受到来自该因子节点的其他相邻变量节点向该因子节点传递的消息。 为给出消息 $ \mu_{x_m \to f_s } $ 的确切表达式，我们需推导 $ G_m(x_m, X_{sm}) $ 的表达式。根据下图可以写出 G_m(x_m, X_{sm}) = \prod_{ l \in \text {ne} (x_m) \backslash f_s } F_l(x_m, X_{ml} ) \tag{27} 上式中的 $ F_l(x_m, X_{ml}) $ 表达式和式（24）是一致的。我们将式（27）代入式（26）中，并交换求和与求积的顺序，可得 \begin{aligned} \mu_{x_m \to f_s}(x_m) &= \prod_{ l \in \text {ne} (x_m) \backslash f_s } \left [ \sum_{X_{ml} } F_l(x_m, X_{ml}) \right ] \\ &= \prod_{ l \in \text {ne} (x_m) \backslash f_s } \mu_{f_l \to x_m}(x_m) \end{aligned} \tag{28}上式中用到了消息 $ \mu_{f \to x}(x) $ 的定义式（23）。所以，要计算某个变量节点传递给某个因子节点的消息，只需要计算该变量节点的其他因子节点传递给该变量节点的消息的乘积即可。 在变量节点或因子节点是叶子节点的情况下，变量叶子节点传递给相邻因子节点的消息为 $ \mu_{x \to f}(x) = 1 $，因子叶子节点传递给相邻变量节点的消息为 $ \mu_{f \to x}(x) = f(x) $。 有了这两类消息的计算公式（25）和（28）后，我们可以根据式（22）计算出某个因子图中任意变量的边缘概率分布，这就是和积算法。 一般，当需要计算某个变量的边缘分布时，我们会将该变量作为根节点，然后从叶子节点开始计算消息，直至计算出所有该变量节点的相邻因子节点传递给该变量节点的消息为止，若因子图中的边的个数为 $ K $，则计算开销为 $ O(K) $。如果要分别计算 $ N $ 个变量的边缘分布，我们可以遵从以上方法分别计算每个变量的边缘分布，计算开销为 $ O(NK) $，但这样会有很多重复计算量，某些边上的消息会计算多次。在这种情况下，我们倾向于将每一条边上两个方向上的消息提前计算好，这样任意变量的边缘分布都可以很方便的计算出，且计算开销仅为 $ O(K + N) $，这一点也充分地利用了动态规划算法的核心思想。 下面举一个详细的例子，来说明和积算法的运行过程。 对于上图中的因子图，我们可以分 5 步计算出所有边上的消息函数（步骤编号也在上图中标明），计算过程如下： 第一步 \begin{aligned} \mu_{f_a \to x_1}(x_1) &= f_a(x_1) \\ \mu_{f_b \to x_2}(x_2) &= f_b(x_2) \\ \mu_{x_4 \to f_d}(x_4) &= 1 \\ \mu_{x_5 \to f_e}(x_5) &= 1 \end{aligned}第二步 \begin{aligned} \mu_{x_1 \to f_c}(x_1) &= \mu_{f_a \to x_1}(x_1) \\ \mu_{x_2 \to f_c}(x_2) &= \mu_{f_b \to x_2}(x_2) \\ \mu_{f_d \to x_3}(x_3) &= \sum_{ x_4 } \mu_{x_4 \to f_d}(x_4)f_d(x_3, x_4) \\ \mu_{f_e \to x_3}(x_3) &= \sum_{ x_5 } \mu_{x_5 \to f_e}(x_5)f_e(x_3, x_5) \end{aligned}第三步 \begin{aligned} \mu_{f_c \to x_3}(x_3) &= \sum_{x_1, x_2} \mu_{x_1 \to f_c}(x_1) \mu_{x_2 \to f_c}(x_2)f_c(x_1, x_2, x_3) \\ \mu_{x_3 \to f_c}(x_3) &= \mu_{f_d \to x_3}(x_3) \mu_{f_e \to x_3}(x_3) \end{aligned}第四步 \begin{aligned} \mu_{f_c \to x_1}(x_1) &= \sum_{x_2, x_3} \mu_{x_3 \to f_c}(x_3) \mu_{x_2 \to f_c}(x_2) f_c(x_1, x_2, x_3) \\ \mu_{f_c \to x_2}(x_2) &= \sum_{x_1, x_3} \mu_{x_3 \to f_c}(x_3) \mu_{x_1 \to f_c}(x_1) f_c(x_1, x_2, x_3) \\ \mu_{x_3 \to f_d}(x_3) &= \mu_{f_c \to x_3}(x_3) \mu_{f_e \to x_3}(x_3) \\ \mu_{x_3 \to f_e}(x_3) &= \mu_{f_c \to x_3}(x_3) \mu_{f_d \to x_3}(x_3) \end{aligned}第五步 \begin{aligned} \mu_{x_1 \to f_a}(x_1) &= \mu_{f_c \to x_1}(x_1) \\ \mu_{x_2 \to f_b}(x_2) &= \mu_{f_c \to x_2}(x_2) \\ \mu_{f_d \to x_4}(x_4) &= \sum_{x_3} \mu_{x_3 \to f_d}(x_3)f_d(x_3, x_4) \\ \mu_{f_e \to x_5}(x_5) &= \sum_{x_3} \mu_{x_3 \to f_e}(x_3) f_e(x_3, x_5) \end{aligned}最终求出各变量的边缘分布为 \begin{aligned} p(x_1) &= \mu_{f_a \to x_1}(x_1) \mu_{f_c \to x_1}(x_1) \\ p(x_2) &= \mu_{f_b \to x_2}(x_2) \mu_{f_c \to x_2}(x_2) \\ p(x_3) &= \mu_{f_c \to x_3}(x_3) \mu_{f_d \to x_3}(x_3) \mu_{f_e \to x_3}(x_3) \\ p(x_4) &= \mu_{f_d \to x_4}(x_4) \\ p(x_5) &= \mu_{f_e \to x_5}(x_5) \end{aligned}最大和算法在多变量概率模型中，另一个经常被求解的问题是使得 $ p(\bf {x}) $ 最大的变量序列 $ \bf {x} $ 的取值以及对应的最大概率。一种常见的误区是首先通过和积算法求解出使得每个变量的边缘概率最大所对应的变量取值，这些变量的组合就对应于上述问题的解，但使得各变量的边缘概率的乘积最大并不意味着使得所有变量的联合分布概率最大。因而我们寻求其他的算法来解决上述问题，与和积算法思路相似的最大和算法（max-sum algorithm）是一种对该问题高效的解决方法。 和积算法利用的是乘法对加法的分配律，最大和算法则利用的是乘法对“最大”操作符的分配律： \text {max} (ab, ac) = a \text {max}(b, c) \tag{29}其中 $ a \geq 0 $，这一条件在我们求解的问题中是满足的。这一性质使得我们可以在适当的时候交换“最大”操作符和求积运算。 为避免求取多个概率函数的乘积而导致的“下溢”（underflow）问题，我们通常先对概率函数求取对数，这样式（29）对应的对数版本为 \text {max} (a + b, a + c) = a + \text {max}(b, c) \tag{30}此时，上式中的 $ a $，$ b $，$ c $ 都对应于对数概率，“最大和算法”的名字也是由式（30）的结构而来。 同和积算法一样，我们也可以根据因子图写出对应的两类消息的表达式，且利用这些消息来计算最大概率。不同的地方是将求和变成了求“最大”，同时之前的概率相乘变成了对数概率相加： \mu_{f \to x} = \max_{x_1, \cdots, x_M} \left [ \ln f(x, x_1, \cdots, x_M) + \sum_{ m \in \text {ne}(f_s) \backslash x } \mu_{x_m \to f}(x_m) \right ] \tag{31} \mu_{x \to f}(x) = \sum_{ l \in \text {ne}(x) \backslash f } \mu_{f_l \to x} (x) \tag{32}当变量节点或因子节点为叶子节点时，变量叶子节点传递给其相邻因子节点的消息为 $ \mu_{x \to f} = 0 $，因子叶子节点传递给其相邻变量节点的消息为 $ \mu_{f \to x} = \ln f(x) $。 指定根节点为 $ x $ 时，最大概率可以由下式求出 p({\bf x})^{\text {max} } = \max_{x} \left [ \sum_{s \in \text {ne}(x) } \mu_{f_s \to x}(x) \right ] \tag{33}我们还需要求解使得 $ p({\bf x}) $ 最大的 $ \bf x $，由式（33）可知，根节点对应的随机变量的取值应为 x^{\text {max} } = \arg \max_{x} \left [ \sum_{s \in \text {ne}(x) } \mu_{f_s \to x}(x) \right ] \tag{34}怎样求取其他的随机变量的取值呢？一种很容易想到的方法是再从根节点出发，求取因子图各边反向传播的消息函数，然后再利用式（34）分别得到各随机变量的取值。这种方法实际上是有问题的，因为可能有多个使得 $ p({\bf x}) $ 最大的 $ \bf x $ 序列，而利用上述方法求取的各个随机变量的取值是分散且不成体系的，它们组合起来可能并不对应于一个可行解（例如有两个可行解 $ {\bf x}_A $ 和 $ {\bf x}_B $，利用上述方法求解出来的解中的部分随机变量的取值来自于 $ {\bf x}_A $，部分随机变量的取值来自于 $ {\bf x}_B $ ），这也是最大和算法区别于和积算法的特点之一。 为解决上述问题，我们在利用式（31）计算因子节点 $ f $ 传递至变量节点 $ x $ 的消息时，保留一份使得式（31）中的项取得最大值的 $ x_1^{\text {max} }, \cdots, x_M^{\text {max} }$。这样在求取出 $ x^{\text {max} } $ 的值后，我们从存储的数据中取出 $ x^{\text {max} } $ 对应的序列 $ x_1^{\text {max} }, \cdots, x_M^{\text {max} }$，依此规律进行操作，最终可以得出一个可行的使得 $ p({\bf x}) $ 最大的序列 $ {\bf x}^{\text {max} } $。这种预先存储可能用到的数据并在需要时快速查找它们的方法被称为“回溯”法（back-tracking）。 最大和算法的一个主要应用是在隐马尔可夫模型（Hidden Markov Models）中，给定特定的观测序列，求解隐含变量序列最有可能取得的状态，对应的算法为 Viterbi 算法。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>概率分布</tag>
        <tag>图模型</tag>
        <tag>贝叶斯网络</tag>
        <tag>马尔可夫随机场</tag>
        <tag>和积算法</tag>
        <tag>最大和算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归模型]]></title>
    <url>%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述经典监督学习算法的第一篇文章，其它相关文章请参见 经典监督学习算法系列文章。 线性回归模型一般来讲，监督学习问题可分为两类，一类是分类问题，另一类是回归问题。这两类问题的核心都是“根据一个输入数据预测出一个输出值”，只不过分类问题的输出值是离散、可数的，而回归问题的输出值是连续的。本文主要聚焦用于解决回归问题的一类最基本的模型——线性回归模型。 最简单的线性回归模型就是将输出表示为输入数据各个特征的线性组合： y({\bf x}, {\bf w}) = w_{0} + w_{1}x_{1} + … + w_{D}x_{D} \tag{1}其中 $ {\bf x} = (x_{1},\cdots, x_{D})^{\text T} $。可以看到，上式不仅是关于参数 $ w_{0}, \cdots, w_{D} $ 的线性函数，也是关于输入数据各特征 $ x_{i} $ 的线性函数。但这样会对模型造成很大的局限性，所以我们转而考虑以下的线性模型： y({\bf x}, {\bf w}) = w_{0} + \sum_{j = 1}^{M - 1} w_{j}\phi_{j}({\bf x}) \tag{2}其中函数 $ \phi_{j} ({\bf x}) $ 被称为基函数（basis functions），因而以上模型相当于先对输入数据进行了预处理和特征变换，产生出了一组新的特征 $ \phi_{j} ({\bf x}) $，然后再将这一组新的特征作一个线性组合作为输出，这样一来就大大地增加了模型的丰富程度。注意到，现在式 （2）不再是关于 $ x_{i} $ 的线性函数，但仍然是关于参数 $ w_{i} $ 的线性函数，前者丰富了模型的复杂度，而后者则使得问题的求解仍然十分简单。 参数 $ w_{0} $ 用来刻画输出相对于输入数据的一个偏移，我们需要它是因为关于输入数据的某个仿射空间比关于输入数据的某个线性空间更加具有一般性。以下为分析方便，假设 $ \phi_{0} = 1 $，则 式 （2）可以改写为 y({\bf x}, {\bf w}) = {\bf w}^{\text T} {\bf \phi}({\bf x}) \tag{3}其中 $ {\bf w} = (w_{0}, \cdots, w_{M - 1})^{\text T} $，$ {\bf \phi} = (\phi_{0}, \cdots, \phi_{M - 1})^{\text T} $ 。 一般情况下，我们会先确定好基函数 $ \phi_{j} ({\bf x}) $ 的形式，常用的有多项式函数、高斯函数、sigmoid 函数等等（预处理、特征变换）；然后我们再用式（3）来拟合训练集，使得某个代价函数最小，以此求得参数 $ {\bf w} $ 的值（训练）；最后再用求得的模型来对新的输入数据进行预测（预测）。 最小二乘估计与最大似然估计监督学习的核心问题即是根据训练数据找到一个合理的模型，在线性回归问题中，模型已经确定了下来，所以问题简化为估计模型的参数 $ {\bf w} $，这一部分将阐述参数 $ {\bf w} $ 的最小二乘估计方法及其与最大似然估计问题的联系。 最小二乘参数估计假设训练集的输入数据为 $ {\bf X} = \lbrace {\bf x}_{1}, \cdots, {\bf x}_{N} \rbrace $，其中 $ {\bf x}_{i} \in {\Bbb R}^{D} $， 对应的输出集合为 $ \lbrace t_{1}, \cdots, t_{N} \rbrace $，将输出数据表示为向量的形式为 $ {\bf t} = (t_{1}, \cdots, t_{N})^{\text T} $，那么最小二乘估计希望由下式所表达的真实输出值与拟合得到的输出值之间的平方误差和最小： E_{D}({\bf w}) = \frac{1}{2} \sum_{n = 1}^{N} \lbrace t_{n} - {\bf w}^{\text T} {\bf \phi} ({\bf x}_{n}) \rbrace^{2} \tag{4}上式中的系数 $ \frac{1}{2} $ 是为了后续处理的方便。将上式对 $ {\bf w} $ 求导可得 \nabla E_{D}({\bf w}) = -\sum_{n = 1}^{N} \lbrace t_{n} - {\bf w}^{\text T} {\bf \phi} ({\bf x}_{n}) \rbrace {\bf \phi} ({\bf x}_{n}) \tag{5}将以上关于 $ {\bf w} $ 的导数置为 $ {\bf 0} $ 可得 \sum_{n = 1}^{N} t_{n} {\bf \phi} ({\bf x}_{n}) - \left( \sum_{n = 1}^{N} {\bf \phi} ({\bf x}_{n}){\bf \phi} ({\bf x}_{n}) ^{\text T} \right) {\bf w} = {\bf 0} \tag{6}可以求得参数 $ {\bf w} $ 的最小二乘估计为 {\bf w}_{\text {LS}} = \left( {\bf \Phi}^{\text T} {\bf \Phi} \right)^{-1} {\bf \Phi}^{\text T} {\bf t} \tag{7}该式被称为最小二乘问题的正规方程（normal equations），其中矩阵 $ {\bf \Phi} \in {\Bbb R}^{N \times M} $ 的具体形式为 {\bf \Phi} = \begin{pmatrix} {\bf \phi}({\bf x}_{1})^{\text T} \\ {\bf \phi}({\bf x}_{2})^{\text T} \\ \vdots \\ {\bf \phi}({\bf x}_{N})^{\text T} \end{pmatrix} = \begin{pmatrix} \phi_{0}({\bf x}_{1}) & \phi_{1}({\bf x}_{1}) & \cdots & \phi_{M - 1}({\bf x}_{1}) \\ \phi_{0}({\bf x}_{2}) & \phi_{1}({\bf x}_{2}) & \cdots & \phi_{M - 1}({\bf x}_{2}) \\ \vdots & \vdots & \ddots & \vdots \\ \phi_{0}({\bf x}_{N}) & \phi_{1}({\bf x}_{N}) & \cdots & \phi_{M - 1}({\bf x}_{N}) \end{pmatrix} \tag{8}注意到，最小二乘估计的目的是想求得以下线性方程组的近似解： {\bf \Phi} {\bf w} = {\bf t} \tag{9}当 $ N >> M $ 时（一般情况下都会要求训练样本数远远大于数据的维度，否则容易出现过拟合），$ {\bf \Phi} $ 的列空间为一个嵌在 $ N $ 维空间中的 $ M $ 维子空间，而当 $ {\bf t} $ 正好也在该 $ M $ 维子空间中时，方程（9）有解，而由于 $ N >> M $， $ {\bf t} $ 正好落在 $ {\bf \Phi} $ 的列空间的可能性极小（如果出现这种情况，说明数据集不够好，导致出现过拟合现象），因而我们希望找到一个 $ {\bf w} $，使得 $ {\bf \Phi} {\bf w} $ 与 $ {\bf t} $ 尽可能地接近，$ {\bf t} $ 到 $ {\bf \Phi} $ 的列空间的最近距离为 $ {\bf t} $ 到其在 $ {\bf \Phi} $ 的列空间上的投影的距离，因而我们可以求出向量 $ {\bf t} $ 在 $ {\bf \Phi} $ 的列空间上的投影，可求得该投影为 {\bf y}_{\text {proj}} = {\bf \Phi} \left( {\bf \Phi}^{\text T} {\bf \Phi} \right)^{-1} {\bf \Phi}^{\text T} {\bf t} = {\bf \Phi} {\bf w}_{\text {proj}} \tag{10}可以看到，式（10）得出的最佳 $ {\bf w}_{\text {proj}} $ 和式（7）中的 $ {\bf w}_{\text {LS}} $ 一致，而以上的阐述正好直观地反映了最小二乘方法的几何意义。 与最大似然估计的联系假设输出值 $ t $ 是由下式所产生： t = y({\bf x}, {\bf w}) + \epsilon \tag{11}即我们假设 $ t $ 是由 $ y({\bf x}, {\bf w}) $ 受到一个加性噪声的扰动所产生的，噪声 $ \epsilon $ 服从均值为 $ 0 $，方差为 $ \beta^{-1} $ 的高斯分布，则我们可以得出输出值 $ t $ 服从分布 p(t|{\bf x}, {\bf w}, \beta) = {\cal N}(t| y({\bf x}, {\bf w}), \beta^{-1}) \tag{12}给定训练集 $ {\bf X} $ 和 $ {\bf t} $ 后，我们可以写出参数 $ {\bf w} $ 的对数似然函数： \begin{aligned} \ln p({\bf t} | {\bf w}, \beta) &= \sum_{n = 1}^{N} \ln {\cal N}(t_{n}| {\bf w}^{\text T} {\bf \phi}({\bf x}_{n}), \beta^{-1}) \\ &= \frac{N}{2} \ln \beta - \frac{N}{2} \ln(2 \pi) - \beta E_{D}({\bf w}) \end{aligned} \tag{13}其中 $ E_{D}({\bf w}) $ 的定义见式（4），可以看到，为使似然函数最大，我们希望 $ E_{D}({\bf w}) $ 最小（上式中的前两项均为常数），由此说明了该模型下的关于参数 $ {\bf w} $ 的最大似然估计等价于最小二乘估计。 含正则项的最小二乘估计最大似然估计的一大问题就是容易出现过拟合，所谓过拟合就是模型在训练集上表现得太好，学习到了训练集的一些非泛化的特点（如噪声），从而导致训练出来的模型在新的数据上表现得不佳。当输入训练样本数相对于参数的个数（模型复杂度的表征）较少时，容易出现过拟合现象，表现形式为训练出来的模型过于复杂。为更好地理解这一点，想象当数据空间所处的维度较大（对应于参数的个数）时，要想得到一个好的模型，是需要足够多的输入样本来填充这个数据空间的，当输入样本数过少，模型就倾向于学习到一些局部的特性，从而导致模型在新数据上的表现很差。因而我们需要适当地限制模型的复杂度，这一点可以通过在代价函数中加入一项用来衡量模型复杂度的正则项 $ E_{W}({\bf w}) $ 来实现，这样，新的代价函数变为 E_{D}({\bf w}) + \lambda E_{W}({\bf w}) \tag{14}其中 $ \lambda $ 为权衡正则化项与最小二乘代价项的参数， $ \lambda $ 越大，我们越倾向于选择复杂度低的模型。值得指出的是，$ \lambda $ 为一个超参数（hyperparameter），属于模型的一部分，需要在确定模型的时候指定它的值。 模型的复杂度常常体现在参数 $ {\bf w} $ 的各个分量的幅度上，因而一个最常用的正则项为 $ {\bf w} $ 的 $ {\text L}2 $ 范数，如下式所示，同样地，前面的 $ \frac{1}{2} $ 系数是为了处理方便。 E_{W}({\bf w}) = \frac{1}{2} {\bf w}^{\text T}{\bf w} \tag{15}该正则项会将参数 $ {\bf w} $ 向 $ {\bf 0} $ 的方向引导，将式（4）和式（15）代入式（14），可以写出新的代价函数为 \frac{1}{2} \sum_{n = 1}^{N} \lbrace t_{n} - {\bf w}^{\text T} {\bf \phi} ({\bf x}_{n}) \rbrace^{2} + \frac{\lambda}{2} {\bf w}^{\text T}{\bf w} \tag{16}由于式（16）仍然是参数 $ {\bf w} $ 的二次函数，因而将其对 $ {\bf w} $ 求导并置为 $ {\bf 0} $，可得 {\bf w}_{\text {LSR}} = (\lambda {\bf I} + {\bf \Phi}^{\text T} {\bf \Phi})^{-1} {\bf \Phi}^{\text T} {\bf t} \tag{17}可以看到，加了正则项之后，求得的 $ {\bf w} $ 的表达式中是对 $ (\lambda {\bf I} + {\bf \Phi}^{\text T} {\bf \Phi}) $ 求逆，而原先是直接对 $ {\bf \Phi}^{\text T} {\bf \Phi} $ 求逆。方阵 $ (\lambda {\bf I} + {\bf \Phi}^{\text T} {\bf \Phi}) $ 相对于 $ {\bf \Phi}^{\text T} {\bf \Phi} $ 来说有两点好处：一是保证可逆，二是条件数（condition number）比较小。而这两点就可以保证 $ {\bf w} $ 的每个分量不会很大，从而也就抑制了过拟合现象的发生。 事实上，$ {\bf w} $ 的 $ \rm{L}\it{p}$ 范数都可以被选作正则项。但 $ \rm{L}2$ 范数和 $ \rm{L}1$ 范数被应用的最为广泛，将 $ \rm{L}2$ 范数作为正则项的线性回归问题一般被称作 Ridge 回归问题，而含有 $ \rm{L}1$ 范数约束的优化问题被称作 LASSO 问题 。$ {\bf w} $ 的 $ \rm{L}1$ 范数约束相较于 $ \rm{L}2$ 范数约束来说更倾向于选择出一组稀疏的解（即倾向于将 $ {\bf w} $ 的某些分量置为 0）。 我们后面可以看到，含正则项的最小二乘估计实际上等价于 Bayesian 线性回归模型（参数的先验概率分布取为零均值球形高斯分布）下对参数的最大后验概率估计，采用 Bayesian 的方法来训练模型一般都可以很好地解决过拟合的问题。 Bias-Variance 分解与折中最大似然估计或最小二乘估计倾向于选择较为复杂的模型，从而导致过拟合现象，而加入正则项且将 $ \lambda $ 的值设得比较大时，又会使模型变得过于简单，从而导致其在新的数据上表现得也不好（这种现象被称为“欠拟合”，即模型学习到的有用的东西过少）。最佳的模型的复杂度应该介于以上两者之间。事实上，针对某个模型，我们可以用一套频率学派（freqentist viewpoint）的方法来分析其泛化能力（在测试集上表现的好坏），该方法将模型对输入数据的估计值相对于真实值的均方误差分解为两个部分，一部分由 bias 所表征，另一部分由 variance 所表征，二者此消彼长。这种分解方法非常直观且具有启发性，对于分析模型的泛化能力也非常有帮助，它使得我们可以从 bias 和 variance 这两个维度来分析模型的过拟合\欠拟合程度。 假设由输入数据产生输出值的条件概率分布为 $ p(t | {\bf x}) $，那么由信号估计理论可知，在均方误差最小的准则下，对输出值 $ t $ 的最佳估计器为 h({\bf x}) = {\Bbb E}[t | {\bf x}] = \int t p(t | {\bf x}) {\text d} t \tag{18}该估计器被称为 MMSE 估计器 （Minimum Mean Square Error），其在信号处理领域和通信领域被广泛使用。 在机器学习中，一般我们无从得知 $ p(t | {\bf x}) $ 的确切分布，因而我们也无从根据式（18）得出 $ h({\bf x}) $；但我们可以根据训练集学习到一个模型（或估计器）$ y({\bf x}) $，然后我们再来用均方误差来衡量该模型的好坏： \begin{aligned} {\Bbb E}[L] &= \int \lbrace y({\bf x}) - t \rbrace^{2} p({\bf x}, t) {\text d}{\bf x}{\text d} t \\ &= \int \lbrace y({\bf x}) - {\Bbb E}[t | {\bf x}] + {\Bbb E}[t | {\bf x}] - t \rbrace^{2} p({\bf x}, t) {\text d}{\bf x}{\text d} t \\ &= \int \lbrace y({\bf x}) - {\Bbb E}[t | {\bf x}] \rbrace^{2} p({\bf x}) {\text d}{\bf x} + 2 \int \lbrace y({\bf x}) - {\Bbb E} [t | {\bf x}] \rbrace \left\lbrace \int \lbrace {\Bbb E}[t | {\bf x}] - t \rbrace p(t | {\bf x}) {\text d} t \right\rbrace p({\bf x}) {\text d}{\bf x} \\ &+ \int \lbrace {\Bbb E}[t | {\bf x}] - t \rbrace^{2} p({\bf x}, t) {\text d}{\bf x} {\text d} t \\ &= \int \lbrace y({\bf x}) - h({\bf x}) \rbrace^{2} p({\bf x}) {\text d}{\bf x} + \int \lbrace h({\bf x}) - t \rbrace^{2} p({\bf x}, t) {\text d}{\bf x} {\text d} t \end{aligned} \tag{19}我们可以看到，模型 $ y({\bf x}) $ 的均方误差可以分为两部分，一部分为 $ y({\bf x}) $ 相对于最佳估计器 $ h({\bf x}) $ 的均方误差，对应于式（19）右侧的第一项；另一部分为系统的固有噪声，与模型无关，对应于式（19）右侧的第二项，因此第一部分对模型的性能的评估意义重大。下面我们来专门分析 $ \lbrace y({\bf x}) - h({\bf x}) \rbrace^{2} $ 这一项。 注意到，选定一个模型后，我们会根据训练集来计算出该模型的参数，而对于不同的训练集，得到的模型的参数是不同的。所以给定某个训练集 $ {\cal D} $，式（19）右侧的第一项中的积分项可表示为 \lbrace y({\bf x}, {\cal D}) - h({\bf x}) \rbrace^{2} \tag{20}上式与训练集 $ {\cal D} $ 强相关，因而不能从统计意义上来表征某个模型的好坏。我们假定有无穷个大小相同的训练集 $ {\cal D} $，每个训练集中的数据都依照某个分布 $ p({\bf x}, t) $ 独立同分布地产生，因而我们可以对式（20）依照训练集 $ {\cal D} $ 求统计平均，而这个统计平均是可以很好地反映出某个模型的好坏的。 将式（20）分解，可以得到 \begin{aligned} \lbrace y({\bf x}, {\cal D}) - h({\bf x}) \rbrace^{2} &= \lbrace y({\bf x}, {\cal D}) - {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] + {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] - h({\bf x}) \rbrace^{2} \\ &= \lbrace y({\bf x}, {\cal D}) - {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] \rbrace^{2} + \lbrace {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] - h({\bf x}) \rbrace^{2} \\ &+ 2 \lbrace y({\bf x}, {\cal D}) - {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] \rbrace \lbrace {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] - h({\bf x}) \rbrace \end{aligned} \tag{21}对上式两端依照训练集 $ {\cal D} $ 求期望，可以看到上式右端的第三项会变为 $ 0 $，因而有 \begin{aligned} &{\Bbb E}_{\cal D}[\lbrace y({\bf x}, {\cal D}) - h({\bf x}) \rbrace^{2}] \\ &= \underbrace{\lbrace {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] - h({\bf x}) \rbrace^{2} }_{(\text {bias})^2} + \underbrace{\Bbb {E}_{\cal D}[\lbrace y({\bf x}, {\cal D}) - {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] \rbrace^{2}]}_{\text {variance} } \end{aligned} \tag{22}将上式代入式（19）中，可得 {\Bbb E}[L] = ({\text {bias}})^{2} + {\text {variance}} + {\text {noise}} \tag{23}其中 \begin{align} ({\text {bias}})^{2} &= \int \lbrace {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] - h({\bf x}) \rbrace^{2} p({\bf x}) {\text d} {\bf x} \tag{23} \\ {\text {variance}} &= \int \Bbb {E}_{\cal D}[\lbrace y({\bf x}, {\cal D}) - {\Bbb E}_{\cal D}[y({\bf x}, {\cal D})] \rbrace^{2}] p({\bf x}) {\text d} {\bf x} \tag{24} \\ {\text {noise}} &= \int \lbrace h({\bf x}) - t \rbrace^{2} p({\bf x}, t) {\text d}{\bf x} {\text d} t \tag{25} \end{align}可以看到，某个模型的均方误差由三部分组成，第一部分为模型在无穷个数据集上的平均表现相对于最佳估计器 $ h({\bf x}) $ 的偏差的平方 $ ({\text {bias}})^{2} $，表征的是模型的学习能力；第二部分为在不同数据集情况下训练出来的具体模型之间的差异的大小 $ {\text {variance}} $，表征的是模型的过拟合倾向；第三部分为与模型无关的固有噪声 $ {\text {noise}} $。 为使模型的泛化能力强（即均方误差尽可能地小），我们要使 $ ({\text {bias}})^{2} $ 与 $ {\text {variance}} $ 之和尽量地小。而 bias 与 variance 之间存在一个 trade-off，对于复杂的模型，其 bias 会很小（学习能力强），variance 大（倾向于过拟合）；而对于简单的模型，其 bias 大（学习能力弱），variance 小（在各个数据集上训练出的参数差异不大）。因而具有最佳泛化能力的模型应当介于以上二者之间，需要在 bias 和 variance 之间寻求一个平衡。 下面举个具体的例子来阐释 bias-variance 分解与折中。假定输入样本 $ x_{n} $ 由一个取值范围为 $ (0, 1) $ 的均匀分布所产生，输出值 $ t_{n} $ 先由函数 $ h(x) = \text {sin}(2\pi x) $ 计算出一个值，然后再加上一个偏移噪声，噪声由服从均值为 $ 0 $，标准差为 $ 0.3 $ 的高斯分布产生。依照以上过程产生 100 个数据集，每个数据集包含 $ N = 25 $ 个样本点。我们使用线性回归模型，选取 $ \phi_{m} (x) = x^m $，其中 $ m = 0,1,\cdots,24 $，采用含 $ {\text L}2 $ 正则项的最小二乘估计作为对参数的估计方法，分别在每个数据集上计算出一个预测函数 $ y^{(l)}(x) $，其中 $ l = 1,2,\cdots, 100 $。为展示不同模型之间的泛化能力的差异，我们使用正则系数 $ \lambda $ 来控制模型的复杂度，这里我们选取了三个模型，分别对应于 $ \ln \lambda = 2.6 $ （低复杂度）、$ \ln \lambda = -0.31 $（中复杂度）、$ \ln \lambda = 2.6 $ （高复杂度）。 下图中每一行展示一个模型，每一行的左侧展示了该模型在不同数据集下得到的预测函数的曲线，右侧展示了平均预测函数（红色曲线）和最佳估计器 $ h(x) = \text {sin}(2\pi x) $ （绿色曲线）之间的差异。 可以看到，不同复杂度下的模型的 bias 和 variance 的变化趋势确实如之前分析的一样。 我们还可以取更多不同的 $ \lambda $ 值，以对应更多不同复杂度的模型，从而将 bias 和 variance 的变化趋势更加直观的画出来，如下图所示。 上图中我们还画出了不同模型在测试集上的误差变化曲线，和 $ ({\text {bias}})^{2} $ 与 $ {\text {variance}} $ 之和的曲线的变化趋势基本一致。同时还可以看到，具有最佳泛化能力（最佳表示在我们使用的模型空间中的最佳模型）的模型的复杂度既没有太低，也没有太高。 Bayesian 线性回归模型最大似然估计体现了频率学派对未知参数（如 $ {\bf w} $ ）的一种经典的处理手法：假设未知参数是一个我们不知道的确定量，然后再利用手上有的资料或信息（例如训练数据集）来对该未知参数作一个点估计，并利用该估计量去解决现有问题（如对新的数据进行预测）。但由于我们已知的可以用来推断未知参数的信息总是有限的或有偏的，因而非常容易导致过拟合。 假设我们有多份数据集，每份数据集的样本数都很可观，我们可以根据每份数据集推断出一个对未知参数的估计值，然后再将所有的估计值取平均得到一个新的估计量，这个新的估计量应当更接近未知参数的真实值。而事实上，我们仅有一份数据集，因而无法利用上述的方法来提高估计量的精度，但利用 Bayesian 的方法，我们只需要一份数据集就可以实现上面的取平均的方法，这里的取平均是利用参数的分布做加权平均，而不是对不同的数据集取平均，因而 Bayesian 的方法在抑制过拟合或模型选择方面益处多多。 Bayesian 学派将未知参数当作一个随机变量，且这个随机变量有一个先验概率分布，当我们有了数据集之后，根据该数据集，我们可以得出一个关于该参数的后验分布。这样我们最终拿到的关于未知参数的信息不再是一个武断的点估计量，而是一个概率分布，而利用该概率分布可做的事情很多，例如对参数进行最大后验概率估计（仍然是点估计）、直接对新数据进行预测等等。 下面我们利用 Bayesian 的方法来解决线性回归问题。我们首先还是确定一个模型（确定 $ {\bf \phi}({\bf x}) $ 的形式和维度），然后再给参数 $ {\bf w} $ 假定一个先验分布 $ p({\bf w}) $，为处理方便，我们假定其先验分布服从高斯分布 p({\bf w}) = {\cal N}({\bf w} | {\bf m}_{0}, {\bf S}_{0}) \tag{26}参数 $ {\bf w} $ 的后验分布满足 p({\bf w} | {\bf t}) \propto p({\bf w})p({\bf t} | {\bf w}) \tag{27}由式（12）可以写出 $ p({\bf t} | {\bf w}) $ 的形式为 p({\bf t} | {\bf w}) = \prod_{n = 1}^{N} {\cal N}(t_{n} | {\bf w}^{\text T}{\bf \phi}({\bf x}_{n}), \beta^{-1}) \tag{28}可以算得，参数 $ {\bf w} $ 的后验概率为 p({\bf w} | {\bf x}) = {\cal N}({\bf w} | {\bf m}_{N}, {\bf S}_{N}) \tag{29}其中 \begin{align} {\bf m}_{N} &= {\bf S}_{N}({\bf S}_{0}^{-1} {\bf m}_{0} + \beta {\bf \Phi}^{\text T} {\bf t}) \tag{30} \\ {\bf S}_{N}^{-1} &= {\bf S}_{0}^{-1} + \beta{\bf \Phi}^{\text T}{\bf \Phi} \tag{31} \end{align}注意到，由于参数 $ {\bf w} $ 的后验概率为高斯分布，因而其最大值与其均值一致，所以对参数 $ {\bf w} $ 的最大后验概率估计即为 $ {\bf w}_{\text {MAP}} = {\bf m}_{N} $。当参数 $ {\bf w} $ 的先验分布不偏好于任意的取值时，即 $ {\ S}_{0} = \alpha^{-1} {\bf I} $ 且 $ \alpha \to 0 $，均值 $ {\bf m}_{N} $ 退化为最大似然估计值 $ {\bf w}_{\text {ML}} $ （或 $ {\bf w}_{\text {LS}} $）；当样本数量 $ N = 0 $ 时，参数 $ {\bf w} $ 的后验概率分布和其先验概率分布一致。另一个值得注意的地方是，当训练数据是序贯地到达时（在 online learning 中非常常见），每到达一份数据，我们都可以根据该数据计算出参数的后验概率分布，而这个后验概率分布又会被下一份数据当作该参数的先验概率分布，以此来估计新的后验概率分布。 当取先验概率分布的参数 $ {\ m}_{0} = {\bf 0} $，$ {\bf S}_{0} = \alpha^{-1} {\bf I} $ 时，后验概率分布的参数变为 \begin{align} {\bf m}_{N} &= \beta {\bf S}_{N} {\bf \Phi}^{\text T}{\bf t} \tag{32} \\ {\bf S}_{N}^{-1} &= \alpha {\bf I} + \beta{\bf \Phi}^{\text T}{\bf \Phi} \tag{33} \end{align}对该后验概率分布取对数，可得 \ln p({\bf w} | {\bf t}) = -\frac{\beta}{2} \sum_{n = 1}^{N} \lbrace t_{n} - {\bf w}^{\text T} {\bf \phi}({\bf x}_{n}) \rbrace^{2} - \frac{\alpha}{2} {\bf w}^{\text T} {\bf w} + {\text {const}} \tag{34}对该式求最大值（即对参数 $ {\bf w} $ 求最大后验概率估计）和对式（16）求最小值是等价的，这也印证了为什么在代价函数里面加入正则项会抑制过拟合：加入正则项本质上是采用了 Bayesian 的方法，即我们作了系统偏好于简单的模型的假设（在参数的先验概率分布中体现）。 计算出了参数 $ {\bf w} $ 的后验概率分布后，我们可以直接用其对新数据进行预测。假定给定的新数据为 $ {\bf x} $，输出 $ t $ 的分布即是对输出分布按照 $ {\bf w} $ 的后验概率作加权平均，如下所示： \begin{align} p(t|{\bf x}, {\bf t}, \alpha, \beta) &= \int p(t|{\bf x}, {\bf w}, \beta)p({\bf w} | {\bf t}, \alpha, \beta) {\text d} {\bf w} \\ &= {\cal N}(t | {\bf m}_{N}^{\text T} {\bf \phi}({\bf x}), \sigma_{N}^{2}({\bf x})) \end{align} \tag{35}可以看到，输出 $ t $ 仍然服从高斯分布，其方差为 \sigma_{N}^{2}({\bf x}) = \frac{1}{\beta} + {\bf \phi}({\bf x})^{\text T} {\text S}_{N} {\bf \phi}({\bf x}) \tag{36}在 Bayesian 的框架下，我们可以给出关于输出 $ t $ 的更丰富的信息，而不像之前一样根据一个数据集学习到一个参数，然后再根据该参数算出一个确定的输出值。分析输出 $ t $ 的分布可知，其均值为 $ {\bf m}_{N}^{\text T} {\bf \phi}({\bf x}) $，和最大后验概率得到的模型对新数据的输出一致；方差分为两个部分，一部分为噪声，另一部分为由训练集带来的不确定性，$ \sigma_{N}^{2}({\bf x}) $ 是关于训练样本数 $ N $ 的减函数，且 $ \lim_{N \to +\infty} \sigma_{N}^{2}({\bf x}) = \frac{1}{\beta} $，因而训练样本数越多，我们对 $ t = {\bf m}_{N}^{\text T} {\bf \phi}({\bf x}) $ 的把握越大。 Bayesian 方法也可以被用于作模型选择，思路和我们前面讲到的一致，这里不加赘述。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>线性模型</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（四）：DBSCAN 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 聚类分析系列文章。 DBSCAN 算法DBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的密度聚类算法。密度聚类算法认为各个 cluster 是样本点密度高的区域，而 cluster 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。 要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。 几个定义在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 cluster，而噪声点则存在于各 cluster 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。 定义1： （样本点的 $ \text {Eps} $-邻域）假设数据集为 $ \bf X $，则样本点 $ \bf p $ 的 $ \text {Eps} $-邻域定义为 $ N_{\text {Eps}} ({\bf p}) = \lbrace {\bf q} \in {\bf X} | d({\bf p}, {\bf q}) \le {\text {Eps}} \rbrace $. 我们再给定一个参数 $ \text {MinPts} $，并定义核心点须满足的条件为：其 $ \text {Eps} $-邻域内包含的样本点的数目不小于 $ \text {MinPts} $ ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 cluster 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 cluster 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 cluster 所具有的形式。 定义2： （密度直达）我们称样本点 $ \bf p $ 是由样本点 $ \bf q $ 对于参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $ 密度直达的，如果它们满足 $ {\bf p} \in N_{\text {Eps}} ({\bf q}) $ 且 $ |N_{\text {Eps}}({\bf q})| \ge \text {MinPts} $ （即样本点 $ \bf q $ 是核心点）. 很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 $ \text {Eps} = 5 $，可以看到，样本点 $ \bf q $ 为核心点，样本点 $ \bf p $ 不是核心点，且 $ \bf p $ 在 $ \bf q $ 的 $ \text {Eps} $-邻域内，因而 $ \bf p $ 可由 $ \bf q $ 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。 定义3：（密度可达）我们称样本点 $ \bf p $ 是由样本点 $ \bf q $ 对于参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $ 密度可达的，如果存在一系列的样本点 $ {\bf p}_{1}, …, {\bf p}_n $（其中 $ {\bf p}_1 = {\bf q}, {\bf p}_n = {\bf p} $）使得对于 $ i = 1, …, n-1 $，样本点 $ {\bf p}_{i + 1} $ 可由样本点 $ {\bf p}_{i} $ 密度可达. 我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 cluster 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。 定义4：（密度相连）我们称样本点 $ \bf p $ 与样本点 $ \bf q $ 对于参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $ 是密度相连的，如果存在一个样本点 $ {\bf o} $，使得 $ \bf p $ 和 $ \bf q $ 均由样本点 $ \bf o $ 密度可达。 密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 cluster ，使得每一个 cluster 中的任意两个样本点都是密度相连的，且每一个 cluster 在密度可达的意义上都是最大的。cluster 的定义如下： 定义5：（cluster）假设数据集为 $ \bf X $，给定参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $，则某个 cluster $ C $ 是数据集 $ \bf X $ 的一个非空子集，且满足如下条件：1）对于任意的样本点 $ \bf p $ 和 $ \bf q $，如果 $ {\bf p} \in C $ 且 $ \bf q $ 可由 $ \bf p $ 密度可达，则 $ {\bf q} \in C $ .（最大性）2）对于 $ C $ 中的任意样本点 $ \bf p $ 和 $ \bf q $， $ \bf p $ 和 $ \bf q $ 关于参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $ 是密度相连的.（连接性） 这样我们就定义出了 DBSCAN 算法最终产生的 cluster 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 cluster 的样本点。根据以上定义，由于一个 cluster 中的任意两个样本点都是密度相连的，每一个 cluster 至少包含 $ \text {MinPts} $ 个样本点。 算法描述DBSCAN 算法就是为了寻找以上定义 5 中定义的 cluster，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 cluster，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 cluster，…，依此过程，直至没有未被聚类的核心点为止。依照 cluster 的“最大性”和“连接性”，在给定参数 $ \lbrace \text {Eps}, \text {MinPts} \rbrace $ 的情况下，最终产生的 cluster 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 cluster 的边缘的情况除外，这种情况下，这些临界位置的样本点的 cluster 归属与种子的选取顺序有关）。 合理地选取参数DBSCAN 的聚类结果和效果取决于参数 $ \text {Eps} $ 和 $ \text {MinPts} $ 以及距离衡量方法的选取。 由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 聚类分析（一）：层次聚类算法。 然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 cluster 所对应的 $ \text {Eps} $ 和 $ \text {MinPts} $。这里我们给出一种启发式的参数选取方法。假设 $ d $ 是某个样本点 $ \bf p $ 距离它的第 $ k $ 近邻的距离，则一般情况下 $ \bf p $ 的 $ d $-邻域内正好包含 $ k + 1 $ 个样本点。我们可以推断，在一个合理的 cluster 内，改变 $ k $ 的值不应该导致 $ d $ 值有较大的变化，除非 $ \bf p $ 的第 $ k $ 近邻们（$ k = 1, 2, 3,… $ ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 cluster。 因而我们一般将 $ k $ 的值固定下来，一个合理的选择是令 $ k = 3 $ 或 $ k = 4 $，那么 $ \text {MinPts} $ 的值也确定了（为 $ k + 1 $）；然后再来看每个样本点的 $ \text {k-dist} $ 距离（即该样本点距离它的第 $ k $ 近邻的距离）的分布情况，我们把每个样本点的 $ \text {k-dist} $ 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 $ \text {Eps} $，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 $ \text {k-dist} $ 距离作为 $ \text {Eps} $，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 $ \text {k-dist} $ 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 $ \text {k-dist} $ 距离作为 $ \text {k-dist} $ ，如下图中的图（b）所示。 算法的复杂度及其优缺点算法复杂度DBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 $ \text {Eps} $-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 $ O(N) $，其中 $ N $ 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 R* tree 建立索引时，查询邻域的平均时间复杂度为 $ O(\log N) $。因而，DBSCAN 算法的平均时间复杂度为 $ O(N\log N) $；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 $ O(N) $。 优缺点DBSCAN 算法有很多优点，总结如下： DBSCAN 不需要事先指定最终需要生成的 cluster 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 cluster 的数目，实际上我们对数据集是有相当多的了解的。 DBSCAN 可以找到具有任意形状的 cluster，如非凸的 cluster，这基于其对 cluster 的定义（cluster 是由密度低的区域所隔开的密度高的区域）。 DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。 DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 cluster 的边缘的情况，这种情况下，这些临界位置的样本点的 cluster 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。 DBSCAN 的运行速度快，当采用索引时，其复杂度仅为 $ O(N\log N) $。 当然，它也有一个主要缺点，即对于具有密度相差较大的 cluster 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 $ \text {MinPts} $ 和 $ \text {Eps} $ 是参照数据集中最稀疏的 cluster 所选取的，那么很有可能最终所有的样本最终都被归为一个 cluster，因为可能数据集中的 cluster 之间的区域的密度和最稀疏的 cluster 的密度相当；如果选取的参数 $ \text {MinPts} $ 和 $ \text {Eps} $ 倾向于聚出密度比较大的 cluster，那么极有可能，比较稀疏的这些 cluster 都被归为噪声。 OPTICS 算法一般被用来解决这一问题。 实现 DBSCAN 聚类现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 DBSCAN，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 min_pts 和 eps 以及距离衡量方法（默认为 euclidean），对数据集进行聚类时，我们对构造出来的实例调用方法 predict。predict 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 rtree 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import numpy as npimport timeimport matplotlib.pyplot as pltfrom shapely.geometry import Pointimport rtreeUNCLASSIFIED = -2NOISE = -1class DBSCAN(): def __init__(self, min_pts, eps, metric='euclidean', index_flag=True): self.min_pts = min_pts self.eps = eps self.metric = metric self.index_flag = index_flag self.data_set = None self.pred_label = None self.core_points = set() def predict(self, data_set): self.data_set = data_set self.n_samples, self.n_features = self.data_set.shape self.data_index = None self.dist_matrix = None start_time = time.time() if self.n_features == 2 and self.metric == 'euclidean' \ and self.index_flag: # 此种情形下对数据集建立空间索引 self.construct_index() else: # 其它情形下对数据集计算距离矩阵 self.cal_dist_matrix() self.pred_label = np.array([UNCLASSIFIED] * self.n_samples) # 开始 DBSCAN 聚类 crt_cluster_label = -1 for i in range(self.n_samples): if self.pred_label[i] == UNCLASSIFIED: query_result = self.query_eps_region_data(i) if len(query_result) &lt; self.min_pts: self.pred_label[i] = NOISE else: crt_cluster_label += 1 self.core_points.add(i) for j in query_result: self.pred_label[j] = crt_cluster_label query_result.discard(i) self.generate_cluster_by_seed(query_result, crt_cluster_label) print("time used: %.4f seconds" % (time.time() - start_time)) def construct_index(self): self.data_index = rtree.index.Index() for i in range(self.n_samples): data = self.data_set[i] self.data_index.insert(i, (data[0], data[1], data[0], data[1])) @staticmethod def distance(data1, data2, metric='euclidean'): if metric == 'euclidean': dist = np.sqrt(np.dot(data1 - data2, data1 - data2)) elif metric == 'manhattan': dist = np.sum(np.abs(data1 - data2)) elif metric == 'chebyshev': dist = np.max(np.abs(data1 - data2)) else: raise Exception("invalid or unsupported distance metric!") return dist def cal_dist_matrix(self): self.dist_matrix = np.zeros((self.n_samples, self.n_samples)) for i in range(self.n_samples): for j in range(i + 1, self.n_samples): dist = self.distance(self.data_set[i], self.data_set[j], self.metric) self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist def query_eps_region_data(self, i): if self.data_index: data = self.data_set[i] query_result = set() buff_polygon = Point(data[0], data[1]).buffer(self.eps) xmin, ymin, xmax, ymax = buff_polygon.bounds for idx in self.data_index.intersection((xmin, ymin, xmax, ymax)): if Point(self.data_set[idx][0], self.data_set[idx][1]).intersects(buff_polygon): query_result.add(idx) else: query_result = set(item[0] for item in np.argwhere(self.dist_matrix[i] &lt;= self.eps)) return query_result def generate_cluster_by_seed(self, seed_set, cluster_label): while seed_set: crt_data_index = seed_set.pop() crt_query_result = self.query_eps_region_data(crt_data_index) if len(crt_query_result) &gt;= self.min_pts: self.core_points.add(crt_data_index) for i in crt_query_result: if self.pred_label[i] == UNCLASSIFIED: seed_set.add(i) self.pred_label[i] = cluster_label 我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 sklearn 中实现的 DBSCAN 类对同一份数据集进行了 DBSCAN 聚类，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import make_blobsfrom sklearn.cluster import DBSCAN as DBSCAN_SKLEARNdef plot_clustering(X, y, core_pts_idx=None, title=None): if core_pts_idx is not None: core_pts_idx = np.array(list(core_pts_idx), dtype=int) core_sample_mask = np.zeros_like(y, dtype=bool) core_sample_mask[core_pts_idx] = True unique_labels = set(y) colors = [plt.cm.Spectral(item) for item in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: col = [0, 0, 0, 1] class_member_mask = (y == k) xy = X[class_member_mask &amp; core_sample_mask] plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=12, alpha=0.6) xy = X[class_member_mask &amp; ~core_sample_mask] plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6, alpha=0.6) else: plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6) if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout()# 构造数据集n_samples = 1500random_state = 170X, y = make_blobs(n_samples=n_samples, random_state=random_state)# 利用我自己手写的 DBSCAN 算法对数据集进行聚类dbscan_diy = DBSCAN(min_pts=20, eps=0.5)dbscan_diy.predict(X)n_clusters = len(set(dbscan_diy.pred_label)) - (1 if -1 in dbscan_diy.pred_label else 0)print("count of clusters generated: %s" % n_clusters)print("propotion of noise data for dbscan_diy: %.4f" % (np.sum(dbscan_diy.pred_label == -1) / n_samples))plt.subplot(1, 2, 1)plot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points, title="DBSCAN(DIY) Results")# 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类dbscan_sklearn = DBSCAN_SKLEARN(min_samples=20, eps=0.5)dbscan_sklearn.fit(X)print("propotion of noise data for dbscan_sklearn: %.4f" % (np.sum(dbscan_sklearn.labels_ == -1) / n_samples))plt.subplot(1, 2, 2)plot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_, title="DBSCAN(SKLEARN) Results")plt.show() 运行得到的输出和可视化结果如下所示：1234time used: 4.2602 secondscount of clusters generated: 3propotion of noise data for dbscan_diy: 0.1220propotion of noise data for dbscan_sklearn: 0.1220 上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。 我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：123456789101112131415161718from sklearn.datasets import make_circles, make_moonsfrom sklearn.preprocessing import StandardScalern_samples = 1500noisy_circles, _ = make_circles(n_samples=n_samples, factor=.5, noise=.05)noisy_circles = StandardScaler().fit_transform(noisy_circles)noisy_moons, _ = make_moons(n_samples=n_samples, noise=.05)noisy_moons = StandardScaler().fit_transform(noisy_moons)dbscan = DBSCAN(min_pts=5, eps=0.22)dbscan.predict(noisy_circles)plt.subplot(1, 2, 1)plot_clustering(noisy_circles, dbscan.pred_label, title="Concentric Circles Dataset")dbscan.predict(noisy_moons)plt.subplot(1, 2, 2)plot_clustering(noisy_moons, dbscan.pred_label, title="Interleaved Moons DataSet")plt.show() 运行的结果如下图所示： 可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>密度聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（三）：高斯混合模型与 EM 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 聚类分析系列文章。 高斯混合模型高斯混合模型简介高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示： p({\bf x}) = \sum_{k = 1}^{K} \pi_{k} {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k) \tag {1}如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 $ K $ 个类别中以 $ \pi_k $ 的概率随机选择一个类别 $ k $，然后再依照该类别所对应的高斯分布 $ {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k) $ 随机产生一个数据 $ {\bf x} $。但最终生成数据集后，我们所观察到的仅仅只是 $ {\bf x} $，而观察不到用于产生 $ {\bf x} $ 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 $ {\bf z} \in {\lbrace 0, 1 \rbrace}^{K} $ 来表示高斯混合模型中的类别变量，$ {\bf z} $ 中仅有一个元素的值为 $ 1 $，而其它元素的值为 $ 0 $，例如当 $ z_{k} = 1$ 时，表示当前数据是由高斯混合分布中的第 $ k $ 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 $ {\bf z} $ 的概率质量函数为 p(z_{k} = 1) = \pi_k \tag {2}其中 $ \lbrace \pi_{k} \rbrace $ 须满足 \sum_{k = 1}^{K} \pi_k = 1 \text{,} \ \ \ 0 \le \pi_k \le 1给定 $ {\bf z} $ 的值的情况下，$ {\bf x} $ 服从高斯分布 p({\bf x} | z_{k} = 1) = {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) \tag {3}因而可以得到 $ {\bf x} $ 的边缘概率分布为 p({\bf x}) = \sum_{\bf z} p({\bf z})p({\bf x} | {\bf z}) = \sum_{k =1}^{K} \pi_k {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) \tag {4}该分布就是我们前面所看到的高斯混合分布。 最大似然估计问题假设有数据集 $ \lbrace {\bf x}_1, {\bf x}_2, …, {\bf x}_N \rbrace $，其中样本的维度为 $ D $，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 $ {\bf X} = [ {\bf x}_1, {\bf x}_2, …, {\bf x}_N ]^{T} \in {\Bbb R}^{N \times D} $ 来表示数据集，以矩阵 $ {\bf Z} = [ {\bf z}_1, {\bf z}_2, …, {\bf z}_N ]^{T} \in {\Bbb R}^{N \times K} $ 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为 L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \ln \lbrace \sum_{k = 1}^{K} \pi_k {\cal N}({\bf x}_{n} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) \rbrace \tag {5}我们希望求解出使得以上对数似然函数最大的参数集 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 $ L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 发散的解。具体地，观察公式 $ (5) $，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 $ 0 $）的时候，会导致求和项中的该项的值为无穷大，从而也使得 $ L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 的值为无穷大；这样确实使得式 $ (5) $ 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。 EM 算法求解高斯混合模型EM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。 我们对对数似然函数 $ (5) $ 关于各参数 $ {\bf \pi} $， $ {\bf \mu} $， $ {\bf \Sigma} $ 分别求偏导，并将其置为 $ 0 $ 可以得到一系列的方程，而使得式 $ (5) $ 最大的解也一定满足这些方程。 首先令式 $ (5) $ 关于 $ {\bf \mu}_k $ 的偏导为 $ 0 $ 可得以下方程： \sum_{n = 1}^{N} \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} {\bf \Sigma}_k ({\bf x}_n - {\bf \mu}_k) = 0 \tag {6}注意到，上式中含有项 \gamma (z_{nk}) = \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = p(z_{nk} = 1 | {\bf x}_n) \tag {7}该项具有重要的物理意义，它为给定样本点 $ {\bf x}_n $ 后隐变量 $ {\bf z}_n $ 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 $ \gamma (z_{nk}) $ 的值来做软分配的。 因而，我们可以由式 $ (6) $ 和式 $ (7) $ 写出 {\bf \mu}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) {\bf x}_n \tag {8}其中 $ N_k = \sum_{n = 1}^{N} \gamma(z_{nk}) $，我们可以将 $ N_{k} $ 解释为被分配给类别 $ k $ 的有效样本数量，而 $ {\bf \mu}_{k} $ 即为所有样本点的加权算术平均值，每个样本点的权重等于第 $ k$ 个高斯分量对产生该样本点所负有的“责任”。 我们再将式 $ (5) $ 对 $ {\bf \Sigma}_k $ 的偏导数置为 $ 0 $ 可求得 {\bf \Sigma}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) ({\bf x}_n - {\bf \mu}_k)({\bf x}_n - {\bf \mu}_k)^{\text T} \tag {9}可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 $ \gamma(z_{nk}) $。 最后我们再来推导 $ \pi_k $ 的最大似然解须满足的条件。由于 $ \pi_k $ 有归一化的约束，我们可以利用 Lagrange 乘数法 来求解（将 $ \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) + \lambda (\sum_{k = 1}^{K} \pi_k - 1) $ 关于 $ \pi_k $ 的偏导数置 $ 0 $），最后可求得 \pi_k = \frac{N_k} {N} \tag {10}关于类别 $ k $ 的先验概率的估计值可以理解为所有样本点中被分配给第 $ k $ 个类别的有效样本点的个数占总样本数量的比例。 注意到，式 $ (8) $ 至 $ (10) $ 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 的闭式解，因为这些式子中给出的表达式中都含有 $ \gamma (z_{nk}) $，而 $ \gamma (z_{nk}) $ 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 $ (7) $）。 尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 固定为一个初始值，再按式 $ (7) $ 计算出隐变量的后验概率 $ \gamma (z_{nk}) $ （E 步）；然后再固定 $ \gamma (z_{nk}) $，按式 $ (8) $ 至 $ (10) $ 分别更新参数 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。 实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。 我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率 $ \gamma (z_{nk}) $，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 $ p_1 $ ，由红色的高斯分量产生的概率为 $ p_2 $，则我们将其染上 $ p_1 $ 比例的蓝色，染上 $ p_2 $ 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。 一般 EM 算法前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。 假设观测到的变量集为 $ {\bf X} $，隐变量集为 $ {\bf Z} $，模型中所涉及到的参数集为 $ {\bf \theta} $，我们的目的是最大化关于 $ {\bf X} $ 的似然函数 p({\bf X} | {\bf \theta}) = \sum_{\bf Z} p({\bf X}, {\bf Z} | {\bf \theta}) \tag {11}一般来讲，直接优化 $ p({\bf X} | {\bf \theta}) $ 是比较困难的，而优化完全数据集的似然函数 $ p({\bf X}, {\bf Z} | {\bf \theta}) $ 的难度则会大大减小，EM 算法就是基于这样的思路。 首先我们引入一个关于隐变量 $ {\bf Z} $ 的分布 $ q({\bf Z}) $，然后我们可以将对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 分解为如下 \ln p({\bf X} | {\bf \theta}) = {\cal L}(q, {\bf \theta}) + {\text KL}(q || p) \tag {12}其中 {\cal L}(q, {\bf \theta}) = \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{p({\bf X}, {\bf Z} | {\bf \theta})} {q({\bf Z})} \rbrace \tag {13} {\text KL}(q || p) = - \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{ p({\bf Z} | {\bf X}, {\bf \theta})} {q({\bf Z})} \rbrace \tag {14}其中 $ {\cal L}(q, {\bf \theta}) $ 为关于概率分布 $ q({\bf Z}) $ 的泛函，且为关于参数集 $ {\bf \theta} $ 的函数，另外，$ {\cal L}(q, {\bf \theta}) $ 的表达式中包含了关于完全数据集的似然函数 $ p({\bf X}, {\bf Z} | {\bf \theta}) $，这是我们需要好好加以利用的；$ {\text KL}(q || p) $ 为概率分布 $ q({\bf Z}) $ 与隐变量的后验概率分布 $ p({\bf Z} | {\bf X}, {\bf \theta}) $ 间的 KL 散度，它的值一般大于 $ 0 $，只有在两个概率分布完全相同的情况下才等于 $ 0 $，因而其一般被用来衡量两个概率分布之间的差异。 利用 $ {\text KL}(q || p) \ge 0 $ 的性质，我们可以得到 $ {\cal L}(q, {\bf \theta}) \le \ln p({\bf X} | {\bf \theta}) $，即 $ {\cal L}(q, {\bf \theta}) $ 是对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 的一个下界。 $ \ln p({\bf X} | {\bf \theta}) $ 与 $ {\cal L}(q, {\bf \theta}) $ 及 $ {\text KL}(q || p) $ 的关系可用下图中的图（a）来表示。 有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。 假设当前迭代步骤的参数的值为 $ {\bf \theta}^{\text {old}} $，我们先固定 $ {\bf \theta}^{\text {old}} $ 的值，来求 $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 关于概率分布 $ q({\bf Z}) $ 的最大值。可以看到，$ \ln p({\bf X} | {\bf \theta} ^{\text {old}}) $ 现在是一个定值，所以当 $ {\text KL}(q || p) $ 等于 $ 0 $ 时， $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 最大，如上图中的图（b）所示。此时由 $ {\text KL}(q || p) = 0 $ 可以推出，$ q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $。 现在再固定 $ q({\bf Z}) $，来求 $ {\cal L}(q, {\bf \theta}) $ 关于 $ {\bf \theta} $ 的最大值，假设求得的最佳 $ {\bf \theta} $ 的值为 $ {\bf \theta} ^{\text {new}} $，此时 $ {\cal L}(q, {\bf \theta} ^{\text {new}}) $ 相比 $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 的值增大了，而由于 $ {\bf \theta} $ 值的改变又使得当前 $ {\text KL}(q || p) $ 的值大于或等于 $ 0 $ （当算法收敛时保持 $ 0 $ 的值不变），所以根据式 $ (14) $，对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。 更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 $ p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $，我们将 $ q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $ 带入 $ {\cal L}(q, {\bf \theta}) $ 中，可得 \begin{aligned} {\cal L}(q, {\bf \theta}) &= \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta}) - \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old} }) \ln p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old} }) \\ & = {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) + {\text {const} } \end{aligned} \tag {15}我们只对上式中的第一项 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。 第二个迭代步骤被称为 M 步（Maximization），是因为要对式 $ (15) $ 中的 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 求解关于 $ {\bf \theta} $ 的最大值，由于 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 的形式相较于对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。 最后还是来总结一下 EM 算法的运行过程： 选择一个初始参数集 $ {\bf \theta}^{\text {old}} $； E 步，计算隐变量的后验概率函数 $ p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $； M 步，按下式计算 $ {\bf \theta}^{\text {new}} $ 的值 {\bf \theta}^{\text {new}} = \rm {arg} \rm {max}_{\bf \theta} {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old} }) \tag {16}其中 {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) = \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta}) \tag {17} 检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 $ {\bf \theta}^{\text {old}} = {\bf \theta}^{\text {new}} $ ，回到第 2 步继续迭代。 再探高斯混合模型在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 $ (7) $ 至 $ (10) $ ）。 推导过程在高斯混合模型中，参数集 $ {\bf \theta} = \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $，完整数据集 $ \lbrace {\bf X}, {\bf Z} \rbrace $ 的似然函数为 p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} {\pi_k}^{z_{nk}} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)^{z_{nk}} \tag {18}对其取对数可得 \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} z_{nk} \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace \tag {19}按照 EM 算法的迭代步骤，我们先求解隐变量 $ {\bf Z} $ 的后验概率函数，其具有如下形式 p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) \propto p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} ({\pi_k} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k))^{z_{nk}} \tag {20}再来推导完全数据集的对数似然函数在 $ p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 下的期望 {\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} {\Bbb E}[z_{nk}] \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace \tag {21}而 $ {\Bbb E}[z_{nk}] $ 的值可以根据式 $ (20) $ 求出，由于 $ z_{nk} $ 只可能取 $ 1 $ 或 $ 0 $，而取 $ 0 $ 时对期望没有贡献，故有 {\Bbb E}[z_{nk}] = p(z_{nk} = 1 | {\bf x}_n, {\bf \pi}, {\bf \mu}_k, {\bf \Sigma}_k) = \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = \gamma (z_{nk}) \tag {22}将上式代入公式 $ (21) $ 中，可得 {\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} \gamma (z_{nk}) \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace \tag {23}接下来我们就可以对该式关于参数 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 求解最大值了，可以验证，各参数的更新方程就是式 $ (8) $ 至 $ (10) $。 与 k-means 算法的关系敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。 首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 $ K $ 个高斯分量，但 k-means 算法做了如下假设： 假设每个高斯分量的先验概率相等，即 $ \pi_k = 1 / K $; 假设每个高斯分量的协方差矩阵均为 $ \epsilon {\bf I} $。 所以某一个高斯分量的概率密度函数为 p({\bf x} | {\bf \mu}_k, {\bf \Sigma}_k) = \frac {1} {(2\pi\epsilon)^{D/2}} \exp \lbrace -\frac {\| {\bf x} - {\bf \mu}_k \|^{2}} {2\epsilon} \rbrace \tag {24}故根据 EM 算法，可求得隐变量的后验概率函数为 \gamma(z_{nk}) = \frac{\pi_k \exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \pi_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace } = \frac{\exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace } \tag {25}在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 $ \gamma(z_{nk}) $ 的值最大的那个高斯分量，即有 r_{nk} = \begin{cases} 1, & \text {if \( k = \rm {arg} \rm {min}_{j} \| {\bf x}_n - {\bf \mu}_j \|^{2} \) } \\ 0, & \text {otherwise} \end{cases} \tag {26}由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 $ {\bf \mu}_k $ 求优，将式 $ (8) $ 中的 $ \gamma(z_{nk}) $ 替换为 $ r_{nk} $，即可得到 $ {\bf \mu}_k $ 的更新方法，与 k-means 算法中对中心点的更新方法一致。 现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 cluster、期待每个 cluster 中的样本数量相近、期待每个 cluster 的密度相近。 实现 GMM 聚类前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 $ \gamma(z_{nk}) $ 的值来对数据集进行聚类。$ \gamma(z_{nk}) $ 给出了样本 $ {\bf x}_n $ 是由 cluster $ k $ 产生的置信程度，最简单的 GMM 聚类即是将样本 $ {\bf x}_n $ 分配给 $ \gamma(z_{nk}) $ 值最大的 cluster。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 GaussianMixture 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。 利用 python 实现 GMM 聚类首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import numpy as npimport matplotlib.pyplot as pltimport timefrom sklearn.datasets import make_blobsclass GMMClust(): def __init__(self, n_components=2, max_iter=100, tol=1e-10): self.data_set = None self.n_components = n_components self.pred_label = None self.gamma = None self.component_prob = None self.means = None self.covars = None self.max_iter = max_iter self.tol = tol # 计算高斯分布的概率密度函数 @staticmethod def cal_gaussian_prob(x, mean, covar, delta=1e-10): n_dim = x.shape[0] covar = covar + delta * np.eye(n_dim) prob = np.exp(-0.5 * np.dot((x - mean).reshape(1, n_dim), np.dot(np.linalg.inv(covar), (x - mean).reshape(n_dim, 1)))) prob /= np.sqrt(np.linalg.det(covar) * ((2 * np.pi) ** n_dim)) return prob # 计算每一个样本点的似然函数 def cal_sample_likelihood(self, i): sample_likelihood = sum(self.component_prob[k] * self.cal_gaussian_prob(self.data_set[i], self.means[k], self.covars[k]) for k in range(self.n_components)) return sample_likelihood def predict(self, data_set): self.data_set = data_set self.n_samples, self.n_features = self.data_set.shape self.pred_label = np.zeros(self.n_samples, dtype=int) self.gamma = np.zeros((self.n_samples, self.n_components)) start_time = time.time() # 初始化各参数 self.component_prob = [1.0 / self.n_components] * self.n_components self.means = np.random.rand(self.n_components, self.n_features) for i in range(self.n_features): dim_min = np.min(self.data_set[:, i]) dim_max = np.max(self.data_set[:, i]) self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i] self.covars = np.zeros((self.n_components, self.n_features, self.n_features)) for i in range(self.n_components): self.covars[i] = np.eye(self.n_features) # 开始迭代 pre_L = 0 iter_cnt = 0 while iter_cnt &lt; self.max_iter: iter_cnt += 1 crt_L = 0 # E 步 for i in range(self.n_samples): sample_likelihood = self.cal_sample_likelihood(i) crt_L += np.log(sample_likelihood) for k in range(self.n_components): self.gamma[i, k] = self.component_prob[k] * \ self.cal_gaussian_prob(self.data_set[i], self.means[k], self.covars[k]) / sample_likelihood # M 步 effective_num = np.sum(self.gamma, axis=0) for k in range(self.n_components): self.means[k] = sum(self.gamma[i, k] * self.data_set[i] for i in range(self.n_samples)) self.means[k] /= effective_num[k] self.covars[k] = sum(self.gamma[i, k] * np.outer(self.data_set[i] - self.means[k], self.data_set[i] - self.means[k]) for i in range(self.n_samples)) self.covars[k] /= effective_num[k] self.component_prob[k] = effective_num[k] / self.n_samples print("iteration %s, current value of the log likelihood: %.4f" % (iter_cnt, crt_L)) if abs(crt_L - pre_L) &lt; self.tol: break pre_L = crt_L self.pred_label = np.argmax(self.gamma, axis=1) print("total iteration num: %s, final value of the log likelihood: %.4f, " "time used: %.4f seconds" % (iter_cnt, crt_L, time.time() - start_time)) # 可视化算法的聚类结果 def plot_clustering(self, kind, y=None, title=None): if kind == 1: y = self.pred_label plt.scatter(self.data_set[:, 0], self.data_set[:, 1], c=y, alpha=0.8) if kind == 1: plt.scatter(self.means[:, 0], self.means[:, 1], c='r', marker='x') if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout() 创建一个 GMMClust 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 predict 即可对给定的数据集进行 GMM 聚类；方法 plot_clustering 则可以可视化聚类的结果。利用 GMMClust 类进行 GMM 聚类的代码如下所示：12345678910111213141516171819# 生成数据集n_samples = 1500centers = [[0, 0], [5, 6], [8, 3.5]]cluster_std = [2, 1.0, 0.5]X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)# 运行 GMM 聚类算法gmm_cluster = GMMClust(n_components=3)gmm_cluster.predict(X)for i in range(3): print("cluster %s" % i) print(" mean: %s, covariance: %s" %(gmm_cluster.means[i], gmm_cluster.covars[i]))# 可视化数据集的原始类别情况以及算法的聚类结果plt.subplot(1, 2, 1)gmm_cluster.plot_clustering(kind=0, y=y, title='The Original Dataset')plt.subplot(1, 2, 2)gmm_cluster.plot_clustering(kind=1, title='GMM Clustering Result')plt.show() 以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：123456789101112131415161718192021222324252627iteration 1, current value of the log likelihood: -15761.9757iteration 2, current value of the log likelihood: -6435.3937iteration 3, current value of the log likelihood: -6410.5633iteration 4, current value of the log likelihood: -6399.4306iteration 5, current value of the log likelihood: -6389.0317iteration 6, current value of the log likelihood: -6377.9131iteration 7, current value of the log likelihood: -6367.5704iteration 8, current value of the log likelihood: -6359.2076iteration 9, current value of the log likelihood: -6350.8678iteration 10, current value of the log likelihood: -6338.6458... ...iteration 35, current value of the log likelihood: -5859.0324iteration 36, current value of the log likelihood: -5859.0324iteration 37, current value of the log likelihood: -5859.0324iteration 38, current value of the log likelihood: -5859.0324iteration 39, current value of the log likelihood: -5859.0324iteration 40, current value of the log likelihood: -5859.0324total iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 secondscluster 0 mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269] [0.08460269 3.95599185]]cluster 1 mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461 -0.00345936] [-0.00345936 0.9630804 ]]cluster 2 mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954] [0.01120954 0.27281129]] 可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 cluster 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 cluster 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。 利用 sklearn 实现 GMM 聚类sklearn 中的 GaussianMixture 类可以用来进行 GMM 聚类，其中的 fit 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；predict 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npimport matplotlib.pyplot as pltfrom sklearn.mixture import GaussianMixturefrom sklearn.datasets import make_blobsplt.figure(figsize=(12, 12))n_samples = 1500random_state = 170# 产生一个理想的数据集X, y = make_blobs(n_samples=n_samples, random_state=random_state)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X)y_pred = gmm.predict(X)plt.subplot(221)plt.scatter(X[:, 0], X[:, 1], c=y_pred)plt.title("Normal Blobs")# 产生一个非球形分布的数据集transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]X_aniso = np.dot(X, transformation)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_aniso)y_pred = gmm.predict(X_aniso)plt.subplot(222)plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)plt.title("Anisotropicly Distributed Blobs")# 产生一个各 cluster 的密度不一致的数据集X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_varied)y_pred = gmm.predict(X_varied)plt.subplot(223)plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)plt.title("Unequal Density Blobs")# 产生一个各 cluster 的样本数目不一致的数据集X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_filtered)y_pred = gmm.predict(X_filtered)plt.subplot(224)plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)plt.title("Unevenly Sized Blobs")plt.show() 运行结果如下图所示： 可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 cluster，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 cluster 都是超椭球形的。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>高斯混合模型</tag>
        <tag>GMM</tag>
        <tag>生成模型</tag>
        <tag>EM 算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（二）：k-means 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 聚类分析系列文章。 k-means 聚类算法k-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 聚类分析（一）：层次聚类算法 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 cluster 都维持一个中心点（为属于该 cluster 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 cluster。在该算法中，cluster 的数目 $ K $ 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 $ K $ 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 cluster 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。 表示为求解特定的优化问题假定数据集为 $ \lbrace {\bf x}_1, {\bf x}_2, …, {\bf x}_N \rbrace $，其包含 $ N $ 个样本点，每个样本点的维度为 $ D $。我们的目的是将该数据集划分为 $ K $ 个 cluster，其中 $ K $ 是一个预先给定的值。假设每个 cluster 的中心点为向量 $ {\bf \mu}_k \in \Bbb{R}^{d} $，其中 $ k = 1, …, K $。如前面所述，我们的目的是找到中心点 $ \lbrace {\bf \mu}_k \rbrace $，以及每个样本点所属的类别，以使每个样本点距其被分配的 cluster 所对应的中心点的平方 Euclidean 距离之和最小。 为方便用数学符号描述该优化问题，我们以变量 $ r_{nk} \in \lbrace 0, 1 \rbrace $ 来表示样本点 $ {\bf x}_n $ 是否被分配至第 $ k $ 个 cluster，若样本点 $ {\bf x}_n $ 被分配至第 $ k $ 个 cluster，则 $ r_{nk} = 1 $ 且 $ r_{nj} = 0 $ $ (j \neq k) $。由此我们可以写出目标函数 J = \sum_{n = 1}^{N} \sum_{k = 1}^{K} r_{nk} \| {\bf x}_n - {\bf \mu}_k \|^{2}它表示的即是每个样本点与其被分配的 cluster 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 $ \lbrace r_{nk} \rbrace $ 和 $ \lbrace {\bf \mu}_k \rbrace $ 的值，以使得目标函数 $ J $ 最小。 我们可以看到，由于 $ \lbrace r_{nk} \rbrace $ 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。 k-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 $ \lbrace {\bf \mu}_k \rbrace $ 选定初始值；然后在第一个迭代步骤中固定 $ \lbrace {\bf \mu}_k \rbrace $ 的值，对目标函数 $ J $ 根据 $ \lbrace r_{nk} \rbrace $ 求最小值；再在第二个迭代步骤中固定 $ \lbrace r_{nk} \rbrace $ 的值，对 $ J $ 根据 $ {\bf \mu}_k $ 求最小值；如此交替迭代，直至目标函数收敛。 考虑迭代过程中的两个优化问题。首先考虑固定 $ {\bf \mu}_k $ 求解 $ r_{nk} $ 的问题，可以看到 $ J $ 是关于 $ r_{nk} $ 的线性函数，因此我们很容易给出一个闭式解：$ J $ 包含 $ N $ 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，$ r_{nk} $ 的解为 r_{nk} = \begin{cases} 1, & \text {if $ k = \rm {arg} \rm {min}_{j} \| {\bf x}_n - {\bf \mu}_j \|^{2} $ } \\ 0, & \text {otherwise} \end{cases}从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 cluster。 再来考虑固定 $ r_{nk} $ 求解 $ {\bf \mu}_k $ 的问题，目标函数 $ J $ 是关于 $ {\bf \mu}_k $ 的二次函数，因此可以通过将 $ J $ 关于 $ {\bf \mu}_k $ 的导数置为 0 来求解 $ J $ 关于 $ {\bf \mu}_k $ 的最小值： 2 \sum_{n = 1}^{N} r_{nk}({\bf x}_n - {\bf \mu}_k) = 0容易求出 $ {\bf \mu}_k $ 的值为 {\bf \mu}_k = \frac {\sum_{n} r_{nk} {\bf x}_n} {\sum_{n} r_{nk} }该式表明，这一步迭代是将中心点更新为所有被分配至该 cluster 的样本点的均值。 k-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 $ J $ 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。 虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程： 初始化每个 cluster 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果； 将每个样本点分配给距离其最近的中心点所对应的 cluster； 更新每个 cluster 的中心点为被分配给该 cluster 的所有样本点的均值； 交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。 PRML 教材 中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 $ K = 2 $，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。 算法复杂度及其优缺点算法复杂度k-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 $ O(NK) $ 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 $ O(D) $，其中 $ N $ 为样本点的个数，$ K $ 为指定的聚类个数，$ D $ 为样本点的维度；因此，一次迭代过程的时间复杂度为 $ O(NKD) $，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 $ O(NKD) $。 实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。 缺点k-means 算法虽简单易用，但其有一些很明显的缺点，总结如下： 由于其假设每个 cluster 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 cluster；但事实上的数据的 cluster 有可能并不是如此。 由于其假设每一个 cluster 的分布形状都为球形（spherical），（“球形分布”表明一个 cluster 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 cluster ，对于具有非凸的或者形状很特别的 cluster 的数据集，其聚类效果往往很差。 由于其假设不同的 cluster 具有相似的密度，因此对于具有密度差别较大的 cluster 的数据集，其聚类效果不好。 其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。 cluster 的数目 $ K $ 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 $ K $ 值也是一个问题。一般确定 $ K $ 的值的方法有以下几种：a）选定一个 $ K $ 的区间，例如 2～10，对每一个 $ K $ 值分别运行多次 k-means 算法，取目标函数 $ J $ 的值最小的 $ K $ 作为聚类数目；b）利用 Elbow 方法 来确定 $ K $ 的值；c）利用 gap statistics 来确定 $ K $ 的值；d）根据问题的目的和对数据的粗略了解来确定 $ K $ 的值。 其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 k-means++ 算法 ）来克服此问题。 其仅适用于数值类型的样本。但其扩展算法 k-modes 算法 （专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。 其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 cluster，而关于 cluster 的其他信息一概没有做限制，那么根据 Occam 剃刀原理 ，k-means 算法中的 cluster 应是最简单的那一种，即对应这三个假设。在博文 聚类分析（三）：高斯混合模型与 EM 算法 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。 优点尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下： 实现起来简单，总是可以收敛，算法复杂度低。 其产生的聚类结果容易阐释。 在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。 实现 k-means 聚类在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 KMeans 类来展示 k-means 算法对不同类型的数据集的聚类效果。 利用 python 实现 k-means 聚类首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport matplotlib.pyplot as pltimport copyimport timefrom sklearn.datasets import make_blobsclass KMeansClust(): def __init__(self, n_clust=2, max_iter=50, tol=1e-10): self.data_set = None self.centers_his = [] self.pred_label = None self.pred_label_his = [] self.n_clust = n_clust self.max_iter = max_iter self.tol = tol def predict(self, data_set): self.data_set = data_set n_samples, n_features = self.data_set.shape self.pred_label = np.zeros(n_samples, dtype=int) start_time = time.time() # 初始化中心点 centers = np.random.rand(self.n_clust, n_features) for i in range(n_features): dim_min = np.min(self.data_set[:, i]) dim_max = np.max(self.data_set[:, i]) centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i] self.centers_his.append(copy.deepcopy(centers)) self.pred_label_his.append(copy.deepcopy(self.pred_label)) print("The initializing cluster centers are: %s" % centers) # 开始迭代 pre_J = 1e10 iter_cnt = 0 while iter_cnt &lt; self.max_iter: iter_cnt += 1 # E 步：将各个样本点分配给距其最近的中心点所对应的 cluster for i in range(n_samples): self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** 2, axis=1)) # M 步：更新中心点 for i in range(self.n_clust): centers[i] = np.mean(self.data_set[self.pred_label == i], axis=0) self.centers_his.append(copy.deepcopy(centers)) self.pred_label_his.append(copy.deepcopy(self.pred_label)) # 重新计算目标函数 J crt_J = np.sum((self.data_set - centers[self.pred_label]) ** 2) / n_samples print("iteration %s, current value of J: %.4f" % (iter_cnt, crt_J)) # 若前后两次迭代产生的目标函数的值变化不大，则结束迭代 if np.abs(pre_J - crt_J) &lt; self.tol: break pre_J = crt_J print("total iteration num: %s, final value of J: %.4f, time used: %.4f seconds" % (iter_cnt, crt_J, time.time() - start_time)) # 可视化算法每次迭代产生的结果 def plot_clustering(self, iter_cnt=-1, title=None): if iter_cnt &gt;= len(self.centers_his) or iter_cnt &lt; -1: raise Exception("iter_cnt is not valid!") plt.scatter(self.data_set[:, 0], self.data_set[:, 1], c=self.pred_label_his[iter_cnt], alpha=0.8) plt.scatter(self.centers_his[iter_cnt][:, 0], self.centers_his[iter_cnt][:, 1], c='r', marker='x') if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout() 创建一个 KMeansClust 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 predict 即可对给定的数据集进行 k-means 聚类；方法 plot_clustering 则可以可视化每一次迭代所产生的结果。利用 KMeansClust 类进行 k-means 聚类的代码如下所示：123456789101112131415161718if __name__ == '__main__': # 生成数据集 n_samples = 1500 centers = [[0, 0], [5, 6], [8, 3.5]] cluster_std = [2, 1.0, 0.5] X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std) # 运行 k-means 算法 kmeans_cluster = KMeansClust(n_clust=3) kmeans_cluster.predict(X) # 可视化中心点的初始化以及算法的聚类结果 plt.subplots(1, 2) plt.subplot(1, 2, 1) kmeans_cluster.plot_clustering(iter_cnt=0, title='initialization centers') plt.subplot(1, 2, 2) kmeans_cluster.plot_clustering(iter_cnt=-1, title='k-means clustering result') plt.show() 以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：123456789101112131415161718192021222324The initializing cluster centers are: [[-6.12152378 2.14971475] [ 6.71575768 -5.41421872] [-1.30016464 -2.3824513 ]]iteration 1, current value of J: 12.5459iteration 2, current value of J: 7.3479iteration 3, current value of J: 5.2928iteration 4, current value of J: 5.1493iteration 5, current value of J: 5.1152iteration 6, current value of J: 5.1079iteration 7, current value of J: 5.1065iteration 8, current value of J: 5.1063iteration 9, current value of J: 5.1052iteration 10, current value of J: 5.0970iteration 11, current value of J: 5.0592iteration 12, current value of J: 4.9402iteration 13, current value of J: 4.5036iteration 14, current value of J: 3.6246iteration 15, current value of J: 3.2003iteration 16, current value of J: 3.1678iteration 17, current value of J: 3.1658iteration 18, current value of J: 3.1657iteration 19, current value of J: 3.1657total iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds 可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。 利用 sklearn 实现 k-means 聚类sklearn 中的 KMeans 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 demo 来展示 KMeans 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansfrom sklearn.datasets import make_blobsplt.figure(figsize=(12, 12))n_samples = 1500random_state = 170X, y = make_blobs(n_samples=n_samples, random_state=random_state)# 设定一个不合理的 K 值y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)plt.subplot(221)plt.scatter(X[:, 0], X[:, 1], c=y_pred)plt.title("Incorrect Number of Blobs")# 产生一个非球形分布的数据集transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]X_aniso = np.dot(X, transformation)y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)plt.subplot(222)plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)plt.title("Anisotropicly Distributed Blobs")# 产生一个各 cluster 的密度不一致的数据集X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)plt.subplot(223)plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)plt.title("Unequal Variance")# 产生一个各 cluster 的样本数目不一致的数据集X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)plt.subplot(224)plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)plt.title("Unevenly Sized Blobs")plt.show() 运行结果如下图所示： 上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 $ K $ 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 cluster 的密度相近的假设条件；第四个数据集则破坏了各 cluster 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>k-means 算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（一）：层次聚类算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 聚类分析系列文章。 聚类算法综述聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 cluster，使得同一 cluster 内的对象在某种意义上比不同的 cluster 之间的对象更为相似。 由于 “cluster” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类： 基于连通模型（connectivity-based）的聚类算法： 即本文将要讲述的层次聚类算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 cluster。 基于中心点模型（centroid-based）的聚类算法： 在此类算法中，每个 cluster 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 k 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 cluster 的中心点的距离的平方和，优化变量为每个 cluster 的中心点以及每个对象属于哪个 cluster；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解， k-means 算法 即是其中的一种。 基于分布模型（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 cluster 即可，最常被使用的此类算法为 高斯混合模型（GMM）聚类。 基于密度（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 cluster，cluster 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。 层次聚类综述层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 clusters，后面一层生成的 clusters 基于前面一层的结果。层次聚类算法一般分为两类： Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。本文主要关注此类算法。 Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。 下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。 另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。 树形图 树形图 （dendrogram）可以用来直观地表示层次聚类的成果。一个有 5 个点的树形图如下图所示，其中纵坐标高度表示不同的 cluster 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，$ x_1 $ 和 $ x_2 $ 的距离最近（为 1），因此将 $ x_1 $ 和 $ x_2 $ 合并为一个 cluster $ (x_1, x_2) $，所以在树形图中首先将节点 $ x_1 $ 和 $ x_2 $ 连接，使其成为一个新的节点 $ (x_1, x_2) $ 的子节点，并将这个新的节点的高度置为 1；之后再在剩下的 4 个 cluster $ (x_1, x_2) $， $ x_3 $， $ x_4 $ 和 $ x_5 $ 中选取距离最近的两个 cluster 合并，$ x_4 $ 和 $ x_5 $ 的距离最近（为 2），因此将 $ x_4 $ 和 $ x_5 $ 合并为一个 cluster $ (x_4, x_5) $，体现在树形图上，是将节点 $ x_4 $ 和 $ x_5 $ 连接，使其成为一个新的节点 $ (x_4, x_5) $ 的子节点，并将此新节点的高度置为 2；….依此模式进行树形图的生成，直至最终只剩下一个 cluster $ ((x_1, x_2), x_3), (x_4, x_5)) $。 可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 cluster 之间的距离都不大于 $ h $，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 $ h $，即可获得对应的聚类结果。例如，在下面的树形图中，设 $ h=2.5 $，即可得到 3 个 cluster $ (x_1, x_2) $， $ x_3 $ 和 $ (x_4, x_5) $。 对象之间的距离衡量衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 $ d $ 维空间的两个对象 ${\bf x} =[ x_1, x_2, …, x_d]^{T} $ 和 ${\bf y} = [y_1, y_2, …, y_d]^{T}$，其在不同距离准则下的距离计算方法如下表所示: 距离准则 距离计算方法 Euclidean 距离 $ d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{2} ]^{\frac{1}{2}} = [({\bf x} - {\bf y})^{T} ({\bf x} - {\bf y})]^{\frac{1}{2}} $ Manhattan 距离 $ d({\bf x},{\bf y}) = \sum_{j=1}^{d} \mid{x_j-y_j}\mid $ Chebyshev 距离 $ d({\bf x},{\bf y}) = \max_{1\leq{j}\leq{d}} \mid{x_j-y_j}\mid $ Minkowski 距离 $ d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{p} ]^{\frac{1}{p}}, p\geq{1} $ Minkowski 距离就是 $ \rm{L}\it{p}$ 范数（$ p\geq{1} $)，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 $ p = 1, 2, \infty $ 时的情形。 另一种常用的距离是 Maholanobis 距离，其定义如下： d_{mah}({\bf x}, {\bf y}) = \sqrt{({\bf x} - {\bf y})^{T}\Sigma^{-1} ({\bf x} - {\bf y})}其中 $ \Sigma $ 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 $ {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) \in \Bbb{R}^{d \times n}$，$ {\bf x}_i \in \Bbb{R}^{d} $ 为第 $ i $ 个样本点，每个样本点的维度为 $ d $，样本点的总数为 $ n $ 个；再假设样本点的平均值 $ m_{\bf x} = \frac{1}{n}\sum_{i=1}^{n} {\bf x}_i $ 为 $ {\bf 0} $ 向量（若不为 $ {\bf 0} $，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 $ \Sigma \in \Bbb{R}^{d \times d} $ 可被定义为 \Sigma = \frac{1}{n} {\bf X}{\bf X}^{T}Maholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。 Cluster 之间的距离衡量除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 cluster 之间的距离，常见的 cluster 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，cluster 是由样本点或一些子 cluster （这些样本点或子 cluster 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 cluster 。假设 $ C_i $ 和 $ C_j $ 为两个 cluster，则前四种方法定义的 $ C_i $ 和 $ C_j $ 之间的距离如下表所示： 方法 定义 Single-link $ D(C_i, C_j) = \min_{ {\bf x} \in C_i, {\bf y} \in C_j } d({\bf x}, {\bf y}) $ Complete-link $ D(C_i, C_j) = \max_{ {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) $ UPGMA $ D(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) $ WPGMA omitting 其中 Single-link 定义两个 cluster 之间的距离为两个 cluster 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 cluster；而 Complete-link 则定义两个 cluster 之间的距离为两个 cluster 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 cluster 之间的距离为两个 cluster 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 cluster 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 cluster 对距离的计算的影响在同一层次上，而不受 cluster 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 cluster 之间的距离，而是通过已有的 cluster 之间的距离来计算合并后的新的 cluster 和剩余 cluster 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。 Centroid/UPGMC 方法给每一个 cluster 计算一个质心，两个 cluster 之间的距离即为对应的两个质心之间的距离，一般计算方法如下： D_{\rm{UPGMC}}(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y}\in C_j} d({\bf x}, {\bf y}) - \frac{1}{2{\mid C_i \mid \}^{2}} \sum_ { {\bf x}, {\bf y}\in C_i} d( {\bf x}, {\bf y}) - \frac{1}{2{\mid \it{C_j} \mid \}^{2}} \sum_ { {\bf x}, {\bf y} \in C_j} d( {\bf x}, {\bf y})当上式中的 $ d(.,.) $ 为平方 Euclidean 距离时，$ D_{\rm{UPGMC}}(C_i, C_j) $ 为 $ C_i $ 和 $ C_j $ 的中心点（每个 cluster 内所有样本点之间的平均值）之间的平方 Euclidean 距离。 Median/WPGMC 方法为每个 cluster 计算质心时，引入了权重。 Ward 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 cluster 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，cluster $ C $ 的 $ {\rm ESS} $ 定义如下： {\rm ESS} ( C ) = \sum_{ {\bf x} \in C} ({\bf x} - m_{\bf x})^{T} ({\bf x} - m_{\bf x})其中 $ m_{\bf x} $ 为 $ C $ 中样本点的均值。可以看到 $ {\rm ESS} $ 衡量的是一个 cluster 内的样本点的聚合程度，样本点越聚合，$ {\rm ESS} $ 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 cluster 的 $ {\rm ESS} $ 之和相对于合并前的 cluster 的 $ {\rm ESS} $ 之和的增长最小。 Agglomerative 层次聚类算法这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 cluster 之间的距离的 Lance-Williams 方法。 Lance-Williams 方法在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 cluster 合并为一个新的 cluster，然后再计算这个新的 cluster 与其他当前未被合并的 cluster 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 cluster 距离衡量方法都适用。具体地，对于三个 cluster $ C_k $，$ C_i $ 和 $ C_j $， Lance-Williams 给出的 $ C_k $ 与 $ C_i $ 和 $ C_j $ 合并后的新 cluster 之间的距离的计算方法如下式所示： D(C_k, C_i \cup C_j) = \alpha_i D(C_k, C_i) + \alpha_j D(C_k, C_j) + \beta D(C_i, C_j) + \gamma \mid D(C_k, C_i) - D(C_k, C_j) \mid其中，$ \alpha_i $，$ \alpha_j $，$ \beta $，$ \gamma $ 均为参数，随 cluster 之间的距离计算方法的不同而不同，具体总结为下表（注：$ n_i $ 为 cluster $ C_i $ 中的样本点的个数)： 方法 参数 $ \alpha_i $ 参数 $ \alpha_j $ 参数 $ \beta $ 参数 $ \gamma $ Single-link $ 1/2 $ $ 1/2 $ $ 0 $ $ -1/2 $ Complete-link $ 1/2 $ $ 1/2 $ $ 0 $ $ 1/2 $ UPGMA $ n_i/(n_i + n_j) $ $ n_j/(n_i + n_j) $ $ 0 $ $ 0 $ WPGMA $ 1/2 $ $ 1/2 $ $ 0 $ $ 0 $ UPGMC $ n_i/(n_i + n_j) $ $ n_j/(n_i + n_j) $ $ n_{i}n_{j}/(n_i + n_j)^{2} $ $ 0 $ WPGMC $ 1/2 $ $ 1/2 $ $ 1/4 $ $ 0 $ Ward $ (n_k + n_i)/(n_i + n_j + n_k) $ $ (n_k + n_j)/(n_i + n_j + n_k) $ $ n_k/(n_i + n_j + n_k) $ $ 0 $ 其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。 Naive 算法给定数据集 $ {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) $，Agglomerative 层次聚类最简单的实现方法分为以下几步： 初始时每个样本为一个 cluster，计算距离矩阵 $ \bf D $，其中元素 $ D_{ij} $ 为样本点 $ {\bf x}_i $ 和 $ {\bf x}_j $ 之间的距离； 遍历距离矩阵 $ \bf D $，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 cluster 的编号，将这两个 cluster 合并为一个新的 cluster 并依据 Lance-Williams 方法更新距离矩阵 $ \bf D $ （删除这两个 cluster 对应的行和列，并把由新 cluster 所算出来的距离向量插入 $ \bf D $ 中），存储本次合并的相关信息； 重复 2 的过程，直至最终只剩下一个 cluster 。 当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。 可以看到，该 Naive 算法的时间复杂度为 $ O(n^3) $ （由于每次合并两个 cluster 时都要遍历大小为 $ O(n^2) $ 的距离矩阵来搜索最小距离，而这样的操作需要进行 $ n - 1 $ 次），空间复杂度为 $ O(n^2) $ （由于要存储距离矩阵）。 当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 $ O(n^2 ) $ 或更低，例如 SLINK 算法（Single-link 方法），CLINK 算法（Complete-link 方法），BIRCH 算法（适用于 Euclidean 距离准则）等等。 利用 Scipy 实现层次聚类在这里我们将利用 SciPy（python 中的一个用于数值分析和科学计算的第三方包，功能强大，NumPy+SciPy+matplotlib 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。 生成实验样本集首先，我们需要导入相关的模块，代码如下所示：12345# python 3.6&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from scipy.cluster.hierarchy import linkage, dendrogram, fcluster&gt;&gt;&gt; from sklearn.datasets.samples_generator import make_blobs&gt;&gt;&gt; import matplotlib.pyplot as plt 其中 make_blobs 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 cluster 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 X 的维度为 n*d。1234567&gt;&gt;&gt; centers = [[1, 1], [-1, -1], [1, -1]] # 定义 3 个中心点# 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别&gt;&gt;&gt; X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0） &gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c='b')&gt;&gt;&gt; plt.title('The dataset')&gt;&gt;&gt; plt.show() 样本的分布如下图所示。 进行 Agglomerative 层次聚类SciPy 里面进行层次聚类非常简单，直接调用 linkage 函数，一行代码即可搞定。1&gt;&gt;&gt; Z = linkage(X, method='ward', metric='euclidean') 以上即进行了一次 cluster 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 method 参数可以为 &#39;single&#39;、 &#39;complete&#39; 、&#39;average&#39;、 &#39;weighted&#39;、 &#39;centroid&#39;、 &#39;median&#39;、 &#39;ward&#39; 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 metric 参数调整。 linkage 函数的返回值 Z 为一个维度 (n-1)*4 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 4 个值分别对应合并的两个 cluster 的序号、两个 cluster 之间的距离以及本次合并后产生的新的 cluster 所包含的样本点的个数；具体地，对于第 i 次迭代（对应 Z 的第 i 行），序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 合并产生新的 cluster n + i, Z[i, 2] 为序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 之间的距离，合并后的 cluster 包含 Z[i, 3] 个样本点。 例如本次实验中 Z 记录到的前 25 次合并的信息如下所示：12345678910111213141516171819202122232425262728&gt;&gt;&gt; print(Z.shape)(749, 4)&gt;&gt;&gt; print(Z[: 25])[[253. 491. 0.00185 2. ] [452. 696. 0.00283 2. ] [ 70. 334. 0.00374 2. ] [237. 709. 0.00378 2. ] [244. 589. 0.00423 2. ] [141. 550. 0.00424 2. ] [195. 672. 0.00431 2. ] [ 71. 102. 0.00496 2. ] [307. 476. 0.00536 2. ] [351. 552. 0.00571 2. ] [ 62. 715. 0.00607 2. ] [ 98. 433. 0.0065 2. ] [255. 572. 0.00671 2. ] [437. 699. 0.00685 2. ] [ 55. 498. 0.00765 2. ] [143. 734. 0.00823 2. ] [182. 646. 0.00843 2. ] [ 45. 250. 0.0087 2. ] [298. 728. 0.00954 2. ] [580. 619. 0.01033 2. ] [179. 183. 0.01062 2. ] [101. 668. 0.01079 2. ] [131. 544. 0.01125 2. ] [655. 726. 0.01141 2. ] [503. 756. 0.01265 3. ]] 从上面的信息可以看到，在第 6 次合并中，样本点 141 与样本点 550 进行了合并，生成新 cluster 756；在第 25 次合并中，样本点 503 与 cluster 756 进行了合并，生成新的 cluster 770。我们可以将样本点 141、550 和 503 的特征信息打印出来，来看看它们是否确实很接近。1234&gt;&gt;&gt; print(X[[141, 550, 503]])[[ 1.27098 -0.97927] [ 1.27515 -0.98006] [ 1.37274 1.13599]] 看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 cluster 中相隔最近的。 画出树形图SciPy 中给出了根据层次聚类的结果 Z 绘制树形图的函数 dendrogram，我们由此画出本次实验中的最后 20 次的合并过程。123456&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; dendrogram(Z, truncate_mode='lastp', p=20, show_leaf_counts=False, leaf_rotation=90, leaf_font_size=15, show_contracted=True)&gt;&gt;&gt; plt.title('Dendrogram for the Agglomerative Clustering')&gt;&gt;&gt; plt.xlabel('sample index')&gt;&gt;&gt; plt.ylabel('distance')&gt;&gt;&gt; plt.show() 得到的树形图如下所示。 可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 3 个 cluster（和实际相符），分别在上图中由三种颜色所表示。 获取聚类结果在得到了层次聚类的过程信息 Z 后，我们可以使用 fcluster 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 d，得到在该距离以下的未合并的所有 cluster 作为聚类结果；另一个是指定 cluster 的数量 k，函数会返回最后的 k 个 cluster 作为聚类结果。使用哪个维度由参数 criterion 决定，对应的临界距离或聚类的数量则由参数 t 所记录。fcluster 函数的结果为一个一维数组，记录每个样本的类别信息。 对应的代码与返回结果如下所示。123456789101112131415161718# 根据临界距离返回聚类结果&gt;&gt;&gt; d = 15&gt;&gt;&gt; labels_1 = fcluster(Z, t=d, criterion='distance')&gt;&gt;&gt; print(labels_1[: 100]) # 打印聚类结果[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; print(len(set(labels_1))) # 看看在该临界距离下有几个 cluster3 # 根据聚类数目返回聚类结果&gt;&gt;&gt; k = 3&gt;&gt;&gt; labels_2 = fcluster(Z, t=k, criterion='maxclust')&gt;&gt;&gt; print(labels_2[: 100]) [2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; list(labels_1) == list(labels_2) # 看看两种不同维度下得到的聚类结果是否一致True 下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。1234&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.title('The Result of the Agglomerative Clustering')&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c=labels_2, cmap='prism')&gt;&gt;&gt; plt.show() 可视化结果如下图所示。 上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。 比较不同方法下的聚类结果最后，我们对同一份样本集进行了 cluster 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 3，程序如下：1234567891011121314151617181920212223242526272829303132333435363738from time import timeimport numpy as npfrom scipy.cluster.hierarchy import linkage, fclusterfrom sklearn.datasets.samples_generator import make_blobsfrom sklearn.metrics.cluster import adjusted_mutual_info_scoreimport matplotlib.pyplot as plt# 生成样本点centers = [[1, 1], [-1, -1], [1, -1]]X, labels = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)# 可视化聚类结果def plot_clustering(X, labels, title=None): plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism') if title is not None: plt.title(title, size=17) plt.axis('off') plt.tight_layout()# 进行 Agglomerative 层次聚类linkage_method_list = ['single', 'complete', 'average', 'ward']plt.figure(figsize=(10, 8))ncols, nrows = 2, int(np.ceil(len(linkage_method_list) / 2))plt.subplots(nrows=nrows, ncols=ncols)for i, linkage_method in enumerate(linkage_method_list): print('method %s:' % linkage_method) start_time = time() Z = linkage(X, method=linkage_method) labels_pred = fcluster(Z, t=3, criterion='maxclust') print('Adjust mutual information: %.3f' % adjusted_mutual_info_score(labels, labels_pred)) print('time used: %.3f seconds' % (time() - start_time)) plt.subplot(nrows, ncols, i + 1) plot_clustering(X, labels_pred, '%s linkage' % linkage_method)plt.show() 可以得到 4 种方法下的聚类结果如下图所示。 在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 AMI（Adjust Mutual Information）量，该量越接近于 1 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下： 123456789101112method single:Adjust mutual information: 0.001time used: 0.008 secondsmethod complete:Adjust mutual information: 0.838time used: 0.013 secondsmethod average:Adjust mutual information: 0.945time used: 0.019 secondsmethod ward:Adjust mutual information: 0.956time used: 0.015 seconds 从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 cluster，而其他两个 cluster 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>层次聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的 Unicode String 和 Byte String]]></title>
    <url>%2FPython-%E4%B8%AD%E7%9A%84-Unicode-string-%E5%92%8C-Byte-string%2F</url>
    <content type="text"><![CDATA[python 2.x 和 python 3.x 字符串类型的区别python 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。 python 2.x str 类型：处理 binary 数据和 ASCII 文本数据。 unicode 类型：处理非 ASCII 文本数据。 python 3.x bytes 类型：处理 binary 数据，同 str 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。 str 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。 bytearray 类型：bytes 类型的变种，但是此类型是 mutable 的。 Unicode 简介包括 ASCII 码、latin-1 编码 和 utf-8 编码 等在内的码都被认为是 unicode 码。 编码和解码的概念 编码（encoding）：将字符串映射为一串原始的字节。 解码（decoding）：将一串原始的字节翻译成字符串。 ASCII码 编码长度为 1 个 byte. 编码范围为 0x00~0x7F，只包含一些常见的字符。 latin-1码 编码长度为 1 个 byte. 编码范围为 0x00~0xFF，能支持更多的字符（如 accent character），兼容 ASCII 码。 utf-8码 编码长度可变，为 1~4 个 byte。 当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 0x00 ~ 0x7F；当编码长度大于 1 个 byte 时，每个 byte 的取值为 0x80 ~ 0xFF。 其它编码 utf-16，编码长度为定长 2 个 byte。 utf-32，编码长度为定长 4 个 byte。 Unicode 字符串的存储方式在内存中的存储方式unicode 字符串中的字符在内存中以一种与编码方式无关的方式存储：unicode code point，它是一个数字，范围为 0~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，a 和 b 表示的是同一字符串（其中 &#39;\uNNNN&#39; 即为 unicode code point，N 为一个十六进制位，十六进制位的个数为 4~6 位；当 unicode code point 的取值在 0~255 范围内时，也可以 &#39;\xNN&#39; 的形式表示）：12345678# python 2.7&gt;&gt;&gt; a = u'\u5a1c\u5854\u838e'&gt;&gt;&gt; b = u'娜塔莎'&gt;&gt;&gt; print a, b娜塔莎 娜塔莎&gt;&gt;&gt; c = u'\xe4'&gt;&gt;&gt; print cä 在文件等外部媒介中的存储方式unicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。 字符表示python 3.x在 python 3.x 中，str 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：123456# python 3.6&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; len(a)12 可以看到，python 3.x 中得到的 a 的长度为 12（包含空格），没有任何问题；我们可以对 a 进行编码，将其转换为 bytes 类型：123456# python 3.6&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; bb'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)bytes 从上面可以看出，bytes 类型的对象中的某个字节的取值在 0x00 ~ 0x7F 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。 同理，我们也可以将一个 bytes 类型的对象译码为一个 str 类型的对象：1234# python 3.6&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; a'Natasha, 娜塔莎' python 2.x在 python 2.x 中，如果还是用 str 类型来表示国际字符，就会有问题：12345678910# python 2.7&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; a'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; len(a)18&gt;&gt;&gt; print aNatasha, 娜塔莎 可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 str 字符串，但实际上 a 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 a 正好是字符串 &#39;Natasha, 娜塔莎&#39; 的 utf-8 编码的结果，且将 a 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:123456# python 2.7&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdin.encoding # 控制台的输入编码，可解释前例中 a 在内存中的表现形式'utf-8'&gt;&gt;&gt; sys.stdout.encoding # 控制台的输出编码，可解释前例中打印 a 的显示结果'utf-8' 另外，sys.getdefaultencoding()函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 &#39;ascii&#39;, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 json.dumps() 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：>）。 切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 unicode 类型字符串，如下所示：1234567891011121314# python 2.7&gt;&gt;&gt; a = u'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)unicode&gt;&gt;&gt; len(a)12&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; b'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)str&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; au'Natasha, \u5a1c\u5854\u838e' 另外，我们可以对 unicode 类型字符串进行编码操作，对 str 类型字符串进行译码操作。 文本文件操作python 3.x在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 open 函数的参数 encoding 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 sys.getfilesystemencoding()得到，如：12345678910111213# python 3.6&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)3&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.read()'娜塔莎'&gt;&gt;&gt; f.close() 当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：12345678910# python 3.6&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'wb')&gt;&gt;&gt; f.write(b)9&gt;&gt;&gt; f.close()&gt;&gt;&gt; f.read().decode('utf-8')'娜塔莎'&gt;&gt;&gt; f.close() python 2.x在 python 2.x 中，open 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 str；codecs.open 函数则支持自动读写 unicode 文本文件，如：12345678910# python 2.7&gt;&gt;&gt; import codecs&gt;&gt;&gt; a = u'安德烈'&gt;&gt;&gt; f = codecs.open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = codecs.open('data.txt', 'r', encoding='utf-8') &gt;&gt;&gt; print f.read()安德烈&gt;&gt;&gt; f.close() 类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：123456789# python 2.7&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'w')&gt;&gt;&gt; f.write(b)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'r')&gt;&gt;&gt; print f.read().decode('utf-8')安德烈&gt;&gt;&gt; f.close() 总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。 python 2.x 中的 json.dumps() 操作json 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。 对于数据结构中的字符串类型为 str、 但实际上定义的是一个国际字符串的情况，json.dumps() 的结果如下：12345678# python 2.7&gt;&gt;&gt; a = &#123;'Natasha': '娜塔莎'&#125;&gt;&gt;&gt; a_json_1 = json.dumps(a)&gt;&gt;&gt; a_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; a_json_2 = json.dumps(a, ensure_ascii=False)&gt;&gt;&gt; a_json_2'&#123;"Natasha": "\xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e"&#125;' 可以看到，在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其会将 a 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 &#39;\\u5a1c&#39; 对应 6 个字符而非 1 个）；当 ensure_ascii 为 False 时，json.dumps() 操作的返回值类型仍然为 str，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 a_json_2 译码就可以得到我们想要的 json：12345# python 2.7&gt;&gt;&gt; a_json_2.decode('utf-8')u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print a_json_2.decode('utf-8')&#123;"Natasha": "娜塔莎"&#125; 对于数据结构中的字符串类型为 unicode 的情况，json.dumps() 的结果如下：12345678910# python 2.7&gt;&gt;&gt; u = &#123;u'Natasha': u'娜塔莎'&#125;&gt;&gt;&gt; u_json_1 = json.dumps(u)&gt;&gt;&gt; u_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; u_json_2 = json.dumps(u, ensure_ascii=False)&gt;&gt;&gt; u_json_2u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print u_json_2&#123;"Natasha": "娜塔莎"&#125; 在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其得到的结果和前面对 a 操作返回的结果完全一样；而当ensure_ascii 为 False 时，json.dumps() 操作的返回值类型变为 unicode，原始数据结构中的中文字符在返回值中完整地保留了下来。 对于数据结构中的字符串类型既有 unicode 又有 str 的情形，运用 json.dumps() 时将 ensure_ascii 设为 False 的情况又会完全不同。 当数据结构中的 ASCII 字符串为 str 类型，国际字符串为 unicode 类型时（如 u = {&#39;Natasha&#39;: u&#39;娜塔莎&#39;}），json.dumps() 的返回值是正常的、符合预期的 unicode 字符串；当数据结构中有国际字符串为 str 类型，又存在其他字符串为 unicode 类型时（如 u = {u&#39;Natasha&#39;: &#39;娜塔莎&#39;} 或 u = {u&#39;娜塔莉娅&#39;: &#39;娜塔莎&#39;}），json.dumps() 会抛出异常 UnicodeDecodeError，这是因为系统会将数据结构中 str 类型字符串都转换为 unicode 类型，而系统的默认编译码方式为 ascii 编码，因而对 str 类型的国际字符串进行 ascii 译码就必然会出错。]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>unicode</tag>
      </tags>
  </entry>
</search>
