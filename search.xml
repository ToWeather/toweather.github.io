<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[聚类分析（一）：层次聚类算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[聚类算法综述聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 cluster，使得同一 cluster 内的对象在某种意义上比不同的 cluster 之间的对象更为相似。由于 “cluster” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类： 基于连通模型（connectivity-based）的聚类算法： 即本文将要讲述的层次聚类算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 cluster。 基于中心点模型（centroid-based）的聚类算法： 在此类算法中，每个 cluster 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 k 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 cluster 的中心点的距离的平方和，优化变量为每个 cluster 的中心点以及每个对象属于哪个 cluster；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，k-means 算法即是其中的一种。 基于分布模型（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 cluster 即可。 基于密度（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 cluster，cluster 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。 层次聚类综述层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 clusters，后面一层生成的 clusters 基于前面一层的结果。层次聚类算法一般分为两类： Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。本文主要关注此类算法。 Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。 下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。 另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。 树形图 树形图 （dendrogram）可以用来直观地表示层次聚类的成果。一个有 5 个点的树形图如下图所示，其中纵坐标高度表示不同的 cluster 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\( x_1 \) 和 \( x_2 \) 的距离最近（为 1），因此将 \( x_1 \) 和 \( x_2 \) 合并为一个 cluster \( (x_1, x_2) \)，所以在树形图中首先将节点 \( x_1 \) 和 \( x_2 \) 连接，使其成为一个新的节点 \( (x_1, x_2) \) 的子节点，并将这个新的节点的高度置为 1；之后再在剩下的 4 个 cluster \( (x_1, x_2) \)， \( x_3 \)， \( x_4 \) 和 \( x_5 \) 中选取距离最近的两个 cluster 合并，\( x_4 \) 和 \( x_5 \) 的距离最近（为 2），因此将 \( x_4 \) 和 \( x_5 \) 合并为一个 cluster \( (x_4, x_5) \)，体现在树形图上，是将节点 \( x_4 \) 和 \( x_5 \) 连接，使其成为一个新的节点 \( (x_4, x_5) \) 的子节点，并将此新节点的高度置为 2；….依此模式进行树形图的生成，直至最终只剩下一个 cluster \( ((x_1, x_2), x_3), (x_4, x_5)) \)。可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 cluster 之间的距离都不大于 \( h \)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \( h \)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \( h=2.5 \)，即可得到 3 个 cluster \( (x_1, x_2) \)， \( x_3 \) 和 \( (x_4, x_5) \)。 对象之间的距离衡量衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \( d \) 维空间的两个对象 \({\bf x} =[ x_1, x_2, …, x_d]^{T} \) 和 \({\bf y} = [y_1, y_2, …, y_d]^{T}\)，其在不同距离准则下的距离计算方法如下表所示: 距离准则 距离计算方法 Euclidean 距离 \( d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{2} ]^{\frac{1}{2}} = [({\bf x} - {\bf y})^{T} ({\bf x} - {\bf y})]^{\frac{1}{2}} \) Manhattan 距离 \( d({\bf x},{\bf y}) = \sum_{j=1}^{d} \mid{x_j-y_j}\mid \) Chebyshev 距离 \( d({\bf x},{\bf y}) = \max_{1\leq{j}\leq{d}} \mid{x_j-y_j}\mid \) Minkowski 距离 \( d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{p} ]^{\frac{1}{p}}, p\geq{1} \) Minkowski 距离就是 \( \rm{L}\it{p}\) 范数（\( p\geq{1} \))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \( p = 1, 2, \infty \) 时的情形。另一种常用的距离是 Maholanobis 距离，其定义如下：$$ d_{mah}({\bf x}, {\bf y}) = \sqrt{({\bf x} - {\bf y})^{T}\Sigma^{-1} ({\bf x} - {\bf y})} $$其中 \( \Sigma \) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \( {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) \in \Bbb{R}^{d \times n}\)，\( {\bf x}_i \in \Bbb{R}^{d} \) 为第 \( i \) 个样本点，每个样本点的维度为 \( d \)，样本点的总数为 \( n \) 个；再假设样本点的平均值 \( m_{\bf x} = \frac{1}{n}\sum_{i=1}^{n} {\bf x}_i \) 为 \( {\bf 0} \) 向量（若不为 \( {\bf 0} \)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \( \Sigma \in \Bbb{R}^{d \times d} \) 可被定义为$$ \Sigma = \frac{1}{n} {\bf X}{\bf X}^{T} $$Maholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。 Cluster 之间的距离衡量除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 cluster 之间的距离，常见的 cluster 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，cluster 是由样本点或一些子 cluster （这些样本点或子 cluster 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 cluster 。假设 \( C_i \) 和 \( C_j \) 为两个 cluster，则前四种方法定义的 \( C_i \) 和 \( C_j \) 之间的距离如下表所示： 方法 定义 Single-link \( D(C_i, C_j) = \min_{ {\bf x} \in C_i, {\bf y} \in C_j } d({\bf x}, {\bf y}) \) Complete-link \( D(C_i, C_j) = \max_{ {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) \) UPGMA \( D(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) \) WPGMA omitting 其中 Single-link 定义两个 cluster 之间的距离为两个 cluster 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 cluster；而 Complete-link 则定义两个 cluster 之间的距离为两个 cluster 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 cluster 之间的距离为两个 cluster 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 cluster 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 cluster 对距离的计算的影响在同一层次上，而不受 cluster 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 cluster 之间的距离，而是通过已有的 cluster 之间的距离来计算合并后的新的 cluster 和剩余 cluster 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。 Centroid/UPGMC 方法给每一个 cluster 计算一个质心，两个 cluster 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：$$ D_{\rm{UPGMC}}(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y}\in C_j} d({\bf x}, {\bf y}) - \frac{1}{2{\mid C_i \mid }^{2}} \sum_ { {\bf x}, {\bf y}\in C_i} d( {\bf x}, {\bf y}) - \frac{1}{2{\mid \it{C_j} \mid }^{2}} \sum_ { {\bf x}, {\bf y} \in C_j} d( {\bf x}, {\bf y}) $$当上式中的 \( d(.,.) \) 为平方 Euclidean 距离时，\( D_{\rm{UPGMC}}(C_i, C_j) \) 为 \( C_i \) 和 \( C_j \) 的中心点（每个 cluster 内所有样本点之间的平均值）之间的平方 Euclidean 距离。Median/WPGMC 方法为每个 cluster 计算质心时，引入了权重。Ward 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 cluster 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，cluster \( C \) 的 \( {\rm ESS} \) 定义如下：$$ {\rm ESS} ( C ) = \sum_{ {\bf x} \in C} ({\bf x} - m_{\bf x})^{T} ({\bf x} - m_{\bf x}) $$其中 \( m_{\bf x} \) 为 \( C \) 中样本点的均值。可以看到 \( {\rm ESS} \) 衡量的是一个 cluster 内的样本点的聚合程度，样本点越聚合，\( {\rm ESS} \) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 cluster 的 \( {\rm ESS} \) 之和相对于合并前的 cluster 的 \( {\rm ESS} \) 之和的增长最小。 Agglomerative 层次聚类算法这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 cluster 之间的距离的 Lance-Williams 方法。 Lance-Williams 方法在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 cluster 合并为一个新的 cluster，然后再计算这个新的 cluster 与其他当前未被合并的 cluster 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 cluster 距离衡量方法都适用。具体地，对于三个 cluster \( C_k \)，\( C_i \) 和 \( C_j \)， Lance-Williams 给出的 \( C_k \) 与 \( C_i \) 和 \( C_j \) 合并后的新 cluster 之间的距离的计算方法如下式所示：$$ D(C_k, C_i \cup C_j) = \alpha_i D(C_k, C_i) + \alpha_j D(C_k, C_j) + \beta D(C_i, C_j) + \gamma \mid D(C_k, C_i) - D(C_k, C_j) \mid $$其中，\( \alpha_i \)，\( \alpha_j \)，\( \beta \)，\( \gamma \) 均为参数，随 cluster 之间的距离计算方法的不同而不同，具体总结为下表（注：\( n_i \) 为 cluster \( C_i \) 中的样本点的个数)： 方法 参数 \( \alpha_i \) 参数 \( \alpha_j \) 参数 \( \beta \) 参数 \( \gamma \) Single-link \( 1/2 \) \( 1/2 \) \( 0 \) \( -1/2 \) Complete-link \( 1/2 \) \( 1/2 \) \( 0 \) \( 1/2 \) UPGMA \( n_i/(n_i + n_j) \) \( n_j/(n_i + n_j) \) \( 0 \) \( 0 \) WPGMA \( 1/2 \) \( 1/2 \) \( 0 \) \( 0 \) UPGMC \( n_i/(n_i + n_j) \) \( n_j/(n_i + n_j) \) \( n_{i}n_{j}/(n_i + n_j)^{2} \) \( 0 \) WPGMC \( 1/2 \) \( 1/2 \) \( 1/4 \) \( 0 \) Ward \( (n_k + n_i)/(n_i + n_j + n_k) \) \( (n_k + n_j)/(n_i + n_j + n_k) \) \( n_k/(n_i + n_j + n_k) \) \( 0 \) 其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。 Naive 算法给定数据集 \( {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) \)，Agglomerative 层次聚类最简单的实现方法分为以下几步： 初始时每个样本为一个 cluster，计算距离矩阵 \( \bf D \)，其中元素 \( D_{ij} \) 为样本点 \( {\bf x}_i \) 和 \( {\bf x}_j \) 之间的距离； 遍历距离矩阵 \( \bf D \)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 cluster 的编号，将这两个 cluster 合并为一个新的 cluster 并依据 Lance-Williams 方法更新距离矩阵 \( \bf D \) （删除这两个 cluster 对应的行和列，并把由新 cluster 所算出来的距离向量插入 \( \bf D \) 中），存储本次合并的相关信息； 重复 2 的过程，直至最终只剩下一个 cluster 。 当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。可以看到，该 Naive 算法的时间复杂度为 \( O(n^{3}) \) （由于每次合并两个 cluster 时都要遍历大小为 \( O(n^{2}) \) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \( n - 1 \) 次），空间复杂度为 \( O(n^{2}) \) （由于要存储距离矩阵）。当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \( O(n^{2} ) \) 或更低，例如 SLINK 算法（Single-link 方法），CLINK 算法（Complete-link 方法），BIRCH 算法（适用于 Euclidean 距离准则）等等。 利用 Scipy 实现层次聚类在这里我们将利用 SciPy（python 中的一个用于数值分析和科学计算的第三方包，功能强大，NumPy+SciPy+matplotlib 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。 生成实验样本集首先，我们需要导入相关的模块，代码如下所示：12345# python 3.6&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from scipy.cluster.hierarchy import linkage, dendrogram, fcluster&gt;&gt;&gt; from sklearn.datasets.samples_generator import make_blobs&gt;&gt;&gt; import matplotlib.pyplot as plt 其中 make_blobs 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 cluster 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 X 的维度为 n*d。1234567&gt;&gt;&gt; centers = [[1, 1], [-1, -1], [1, -1]] # 定义 3 个中心点# 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别&gt;&gt;&gt; X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0） &gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c='b')&gt;&gt;&gt; plt.title('The dataset')&gt;&gt;&gt; plt.show() 样本的分布如下图所示。 进行 Agglomerative 层次聚类SciPy 里面进行层次聚类非常简单，直接调用 linkage 函数，一行代码即可搞定。1&gt;&gt;&gt; Z = linkage(X, method='ward', metric='euclidean') 以上即进行了一次 cluster 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 method 参数可以为 &#39;single&#39;、 &#39;complete&#39; 、&#39;average&#39;、 &#39;weighted&#39;、 &#39;centroid&#39;、 &#39;median&#39;、 &#39;ward&#39; 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 metric 参数调整。linkage 函数的返回值 Z 为一个维度 (n-1)*4 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 4 个值分别对应合并的两个 cluster 的序号、两个 cluster 之间的距离以及本次合并后产生的新的 cluster 所包含的样本点的个数；具体地，对于第 i 次迭代（对应 Z 的第 i 行），序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 合并产生新的 cluster n + i, Z[i, 2] 为序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 之间的距离，合并后的 cluster 包含 Z[i, 3] 个样本点。例如本次实验中 Z 记录到的前 25 次合并的信息如下所示：12345678910111213141516171819202122232425262728&gt;&gt;&gt; print(Z.shape)(749, 4)&gt;&gt;&gt; print(Z[: 25])[[253. 491. 0.00185 2. ] [452. 696. 0.00283 2. ] [ 70. 334. 0.00374 2. ] [237. 709. 0.00378 2. ] [244. 589. 0.00423 2. ] [141. 550. 0.00424 2. ] [195. 672. 0.00431 2. ] [ 71. 102. 0.00496 2. ] [307. 476. 0.00536 2. ] [351. 552. 0.00571 2. ] [ 62. 715. 0.00607 2. ] [ 98. 433. 0.0065 2. ] [255. 572. 0.00671 2. ] [437. 699. 0.00685 2. ] [ 55. 498. 0.00765 2. ] [143. 734. 0.00823 2. ] [182. 646. 0.00843 2. ] [ 45. 250. 0.0087 2. ] [298. 728. 0.00954 2. ] [580. 619. 0.01033 2. ] [179. 183. 0.01062 2. ] [101. 668. 0.01079 2. ] [131. 544. 0.01125 2. ] [655. 726. 0.01141 2. ] [503. 756. 0.01265 3. ]] 从上面的信息可以看到，在第 6 次合并中，样本点 141 与样本点 550 进行了合并，生成新 cluster 756；在第 25 次合并中，样本点 503 与 cluster 756 进行了合并，生成新的 cluster 770。我们可以将样本点 141、550 和 503 的特征信息打印出来，来看看它们是否确实很接近。1234&gt;&gt;&gt; print(X[[141, 550, 503]])[[ 1.27098 -0.97927] [ 1.27515 -0.98006] [ 1.37274 1.13599]] 看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 cluster 中相隔最近的。 画出树形图SciPy 中给出了根据层次聚类的结果 Z 绘制树形图的函数 dendrogram，我们由此画出本次实验中的最后 20 次的合并过程。123456&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; dendrogram(Z, truncate_mode='lastp', p=20, show_leaf_counts=False, leaf_rotation=90, leaf_font_size=15, show_contracted=True)&gt;&gt;&gt; plt.title('Dendrogram for the Agglomerative Clustering')&gt;&gt;&gt; plt.xlabel('sample index')&gt;&gt;&gt; plt.ylabel('distance')&gt;&gt;&gt; plt.show() 得到的树形图如下所示。 可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 3 个 cluster（和实际相符），分别在上图中由三种颜色所表示。 获取聚类结果在得到了层次聚类的过程信息 Z 后，我们可以使用 fcluster 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 d，得到在该距离以下的未合并的所有 cluster 作为聚类结果；另一个是指定 cluster 的数量 k，函数会返回最后的 k 个 cluster 作为聚类结果。使用哪个维度由参数 criterion 决定，对应的临界距离或聚类的数量则由参数 t 所记录。fcluster 函数的结果为一个一维数组，记录每个样本的类别信息。对应的代码与返回结果如下所示。123456789101112131415161718# 根据临界距离返回聚类结果&gt;&gt;&gt; d = 15&gt;&gt;&gt; labels_1 = fcluster(Z, t=d, criterion='distance')&gt;&gt;&gt; print(labels_1[: 100]) # 打印聚类结果[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; print(len(set(labels_1))) # 看看在该临界距离下有几个 cluster3 # 根据聚类数目返回聚类结果&gt;&gt;&gt; k = 3&gt;&gt;&gt; labels_2 = fcluster(Z, t=k, criterion='maxclust')&gt;&gt;&gt; print(labels_2[: 100]) [2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; list(labels_1) == list(labels_2) # 看看两种不同维度下得到的聚类结果是否一致True 下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。1234&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.title('The Result of the Agglomerative Clustering')&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c=labels_2, cmap='prism')&gt;&gt;&gt; plt.show() 可视化结果如下图所示。 上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。 比较不同方法下的聚类结果最后，我们对同一份样本集进行了 cluster 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 3，程序如下：1234567891011121314151617181920212223242526272829303132333435363738from time import timeimport numpy as npfrom scipy.cluster.hierarchy import linkage, fclusterfrom sklearn.datasets.samples_generator import make_blobsfrom sklearn.metrics.cluster import adjusted_mutual_info_scoreimport matplotlib.pyplot as plt# 生成样本点centers = [[1, 1], [-1, -1], [1, -1]]X, labels = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)# 可视化聚类结果def plot_clustering(X, labels, title=None): plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism') if title is not None: plt.title(title, size=17) plt.axis('off') plt.tight_layout()# 进行 Agglomerative 层次聚类linkage_method_list = ['single', 'complete', 'average', 'ward']plt.figure(figsize=(10, 8))ncols, nrows = 2, int(np.ceil(len(linkage_method_list) / 2))plt.subplots(nrows=nrows, ncols=ncols)for i, linkage_method in enumerate(linkage_method_list): print('method %s:' % linkage_method) start_time = time() Z = linkage(X, method=linkage_method) labels_pred = fcluster(Z, t=3, criterion='maxclust') print('Adjust mutual information: %.3f' % adjusted_mutual_info_score(labels, labels_pred)) print('time used: %.3f seconds' % (time() - start_time)) plt.subplot(nrows, ncols, i + 1) plot_clustering(X, labels_pred, '%s linkage' % linkage_method)plt.show() 可以得到 4 种方法下的聚类结果如下图所示。 在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 AMI（Adjust Mutual Information）量，该量越接近于 1 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下：123456789101112method single:Adjust mutual information: 0.001time used: 0.008 secondsmethod complete:Adjust mutual information: 0.838time used: 0.013 secondsmethod average:Adjust mutual information: 0.945time used: 0.019 secondsmethod ward:Adjust mutual information: 0.956time used: 0.015 seconds 从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 cluster，而其他两个 cluster 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>层次聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的 Unicode String 和 Byte String]]></title>
    <url>%2FPython-%E4%B8%AD%E7%9A%84-Unicode-string-%E5%92%8C-Byte-string%2F</url>
    <content type="text"><![CDATA[python 2.x 和 python 3.x 字符串类型的区别python 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。 python 2.x str 类型：处理 binary 数据和 ASCII 文本数据。 unicode 类型：处理非 ASCII 文本数据。 python 3.x bytes 类型：处理 binary 数据，同 str 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。 str 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。 bytearray 类型：bytes 类型的变种，但是此类型是 mutable 的。 Unicode 简介包括 ASCII 码、latin-1 编码 和 utf-8 编码 等在内的码都被认为是 unicode 码。 编码和解码的概念 编码（encoding）：将字符串映射为一串原始的字节。 解码（decoding）：将一串原始的字节翻译成字符串。 ASCII码 编码长度为 1 个 byte. 编码范围为 0x00~0x7F，只包含一些常见的字符。 latin-1码 编码长度为 1 个 byte. 编码范围为 0x00~0xFF，能支持更多的字符（如 accent character），兼容 ASCII 码。 utf-8码 编码长度可变，为 1~4 个 byte。 当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 0x00 ~ 0x7F；当编码长度大于 1 个 byte 时，每个 byte 的取值为 0x80 ~ 0xFF。 其它编码 utf-16，编码长度为定长 2 个 byte。 utf-32，编码长度为定长 4 个 byte。 Unicode 字符串的存储方式在内存中的存储方式unicode 字符串在内存中以一种与编码方式无关的方式存储：unicode code point，在表示 unicode 字符串时可以以 unicode code point 的方式表示，例如在下面的例子 中，a 和 b 表示的是同一字符串（其中 &#39;\uNNNN&#39; 即为 unicode code point，N 为一个十六进制位；当 unicode code point 的取值在 0~255 范围内时，也可以 &#39;\xNN&#39; 的形式表示）：1234567# python 2.7&gt;&gt;&gt; a = u'\u5a1c\u5854\u838e'&gt;&gt;&gt; b = u'娜塔莎'&gt;&gt;&gt; print a, b娜塔莎 娜塔莎&gt;&gt;&gt; c = u'\xe4'ä 在文件等外部媒介中的存储方式unicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。 字符表示python 3.x在 python 3.x 中，str 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：123456# python 3.6&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; len(a)12 可以看到，python 3.x 中得到的 a 的长度为 12（包含空格），没有任何问题；我们可以对 a 进行编码，将其转换为 bytes 类型：123456# python 3.6&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; bb'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)bytes 从上面可以看出，bytes 类型的对象中的某个字节的取值在 0x00 ~ 0x7F 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。同理，我们也可以将一个 bytes 类型的对象译码为一个 str 类型的对象：1234# python 3.6&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; a'Natasha, 娜塔莎' python 2.x在 python 2.x 中，如果还是用 str 类型来表示国际字符，就会有问题：12345678910# python 2.7&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; a'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; len(a)18&gt;&gt;&gt; print aNatasha, 娜塔莎 可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 str 字符串，但实际上 a 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 a 正好是字符串 &#39;Natasha, 娜塔莎&#39; 的 utf-8 编码的结果，且将 a 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:123456# python 2.7&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdin.encoding # 控制台的输入编码，可解释前例中 a 在内存中的表现形式'utf-8'&gt;&gt;&gt; sys.stdout.encoding # 控制台的输出编码，可解释前例中打印 a 的显示结果'utf-8' 另外，sys.getdefaultencoding()函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 &#39;ascii&#39;, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 json.dumps() 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：&gt;）。切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 unicode 类型字符串，如下所示：1234567891011121314# python 2.7&gt;&gt;&gt; a = u'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)unicode&gt;&gt;&gt; len(a)12&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; b'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)str&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; au'Natasha, \u5a1c\u5854\u838e' 另外，我们可以对 unicode 类型字符串进行编码操作，对 str 类型字符串进行译码操作。 文本文件操作python 3.x在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 open 函数的参数 encoding 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 sys.getfilesystemencoding()得到，如：12345678910111213# python 3.6&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)3&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.read()'娜塔莎'&gt;&gt;&gt; f.close() 当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：12345678910# python 3.6&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'wb')&gt;&gt;&gt; f.write(b)9&gt;&gt;&gt; f.close()&gt;&gt;&gt; f.read().decode('utf-8')'娜塔莎'&gt;&gt;&gt; f.close() python 2.x在 python 2.x 中，open 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 str；codecs.open 函数则支持自动读写 unicode 文本文件，如：12345678910# python 2.7&gt;&gt;&gt; import codecs&gt;&gt;&gt; a = u'安德烈'&gt;&gt;&gt; f = codecs.open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = codecs.open('data.txt', 'r', encoding='utf-8') &gt;&gt;&gt; print f.read()安德烈&gt;&gt;&gt; f.close() 类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：123456789# python 2.7&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'w')&gt;&gt;&gt; f.write(b)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'r')&gt;&gt;&gt; print f.read().decode('utf-8')安德烈&gt;&gt;&gt; f.close() 总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。 python 2.x 中的 json.dumps() 操作json 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。对于数据结构中的字符串类型为 str、 但实际上定义的是一个国际字符串的情况，json.dumps() 的结果如下：12345678# python 2.7&gt;&gt;&gt; a = &#123;'Natasha': '娜塔莎'&#125;&gt;&gt;&gt; a_json_1 = json.dumps(a)&gt;&gt;&gt; a_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; a_json_2 = json.dumps(a, ensure_ascii=False)&gt;&gt;&gt; a_json_2'&#123;"Natasha": "\xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e"&#125;' 可以看到，在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其会将 a 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 &#39;\\u5a1c&#39; 对应 6 个字符而非 1 个）；当 ensure_ascii 为 False 时，json.dumps() 操作的返回值类型仍然为 str，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 a_json_2 译码就可以得到我们想要的 json：12345# python 2.7&gt;&gt;&gt; a_json_2.decode('utf-8')u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print a_json_2.decode('utf-8')&#123;"Natasha": "娜塔莎"&#125; 对于数据结构中的字符串类型为 unicode 的情况，json.dumps() 的结果如下：12345678910# python 2.7&gt;&gt;&gt; u = &#123;u'Natasha': u'娜塔莎'&#125;&gt;&gt;&gt; u_json_1 = json.dumps(u)&gt;&gt;&gt; u_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; u_json_2 = json.dumps(u, ensure_ascii=False)&gt;&gt;&gt; u_json_2u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print u_json_2&#123;"Natasha": "娜塔莎"&#125; 在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其得到的结果和前面对 a 操作返回的结果完全一样；而当ensure_ascii 为 False 时，json.dumps() 操作的返回值类型变为 unicode，原始数据结构中的中文字符在返回值中完整地保留了下来。对于数据结构中的字符串类型既有 unicode 又有 str 的情形，运用 json.dumps() 时将 ensure_ascii 设为 False 的情况又会完全不同。当数据结构中的 ASCII 字符串为 str 类型，国际字符串为 unicode 类型时（如 u = {&#39;Natasha&#39;: u&#39;娜塔莎&#39;}），json.dumps() 的返回值是正常的、符合预期的 unicode 字符串；当数据结构中有国际字符串为 str 类型，又存在其他字符串为 unicode 类型时（如 u = {u&#39;Natasha&#39;: &#39;娜塔莎&#39;} 或 u = {u&#39;娜塔莉娅&#39;: &#39;娜塔莎&#39;}），json.dumps() 会抛出异常 UnicodeDecodeError，这是因为系统会将数据结构中 str 类型字符串都转换为 unicode 类型，而系统的默认编译码方式为 ascii 编码，因而对 str 类型的国际字符串进行 ascii 译码就必然会出错。]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>unicode</tag>
      </tags>
  </entry>
</search>
