<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/myicon.JPEG?v=6.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/myicon.JPEG?v=6.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.0" color="#222">





  <meta name="keywords" content="Hexo, NexT">




  


  <link rel="alternate" href="/atom.xml" title="Heathcliff's Notes" type="application/atom+xml">






<meta name="description" content="聚类分析（一）：层次聚类算法 聚类分析（二）：k-means 算法 聚类分析（三）：高斯混合模型与 EM 算法 聚类分析（四）：DBSCAN 算法">
<meta property="og:type" content="website">
<meta property="og:title" content="聚类分析系列文章">
<meta property="og:url" content="http://heathcliff.me/clustering-analysis/index.html">
<meta property="og:site_name" content="Heathcliff&#39;s Notes">
<meta property="og:description" content="聚类分析（一）：层次聚类算法 聚类分析（二）：k-means 算法 聚类分析（三）：高斯混合模型与 EM 算法 聚类分析（四）：DBSCAN 算法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-03-22T12:23:10.787Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="聚类分析系列文章">
<meta name="twitter:description" content="聚类分析（一）：层次聚类算法 聚类分析（二）：k-means 算法 聚类分析（三）：高斯混合模型与 EM 算法 聚类分析（四）：DBSCAN 算法">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://heathcliff.me/clustering-analysis/">





  <title>聚类分析（三）：高斯混合模型与 EM 算法 | Heathcliff's Notes</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Heathcliff's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            

            
              首页
            

          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            

            
              标签
            

          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            

            
              分类
            

          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            

            
              归档
            

          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://heathcliff.me/聚类分析（三）：高斯混合模型与EM算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heathcliff">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/myicon.JPEG">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heathcliff's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">聚类分析（三）：高斯混合模型与 EM 算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-12T21:26:08+08:00">2018-03-12</time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-algorithms/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/聚类分析（三）：高斯混合模型与EM算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="聚类分析（三）：高斯混合模型与EM算法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/聚类分析（三）：高斯混合模型与EM算法/" class="leancloud_visitors" data-flag-title="聚类分析（三）：高斯混合模型与 EM 算法">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p><em>本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 <a href="../clustering-analysis/index.html"><strong>聚类分析系列文章</strong></a></em>。</p>
<h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><h2 id="高斯混合模型简介"><a href="#高斯混合模型简介" class="headerlink" title="高斯混合模型简介"></a>高斯混合模型简介</h2><p>高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：</p>
<script type="math/tex; mode=display">
p({\bf x}) = \sum_{k = 1}^{K} \pi_{k} {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k) 
\tag {1}</script><p>如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 $ K $ 个类别中以 $ \pi_k $ 的概率随机选择一个类别 $ k $，然后再依照该类别所对应的高斯分布 $ {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k) $ 随机产生一个数据 $ {\bf x} $。但最终生成数据集后，我们所观察到的仅仅只是 $ {\bf x} $，而观察不到用于产生 $ {\bf x} $ 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 $ {\bf z} \in {\lbrace 0, 1 \rbrace}^{K} $ 来表示高斯混合模型中的类别变量，$ {\bf z} $ 中仅有一个元素的值为 $ 1 $，而其它元素的值为 $ 0 $，例如当 $ z_{k} = 1$ 时，表示当前数据是由高斯混合分布中的第 $ k $ 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 $ {\bf z} $ 的概率质量函数为</p>
<script type="math/tex; mode=display">
p(z_{k} = 1) = \pi_k 
\tag {2}</script><p>其中 $ \lbrace \pi_{k} \rbrace $ 须满足</p>
<script type="math/tex; mode=display">
\sum_{k = 1}^{K} \pi_k = 1  \text{,} \ \ \ 0 \le \pi_k \le 1</script><p>给定 $ {\bf z} $ 的值的情况下，$ {\bf x} $ 服从高斯分布</p>
<script type="math/tex; mode=display">
p({\bf x} | z_{k} = 1) = {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) 
\tag {3}</script><p>因而可以得到 $ {\bf x} $ 的边缘概率分布为</p>
<script type="math/tex; mode=display">
p({\bf x}) = \sum_{\bf z} p({\bf z})p({\bf x} | {\bf z}) = \sum_{k =1}^{K} \pi_k {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) 
\tag {4}</script><p>该分布就是我们前面所看到的高斯混合分布。</p>
<h2 id="最大似然估计问题"><a href="#最大似然估计问题" class="headerlink" title="最大似然估计问题"></a>最大似然估计问题</h2><p>假设有数据集 $ \lbrace {\bf x}_1, {\bf x}_2, …, {\bf x}_N \rbrace $，其中样本的维度为 $ D $，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 $ {\bf X} = [ {\bf x}_1, {\bf x}_2, …, {\bf x}_N ]^{T} \in {\Bbb R}^{N \times D} $ 来表示数据集，以矩阵 $ {\bf Z} = [ {\bf z}_1, {\bf z}_2, …, {\bf z}_N ]^{T} \in {\Bbb R}^{N \times K} $ 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为</p>
<script type="math/tex; mode=display">
L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \ln \lbrace  \sum_{k = 1}^{K} \pi_k {\cal N}({\bf x}_{n} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) \rbrace 
\tag {5}</script><p>我们希望求解出使得以上对数似然函数最大的参数集 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。</p>
<p> 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 $ L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 发散的解。具体地，观察公式 $ (5) $，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 $ 0 $）的时候，会导致求和项中的该项的值为无穷大，从而也使得 $ L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 的值为无穷大；这样确实使得式 $ (5) $ 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。</p>
<hr>
<h1 id="EM-算法求解高斯混合模型"><a href="#EM-算法求解高斯混合模型" class="headerlink" title="EM 算法求解高斯混合模型"></a>EM 算法求解高斯混合模型</h1><p>EM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。</p>
<p>我们对对数似然函数 $ (5) $ 关于各参数 $ {\bf \pi} $， $ {\bf \mu} $， $ {\bf \Sigma} $ 分别求偏导，并将其置为 $ 0 $ 可以得到一系列的方程，而使得式 $ (5) $ 最大的解也一定满足这些方程。</p>
<p>首先令式 $ (5) $ 关于 $ {\bf \mu}_k $ 的偏导为 $ 0 $ 可得以下方程：</p>
<script type="math/tex; mode=display">
\sum_{n = 1}^{N} \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} {\bf \Sigma}_k ({\bf x}_n - {\bf \mu}_k) = 0
\tag {6}</script><p>注意到，上式中含有项</p>
<script type="math/tex; mode=display">
\gamma (z_{nk}) =   \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = p(z_{nk} = 1 | {\bf x}_n) 
\tag {7}</script><p>该项具有重要的物理意义，它为给定样本点 $ {\bf x}_n $ 后隐变量 $ {\bf z}_n $ 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 $ \gamma (z_{nk}) $ 的值来做软分配的。</p>
<p>因而，我们可以由式 $ (6) $ 和式 $ (7) $ 写出</p>
<script type="math/tex; mode=display">
{\bf \mu}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) {\bf x}_n
\tag {8}</script><p>其中 $ N_k = \sum_{n = 1}^{N} \gamma(z_{nk}) $，我们可以将 $ N_{k} $ 解释为被分配给类别 $ k $ 的有效样本数量，而 $ {\bf \mu}_{k} $ 即为所有样本点的加权算术平均值，每个样本点的权重等于第 $ k$ 个高斯分量对产生该样本点所负有的“责任”。</p>
<p>我们再将式 $ (5) $ 对 $ {\bf \Sigma}_k $ 的偏导数置为 $ 0 $ 可求得</p>
<script type="math/tex; mode=display">
{\bf \Sigma}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) ({\bf x}_n - {\bf \mu}_k)({\bf x}_n - {\bf \mu}_k)^{\text T}
\tag {9}</script><p>可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 $ \gamma(z_{nk}) $。</p>
<p>最后我们再来推导 $ \pi_k $ 的最大似然解须满足的条件。由于 $ \pi_k $ 有归一化的约束，我们可以利用 <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier?oldformat=true" rel="external nofollow noopener noreferrer" target="_blank"> Lagrange 乘数法 </a> 来求解（将 $ \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) + \lambda (\sum_{k = 1}^{K} \pi_k - 1) $ 关于 $ \pi_k $ 的偏导数置 $ 0 $），最后可求得</p>
<script type="math/tex; mode=display">
\pi_k = \frac{N_k} {N}
\tag {10}</script><p>关于类别 $ k $ 的先验概率的估计值可以理解为所有样本点中被分配给第 $ k $ 个类别的有效样本点的个数占总样本数量的比例。</p>
<p>注意到，式 $ (8) $ 至 $ (10) $ 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 的闭式解，因为这些式子中给出的表达式中都含有 $ \gamma (z_{nk}) $，而 $ \gamma (z_{nk}) $ 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 $ (7) $）。</p>
<p>尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 固定为一个初始值，再按式 $ (7) $ 计算出隐变量的后验概率 $ \gamma (z_{nk}) $ （E 步）；然后再固定 $ \gamma (z_{nk}) $，按式 $ (8) $ 至 $ (10) $ 分别更新参数  $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。</p>
<p>实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。</p>
<p>我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率  $ \gamma (z_{nk}) $，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 $ p_1 $ ，由红色的高斯分量产生的概率为 $ p_2 $，则我们将其染上 $ p_1 $ 比例的蓝色，染上  $ p_2 $ 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。</p>
<div align="center">
<img src="http://free-cn-01.cdn.bilnn.com/ddimg/jfs/t1/95119/36/19198/564054/5e9d2234E755f59ca/df0c82b898c6b3e1.png" width="660" height="550" alt="EM 算法求解 GMM 运行过程" align="center">
</div>
----
# 一般 EM 算法

前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。

假设观测到的变量集为 $ {\bf X} $，隐变量集为 $ {\bf Z} $，模型中所涉及到的参数集为 $ {\bf \theta} $，我们的目的是最大化关于 $ {\bf X} $ 的似然函数
$$
p({\bf X} | {\bf \theta}) = \sum_{\bf Z} p({\bf X}, {\bf Z} | {\bf \theta})
\tag {11}
$$
一般来讲，直接优化 $ p({\bf X} | {\bf \theta}) $ 是比较困难的，而优化完全数据集的似然函数 $ p({\bf X}, {\bf Z} | {\bf \theta}) $ 的难度则会大大减小，EM 算法就是基于这样的思路。

首先我们引入一个关于隐变量 $ {\bf Z} $ 的分布 $ q({\bf Z}) $，然后我们可以将对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 分解为如下
$$
\ln p({\bf X} | {\bf \theta}) = {\cal L}(q, {\bf \theta}) + {\text KL}(q || p) 
\tag {12}
$$
其中
$$
{\cal L}(q, {\bf \theta}) = \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{p({\bf X}, {\bf Z} | {\bf \theta})} {q({\bf Z})} \rbrace
\tag {13}
$$
$$
{\text KL}(q || p) = - \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{ p({\bf Z} | {\bf X}, {\bf \theta})} {q({\bf Z})} \rbrace
\tag {14}
$$
其中 $ {\cal L}(q, {\bf \theta}) $ 为关于概率分布 $ q({\bf Z}) $ 的泛函，且为关于参数集 $ {\bf \theta} $ 的函数，另外，$ {\cal L}(q, {\bf \theta}) $ 的表达式中包含了关于完全数据集的似然函数 $ p({\bf X}, {\bf Z} | {\bf \theta}) $，这是我们需要好好加以利用的；$ {\text KL}(q || p) $ 为概率分布 $ q({\bf Z}) $ 与隐变量的后验概率分布 $ p({\bf Z} | {\bf X}, {\bf \theta}) $ 间的 [KL 散度][3]，它的值一般大于 $ 0 $，只有在两个概率分布完全相同的情况下才等于 $ 0 $，因而其一般被用来衡量两个概率分布之间的差异。

利用 $ {\text KL}(q || p) \ge 0 $ 的性质，我们可以得到 $ {\cal L}(q, {\bf \theta}) \le \ln p({\bf X} | {\bf \theta}) $，即 $ {\cal L}(q, {\bf \theta}) $ 是对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 的一个下界。 $ \ln p({\bf X} | {\bf \theta}) $ 与 $ {\cal L}(q, {\bf \theta}) $  及 $ {\text KL}(q || p) $ 的关系可用下图中的图（a）来表示。 

<div align="center">
<img src="http://free-cn-01.cdn.bilnn.com/ddimg/jfs/t1/95615/21/19360/177497/5e9d2261E6567325f/4847b152a367efac.png" width="1000" height="600" alt="EM 算法的推导过程示意图" align="center">
</div>


<p>有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。</p>
<p>假设当前迭代步骤的参数的值为 $ {\bf \theta}^{\text {old}} $，我们先固定 $ {\bf \theta}^{\text {old}} $ 的值，来求 $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 关于概率分布 $ q({\bf Z}) $ 的最大值。可以看到，$ \ln p({\bf X} | {\bf \theta} ^{\text {old}}) $ 现在是一个定值，所以当 $ {\text KL}(q || p) $ 等于 $ 0 $ 时， $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 最大，如上图中的图（b）所示。此时由 $ {\text KL}(q || p) = 0 $ 可以推出，$ q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $。</p>
<p>现在再固定 $ q({\bf Z}) $，来求 $ {\cal L}(q, {\bf \theta}) $ 关于 $ {\bf \theta} $ 的最大值，假设求得的最佳 $ {\bf \theta} $ 的值为 $ {\bf \theta} ^{\text {new}} $，此时 $ {\cal L}(q, {\bf \theta} ^{\text {new}}) $ 相比 $ {\cal L}(q, {\bf \theta}^{\text {old}}) $ 的值增大了，而由于 $ {\bf \theta} $ 值的改变又使得当前 $ {\text KL}(q || p) $ 的值大于或等于 $ 0 $ （当算法收敛时保持 $ 0 $ 的值不变），所以根据式 $ (14) $，对数似然函数 $ \ln p({\bf X} | {\bf \theta}) $ 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。</p>
<p>更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 $ p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $，我们将 $ q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $ 带入  $ {\cal L}(q, {\bf \theta}) $  中，可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
{\cal L}(q, {\bf \theta}) &= \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta}) - \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old} }) \ln p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old} }) \\
& = {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) + {\text {const} }
\end{aligned}
\tag {15}</script><p>我们只对上式中的第一项 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。</p>
<p>第二个迭代步骤被称为 M 步（Maximization），是因为要对式 $ (15) $ 中的 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 求解关于  $ {\bf \theta} $ 的最大值，由于 $ {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) $ 的形式相较于对数似然函数  $ \ln p({\bf X} | {\bf \theta}) $ 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。</p>
<p>最后还是来总结一下 EM 算法的运行过程：</p>
<ol>
<li>选择一个初始参数集 $ {\bf \theta}^{\text {old}} $；</li>
<li><strong>E  步</strong>，计算隐变量的后验概率函数 $ p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) $；</li>
<li><strong>M 步</strong>，按下式计算 $ {\bf \theta}^{\text {new}} $ 的值<script type="math/tex; mode=display">
{\bf \theta}^{\text {new}} = \rm {arg}  \rm {max}_{\bf \theta} {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old} })
\tag {16}</script>其中 <script type="math/tex; mode=display">
{\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) = \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta})
\tag {17}</script></li>
<li>检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 $ {\bf \theta}^{\text {old}} = {\bf \theta}^{\text {new}} $ ，回到第 2 步继续迭代。</li>
</ol>
<hr>
<h1 id="再探高斯混合模型"><a href="#再探高斯混合模型" class="headerlink" title="再探高斯混合模型"></a>再探高斯混合模型</h1><p>在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 $ (7) $ 至 $ (10) $ ）。</p>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>在高斯混合模型中，参数集 $ {\bf \theta} = \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $，完整数据集 $ \lbrace {\bf X}, {\bf Z} \rbrace $ 的似然函数为</p>
<script type="math/tex; mode=display">
p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} {\pi_k}^{z_{nk}} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)^{z_{nk}}
\tag {18}</script><p>对其取对数可得</p>
<script type="math/tex; mode=display">
\ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} z_{nk} \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace
\tag {19}</script><p>按照 EM 算法的迭代步骤，我们先求解隐变量 $ {\bf Z} $ 的后验概率函数，其具有如下形式</p>
<script type="math/tex; mode=display">
p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) \propto p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} ({\pi_k} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k))^{z_{nk}}
\tag {20}</script><p>再来推导完全数据集的对数似然函数在 $ p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) $ 下的期望</p>
<script type="math/tex; mode=display">
{\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} {\Bbb E}[z_{nk}] \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace
\tag {21}</script><p>而 $ {\Bbb E}[z_{nk}] $ 的值可以根据式 $ (20) $ 求出，由于 $ z_{nk} $ 只可能取 $ 1 $ 或 $ 0 $，而取 $ 0 $ 时对期望没有贡献，故有</p>
<script type="math/tex; mode=display">
{\Bbb E}[z_{nk}] = p(z_{nk} = 1 | {\bf x}_n, {\bf \pi}, {\bf \mu}_k, {\bf \Sigma}_k) = \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = \gamma (z_{nk})
\tag {22}</script><p>将上式代入公式 $ (21) $ 中，可得</p>
<script type="math/tex; mode=display">
{\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} \gamma (z_{nk}) \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace
\tag {23}</script><p>接下来我们就可以对该式关于参数 $ \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace $ 求解最大值了，可以验证，各参数的更新方程就是式 $ (8) $ 至 $ (10) $。</p>
<h2 id="与-k-means-算法的关系"><a href="#与-k-means-算法的关系" class="headerlink" title="与 k-means 算法的关系"></a>与 k-means 算法的关系</h2><p>敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。</p>
<p>首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 $ K $ 个高斯分量，但 k-means 算法做了如下假设：</p>
<ol>
<li>假设每个高斯分量的先验概率相等，即 $ \pi_k = 1 / K $;</li>
<li>假设每个高斯分量的协方差矩阵均为 $ \epsilon {\bf I} $。</li>
</ol>
<p>所以某一个高斯分量的概率密度函数为</p>
<script type="math/tex; mode=display">
p({\bf x} | {\bf \mu}_k, {\bf \Sigma}_k) = \frac {1} {(2\pi\epsilon)^{D/2}} \exp \lbrace -\frac {\| {\bf x} - {\bf \mu}_k \|^{2}} {2\epsilon} \rbrace
\tag {24}</script><p>故根据 EM 算法，可求得隐变量的后验概率函数为</p>
<script type="math/tex; mode=display">
\gamma(z_{nk}) = \frac{\pi_k \exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \pi_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace } = \frac{\exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace }
\tag {25}</script><p>在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 $ \gamma(z_{nk}) $ 的值最大的那个高斯分量，即有</p>
<script type="math/tex; mode=display">
r_{nk} = \begin{cases} 1, & \text {if \( k = \rm {arg}  \rm {min}_{j}  \| {\bf x}_n -  {\bf \mu}_j \|^{2} \) } \\ 0, & \text {otherwise} \end{cases} 
\tag {26}</script><p>由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 $ {\bf \mu}_k $ 求优，将式 $ (8) $ 中的 $ \gamma(z_{nk}) $ 替换为 $ r_{nk} $，即可得到 $ {\bf \mu}_k $ 的更新方法，与 k-means 算法中对中心点的更新方法一致。</p>
<p>现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 <code>cluster</code>、期待每个 <code>cluster</code> 中的样本数量相近、期待每个 <code>cluster</code> 的密度相近。</p>
<hr>
<h1 id="实现-GMM-聚类"><a href="#实现-GMM-聚类" class="headerlink" title="实现 GMM 聚类"></a>实现 GMM 聚类</h1><p>前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 $ \gamma(z_{nk}) $ 的值来对数据集进行聚类。$ \gamma(z_{nk}) $ 给出了样本 $ {\bf x}_n $ 是由 <code>cluster</code> $ k $ 产生的置信程度，最简单的 GMM 聚类即是将样本 $ {\bf x}_n $ 分配给 $ \gamma(z_{nk}) $ 值最大的 <code>cluster</code>。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 <code>GaussianMixture</code> 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。</p>
<h2 id="利用-python-实现-GMM-聚类"><a href="#利用-python-实现-GMM-聚类" class="headerlink" title="利用 python 实现 GMM 聚类"></a>利用 python 实现 GMM 聚类</h2><p>首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GMMClust</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components=<span class="number">2</span>, max_iter=<span class="number">100</span>, tol=<span class="number">1e-10</span>)</span>:</span></div><div class="line">        self.data_set = <span class="keyword">None</span></div><div class="line">        self.n_components = n_components</div><div class="line">        self.pred_label = <span class="keyword">None</span></div><div class="line">        self.gamma = <span class="keyword">None</span></div><div class="line">        self.component_prob = <span class="keyword">None</span></div><div class="line">        self.means = <span class="keyword">None</span></div><div class="line">        self.covars = <span class="keyword">None</span></div><div class="line">        self.max_iter = max_iter</div><div class="line">        self.tol = tol</div><div class="line"></div><div class="line">    <span class="comment"># 计算高斯分布的概率密度函数 </span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_gaussian_prob</span><span class="params">(x, mean, covar, delta=<span class="number">1e-10</span>)</span>:</span></div><div class="line">        n_dim = x.shape[<span class="number">0</span>]</div><div class="line">        covar = covar + delta * np.eye(n_dim)</div><div class="line">        prob = np.exp(<span class="number">-0.5</span> * np.dot((x - mean).reshape(<span class="number">1</span>, n_dim),</div><div class="line">                                    np.dot(np.linalg.inv(covar),</div><div class="line">                                           (x - mean).reshape(n_dim, <span class="number">1</span>))))</div><div class="line">        prob /= np.sqrt(np.linalg.det(covar) * ((<span class="number">2</span> * np.pi) ** n_dim))</div><div class="line">        <span class="keyword">return</span> prob</div><div class="line"></div><div class="line">    <span class="comment"># 计算每一个样本点的似然函数 </span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_sample_likelihood</span><span class="params">(self, i)</span>:</span></div><div class="line">        sample_likelihood = sum(self.component_prob[k] *</div><div class="line">                                self.cal_gaussian_prob(self.data_set[i],</div><div class="line">                                                       self.means[k], self.covars[k])</div><div class="line">                                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_components))</div><div class="line">        <span class="keyword">return</span> sample_likelihood</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, data_set)</span>:</span></div><div class="line">        self.data_set = data_set</div><div class="line">        self.n_samples, self.n_features = self.data_set.shape</div><div class="line">        self.pred_label = np.zeros(self.n_samples, dtype=int)</div><div class="line">        self.gamma = np.zeros((self.n_samples, self.n_components))</div><div class="line"></div><div class="line">        start_time = time.time()</div><div class="line"></div><div class="line">        <span class="comment"># 初始化各参数</span></div><div class="line">        self.component_prob = [<span class="number">1.0</span> / self.n_components] * self.n_components</div><div class="line">        self.means = np.random.rand(self.n_components, self.n_features)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_features):</div><div class="line">            dim_min = np.min(self.data_set[:, i])</div><div class="line">            dim_max = np.max(self.data_set[:, i])</div><div class="line">            self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i]</div><div class="line">        self.covars = np.zeros((self.n_components, self.n_features, self.n_features))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_components):</div><div class="line">            self.covars[i] = np.eye(self.n_features)</div><div class="line"></div><div class="line">        <span class="comment"># 开始迭代</span></div><div class="line">        pre_L = <span class="number">0</span></div><div class="line">        iter_cnt = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> iter_cnt &lt; self.max_iter:</div><div class="line">            iter_cnt += <span class="number">1</span></div><div class="line">            crt_L = <span class="number">0</span></div><div class="line">            <span class="comment"># E 步</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_samples):</div><div class="line">                sample_likelihood = self.cal_sample_likelihood(i)</div><div class="line">                crt_L += np.log(sample_likelihood)</div><div class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_components):</div><div class="line">                    self.gamma[i, k] = self.component_prob[k] * \</div><div class="line">                                       self.cal_gaussian_prob(self.data_set[i],</div><div class="line">                                                              self.means[k],</div><div class="line">                                                              self.covars[k]) / sample_likelihood</div><div class="line">            <span class="comment"># M 步</span></div><div class="line">            effective_num = np.sum(self.gamma, axis=<span class="number">0</span>)</div><div class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_components):</div><div class="line">                self.means[k] = sum(self.gamma[i, k] * self.data_set[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_samples))</div><div class="line">                self.means[k] /= effective_num[k]</div><div class="line">                self.covars[k] = sum(self.gamma[i, k] *</div><div class="line">                                     np.outer(self.data_set[i] - self.means[k],</div><div class="line">                                              self.data_set[i] - self.means[k])</div><div class="line">                                     <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_samples))</div><div class="line">                self.covars[k] /= effective_num[k]</div><div class="line">                self.component_prob[k] = effective_num[k] / self.n_samples</div><div class="line"></div><div class="line">            print(<span class="string">"iteration %s, current value of the log likelihood: %.4f"</span> % (iter_cnt, crt_L))</div><div class="line"></div><div class="line">            <span class="keyword">if</span> abs(crt_L - pre_L) &lt; self.tol:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            pre_L = crt_L</div><div class="line"></div><div class="line">        self.pred_label = np.argmax(self.gamma, axis=<span class="number">1</span>)</div><div class="line">        print(<span class="string">"total iteration num: %s, final value of the log likelihood: %.4f, "</span></div><div class="line">              <span class="string">"time used: %.4f seconds"</span> % (iter_cnt, crt_L, time.time() - start_time))</div><div class="line"></div><div class="line">    <span class="comment"># 可视化算法的聚类结果</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_clustering</span><span class="params">(self, kind, y=None, title=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> kind == <span class="number">1</span>:</div><div class="line">            y = self.pred_label</div><div class="line">        plt.scatter(self.data_set[:, <span class="number">0</span>], self.data_set[:, <span class="number">1</span>],</div><div class="line">                    c=y, alpha=<span class="number">0.8</span>)</div><div class="line">        <span class="keyword">if</span> kind == <span class="number">1</span>:</div><div class="line">            plt.scatter(self.means[:, <span class="number">0</span>], self.means[:, <span class="number">1</span>],</div><div class="line">                        c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>)</div><div class="line">        <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            plt.title(title, size=<span class="number">14</span>)</div><div class="line">        plt.axis(<span class="string">'on'</span>)</div><div class="line">        plt.tight_layout()</div></pre></td></tr></table></figure></p>
<p>创建一个 <code>GMMClust</code> 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 <code>predict</code> 即可对给定的数据集进行 GMM 聚类；方法 <code>plot_clustering</code> 则可以可视化聚类的结果。利用 <code>GMMClust</code> 类进行 GMM 聚类的代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成数据集</span></div><div class="line">n_samples = <span class="number">1500</span></div><div class="line">centers = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">3.5</span>]]</div><div class="line">cluster_std = [<span class="number">2</span>, <span class="number">1.0</span>, <span class="number">0.5</span>]</div><div class="line">X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)</div><div class="line"></div><div class="line"><span class="comment"># 运行 GMM 聚类算法</span></div><div class="line">gmm_cluster = GMMClust(n_components=<span class="number">3</span>)</div><div class="line">gmm_cluster.predict(X)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    print(<span class="string">"cluster %s"</span> % i)</div><div class="line">    print(<span class="string">"    mean: %s, covariance: %s"</span> %(gmm_cluster.means[i], gmm_cluster.covars[i]))</div><div class="line"></div><div class="line"><span class="comment"># 可视化数据集的原始类别情况以及算法的聚类结果</span></div><div class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</div><div class="line">gmm_cluster.plot_clustering(kind=<span class="number">0</span>, y=y, title=<span class="string">'The Original Dataset'</span>)</div><div class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line">gmm_cluster.plot_clustering(kind=<span class="number">1</span>, title=<span class="string">'GMM Clustering Result'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p>以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">iteration 1, current value of the log likelihood: -15761.9757</div><div class="line">iteration 2, current value of the log likelihood: -6435.3937</div><div class="line">iteration 3, current value of the log likelihood: -6410.5633</div><div class="line">iteration 4, current value of the log likelihood: -6399.4306</div><div class="line">iteration 5, current value of the log likelihood: -6389.0317</div><div class="line">iteration 6, current value of the log likelihood: -6377.9131</div><div class="line">iteration 7, current value of the log likelihood: -6367.5704</div><div class="line">iteration 8, current value of the log likelihood: -6359.2076</div><div class="line">iteration 9, current value of the log likelihood: -6350.8678</div><div class="line">iteration 10, current value of the log likelihood: -6338.6458</div><div class="line">... ...</div><div class="line">iteration 35, current value of the log likelihood: -5859.0324</div><div class="line">iteration 36, current value of the log likelihood: -5859.0324</div><div class="line">iteration 37, current value of the log likelihood: -5859.0324</div><div class="line">iteration 38, current value of the log likelihood: -5859.0324</div><div class="line">iteration 39, current value of the log likelihood: -5859.0324</div><div class="line">iteration 40, current value of the log likelihood: -5859.0324</div><div class="line">total iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 seconds</div><div class="line">cluster 0</div><div class="line">    mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269]</div><div class="line"> [0.08460269 3.95599185]]</div><div class="line">cluster 1</div><div class="line">    mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461  -0.00345936]</div><div class="line"> [-0.00345936  0.9630804 ]]</div><div class="line">cluster 2</div><div class="line">    mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954]</div><div class="line"> [0.01120954 0.27281129]]</div></pre></td></tr></table></figure></p>
<div align="center">
<img src="http://free-cn-01.cdn.bilnn.com/ddimg/jfs/t1/102854/22/19383/939345/5e9d22e0E299b5a2e/67904aca69ecfeb9.png" width="1000" height="500" alt="GMM 聚类结果" align="center">
</div>


<p>可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 <code>cluster</code> 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 <code>cluster</code> 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。</p>
<h2 id="利用-sklearn-实现-GMM-聚类"><a href="#利用-sklearn-实现-GMM-聚类" class="headerlink" title="利用 sklearn 实现 GMM 聚类"></a>利用 sklearn 实现 GMM 聚类</h2><p>sklearn 中的  <code>GaussianMixture</code> 类可以用来进行 GMM 聚类，其中的 <code>fit</code> 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；<code>predict</code> 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">12</span>))</div><div class="line"></div><div class="line">n_samples = <span class="number">1500</span></div><div class="line">random_state = <span class="number">170</span></div><div class="line"></div><div class="line"><span class="comment"># 产生一个理想的数据集</span></div><div class="line">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class="line">gmm = GaussianMixture(n_components=<span class="number">3</span>, random_state=random_state)</div><div class="line">gmm.fit(X)</div><div class="line">y_pred = gmm.predict(X)</div><div class="line"></div><div class="line">plt.subplot(<span class="number">221</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</div><div class="line">plt.title(<span class="string">"Normal Blobs"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 产生一个非球形分布的数据集</span></div><div class="line">transformation = [[<span class="number">0.60834549</span>, <span class="number">-0.63667341</span>], [<span class="number">-0.40887718</span>, <span class="number">0.85253229</span>]]</div><div class="line">X_aniso = np.dot(X, transformation)</div><div class="line">gmm = GaussianMixture(n_components=<span class="number">3</span>, random_state=random_state)</div><div class="line">gmm.fit(X_aniso)</div><div class="line">y_pred = gmm.predict(X_aniso)</div><div class="line"></div><div class="line">plt.subplot(<span class="number">222</span>)</div><div class="line">plt.scatter(X_aniso[:, <span class="number">0</span>], X_aniso[:, <span class="number">1</span>], c=y_pred)</div><div class="line">plt.title(<span class="string">"Anisotropicly Distributed Blobs"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 产生一个各 cluster 的密度不一致的数据集</span></div><div class="line">X_varied, y_varied = make_blobs(n_samples=n_samples,</div><div class="line">                                cluster_std=[<span class="number">1.0</span>, <span class="number">2.5</span>, <span class="number">0.5</span>],</div><div class="line">                                random_state=random_state)</div><div class="line">gmm = GaussianMixture(n_components=<span class="number">3</span>, random_state=random_state)</div><div class="line">gmm.fit(X_varied)</div><div class="line">y_pred = gmm.predict(X_varied)</div><div class="line"></div><div class="line">plt.subplot(<span class="number">223</span>)</div><div class="line">plt.scatter(X_varied[:, <span class="number">0</span>], X_varied[:, <span class="number">1</span>], c=y_pred)</div><div class="line">plt.title(<span class="string">"Unequal Density Blobs"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 产生一个各 cluster 的样本数目不一致的数据集</span></div><div class="line">X_filtered = np.vstack((X[y == <span class="number">0</span>][:<span class="number">500</span>], X[y == <span class="number">1</span>][:<span class="number">100</span>], X[y == <span class="number">2</span>][:<span class="number">50</span>]))</div><div class="line">gmm = GaussianMixture(n_components=<span class="number">3</span>, random_state=random_state)</div><div class="line">gmm.fit(X_filtered)</div><div class="line">y_pred = gmm.predict(X_filtered)</div><div class="line"></div><div class="line">plt.subplot(<span class="number">224</span>)</div><div class="line">plt.scatter(X_filtered[:, <span class="number">0</span>], X_filtered[:, <span class="number">1</span>], c=y_pred)</div><div class="line">plt.title(<span class="string">"Unevenly Sized Blobs"</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p>运行结果如下图所示：</p>
<div align="center">
<img src="http://free-cn-01.cdn.bilnn.com/ddimg/jfs/t1/99160/5/19427/295974/5e9d2328Ea6fca295/44ab6241e831cab7.png" width="780" height="650" alt="GMM在不同数据集下的表现" align="center">
</div>


<p>可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 <code>cluster</code>，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 <code>cluster</code> 都是超椭球形的。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/聚类/" rel="tag"># 聚类</a>
          
            <a href="/tags/非监督学习/" rel="tag"># 非监督学习</a>
          
            <a href="/tags/高斯混合模型/" rel="tag"># 高斯混合模型</a>
          
            <a href="/tags/GMM/" rel="tag"># GMM</a>
          
            <a href="/tags/生成模型/" rel="tag"># 生成模型</a>
          
            <a href="/tags/EM-算法/" rel="tag"># EM 算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/聚类分析（二）：k-means-算法/" rel="next" title="聚类分析（二）：k-means 算法">
                <i class="fa fa-chevron-left"></i> 聚类分析（二）：k-means 算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/聚类分析（四）：DBSCAN-算法/" rel="prev" title="聚类分析（四）：DBSCAN 算法">
                聚类分析（四）：DBSCAN 算法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener noreferrer" target="_blank">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/myicon.JPEG" alt="Heathcliff">
            
              <p class="site-author-name" itemprop="name">Heathcliff</p>
              <p class="site-description motion-element" itemprop="description">We are all in the gutter, but some of us are looking at the stars.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    <span class="site-state-item-count">28</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#高斯混合模型"><span class="nav-number">1.</span> <span class="nav-text">高斯混合模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#高斯混合模型简介"><span class="nav-number">1.1.</span> <span class="nav-text">高斯混合模型简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大似然估计问题"><span class="nav-number">1.2.</span> <span class="nav-text">最大似然估计问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EM-算法求解高斯混合模型"><span class="nav-number">2.</span> <span class="nav-text">EM 算法求解高斯混合模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#再探高斯混合模型"><span class="nav-number">3.</span> <span class="nav-text">再探高斯混合模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#推导过程"><span class="nav-number">3.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#与-k-means-算法的关系"><span class="nav-number">3.2.</span> <span class="nav-text">与 k-means 算法的关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实现-GMM-聚类"><span class="nav-number">4.</span> <span class="nav-text">实现 GMM 聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#利用-python-实现-GMM-聚类"><span class="nav-number">4.1.</span> <span class="nav-text">利用 python 实现 GMM 聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#利用-sklearn-实现-GMM-聚类"><span class="nav-number">4.2.</span> <span class="nav-text">利用 sklearn 实现 GMM 聚类</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heathcliff</span>

  

  
</div>









<span id="busuanzi_container_site_pv">
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.0"></script>



  


  

    
      <script id="dsq-count-scr" src="https://heathcliff.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://heathcliff.me/聚类分析（三）：高斯混合模型与EM算法/';
          this.page.identifier = '聚类分析（三）：高斯混合模型与EM算法/';
          this.page.title = '聚类分析（三）：高斯混合模型与 EM 算法';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://heathcliff.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("s2RkSDxfHn2dwq6oHcIhF6UI-gzGzoHsz", "MhK9geETXDua79cxpft6W6ne");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

  
</body>
</html>
