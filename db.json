{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/myicon.JPEG","path":"images/myicon.JPEG","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"d7a8fda1dc4460218cc07594a5411ba871221c11","modified":1515322985398},{"_id":"source/baidu_verify_5OgYFbhfJP.html","hash":"2422b18c2e1d556854dfa233b390f6de2be3c158","modified":1515322985399},{"_id":"source/.DS_Store","hash":"be61233959fa51147d4d034218a97a17eb29e63e","modified":1520862675987},{"_id":"source/robots.txt","hash":"7eae66f5e4450beaee467197f6fcf992aa249660","modified":1515322985399},{"_id":"source/_posts/Python-中的-Unicode-string-和-Byte-string.md","hash":"6029d38a95b86bf5ed0bc8840413d79021563406","modified":1521693621536},{"_id":"source/_posts/线性回归模型.md","hash":"7f3bacabdbb08c29144570e4e3d9157383bd74b2","modified":1522332872096},{"_id":"source/_posts/聚类分析（一）：层次聚类算法.md","hash":"0b7724e3f9fdfcc1e82534582950420352b348d9","modified":1521720889825},{"_id":"source/_posts/.DS_Store","hash":"3b5b2384914637530ef6643ae5204f7f6c86acc1","modified":1521727540542},{"_id":"source/_posts/聚类分析（三）：高斯混合模型与EM算法.md","hash":"05781f54ec3070cad38ff4a0f29a56d79e4f6396","modified":1522332679653},{"_id":"source/_posts/聚类分析（二）：k-means-算法.md","hash":"5b28c20271c2c3431b1c3f4d73a8c421d0d7084c","modified":1521720894345},{"_id":"source/_posts/聚类分析（四）：DBSCAN-算法.md","hash":"5b9415c45e2df27ffe2613b25693f1c9fa57838a","modified":1522111464377},{"_id":"source/categories/index.md","hash":"97243f97ee3cd4cb7068093e927bfb542f0a81a0","modified":1515322985399},{"_id":"source/clustering-analysis/index.md","hash":"ad947bd89a783820c6e1c4ebafbf77a4d2bb087e","modified":1521721390787},{"_id":"source/clustering-analysis/.DS_Store","hash":"6e873cb33a9b8d0ecae3cffec539efb5fefdfc47","modified":1520862667656},{"_id":"source/tags/index.md","hash":"7c4dc23d0360b97d00d7f4f5fc142534fa624c69","modified":1515322985399},{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1515328664665},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1515328664677},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1515328664677},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1515328664677},{"_id":"themes/next/.DS_Store","hash":"4124594213d3edf2a74e0a2c67b3a8a31da1e041","modified":1518262180751},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1515328664679},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1515328664679},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1515328664682},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1515328664682},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1515328664682},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1515328664683},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1515328664682},{"_id":"themes/next/README.md","hash":"3d438555ca87b1d247536b3b56fc0672eb001518","modified":1515328664683},{"_id":"themes/next/_config.yml","hash":"4e478e55c9d4e9085790f42102c8e0f05387af06","modified":1518263210253},{"_id":"themes/next/bower.json","hash":"2fd34aa37640c87abb391af760569b7e8d54877a","modified":1515328664684},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1515328664687},{"_id":"themes/next/package.json","hash":"b1a8c06318a8edb227a487906ac7b43a21ed68e6","modified":1515328664703},{"_id":"source/_posts/聚类分析（一）：层次聚类算法/.DS_Store","hash":"7a70a7fd3ed133e1c62f6962fab3cf58a90d322e","modified":1517756295470},{"_id":"source/_posts/聚类分析（一）：层次聚类算法/a_dendrogram.PNG","hash":"73c06db16d026e6deed32ccd78e93fc0d048db6b","modified":1515322985398},{"_id":"source/_posts/聚类分析（一）：层次聚类算法/illustration_for_hierarchical_clustering.PNG","hash":"b6c33017e361be11206a07fdf34173fbcdafbe76","modified":1517630557948},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"792b4e3c3544d51164e8a414219dc1b388dc65dc","modified":1515328664678},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"054be54a22f3aea601a29334c7577ffa793f1a48","modified":1515328664678},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1515328664678},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1515328664678},{"_id":"themes/next/.github/stale.yml","hash":"1bbdd20d025010ec57225712be82988a26485836","modified":1515328664678},{"_id":"themes/next/docs/DATA-FILES.md","hash":"de63aa8466ee8c4d4b418dfbe4e8f27fa117751d","modified":1515328664685},{"_id":"themes/next/docs/INSTALLATION.md","hash":"01a383fd1d46752d4b4f22047b535127b44adb75","modified":1515328664685},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1515328664684},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1515328664684},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1515328664685},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1515328664685},{"_id":"themes/next/layout/_layout.swig","hash":"3d13af30b4e7ee7bb6a03eb271b786992e89f35e","modified":1518262706622},{"_id":"themes/next/layout/.DS_Store","hash":"9e6f14c0afb92f2d558b5fd57a3529cc0d598e1d","modified":1518262951929},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1515328664702},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1515328664702},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1515328664702},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1515328664702},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1515328664703},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1515328664703},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1515328664703},{"_id":"themes/next/languages/de.yml","hash":"92964c8ed184fa959a5e2001e7c6d9a07d7a4344","modified":1515328664687},{"_id":"themes/next/languages/default.yml","hash":"b39706b2d22696eed6b036f4c90aa5362e331090","modified":1515328664688},{"_id":"themes/next/languages/en.yml","hash":"df63017548589b2e567647e08d736c2a7f342b12","modified":1515328664688},{"_id":"themes/next/languages/fr-FR.yml","hash":"6ee34c8103a95839207dac1201fef7a8f727d2fc","modified":1515328664688},{"_id":"themes/next/languages/id.yml","hash":"60473cc81a871ceb868c97cd3291d602fda1b338","modified":1515328664688},{"_id":"themes/next/languages/it.yml","hash":"5a8a29d145dd2cd882d52b3fbb1203c1a3540cbd","modified":1515328664688},{"_id":"themes/next/languages/ko.yml","hash":"73253087d7754a0213e2ad72de16ab5138b9ba54","modified":1515328664689},{"_id":"themes/next/languages/ja.yml","hash":"8a3acfb56dc783f261b640dca662c0ec431fea6f","modified":1515328664688},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1515328664689},{"_id":"themes/next/languages/pt-BR.yml","hash":"4d017b7b9542050b87a99931dff98889090fc781","modified":1515328664689},{"_id":"themes/next/languages/ru.yml","hash":"4ba9f4971115bce0213c437158424428e0e13d5a","modified":1515328664689},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1515328664690},{"_id":"themes/next/languages/zh-Hans.yml","hash":"ce7413e9619d55e794dcab1e914bc7359a3c5568","modified":1515328664690},{"_id":"themes/next/languages/pt.yml","hash":"95585897cd108d5ff15e998cd0acdc0fba5d572c","modified":1515328664689},{"_id":"themes/next/scripts/merge-configs.js","hash":"cb617ddf692f56e6b6129564d52e302f50b28243","modified":1515328664703},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1515328664704},{"_id":"themes/next/languages/zh-hk.yml","hash":"54e4aa1d04ccad1e13cf08124fe5f70a930592dd","modified":1515328664690},{"_id":"themes/next/languages/zh-tw.yml","hash":"280dd00495e90b8a8c4d9351bab8ae65c78bbe87","modified":1515328664690},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1515328664736},{"_id":"themes/next/source/.DS_Store","hash":"ed7a8c8b3e452bd82d525d31f7be9c573535da07","modified":1517757972892},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1515328664736},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1515328664737},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664721},{"_id":"themes/next/docs/cn/DATA-FILES.md","hash":"58c58c7d98365395dba904a87f9b5f5e0b1e99cb","modified":1515328664685},{"_id":"themes/next/docs/cn/UPDATE-FROM-5.1.X.md","hash":"b6422e0e1bbb02ddd29a2f9969fc3ff709555560","modified":1515328664686},{"_id":"themes/next/docs/cn/INSTALLATION.md","hash":"15b09b7cf4e4159858895e37ce334fcc96ac08d7","modified":1515328664686},{"_id":"themes/next/docs/cn/README.md","hash":"68b98e7489b8d53e8367500507b9ce43d094d48b","modified":1515328664686},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"1f15b876c106bae74148fb526bc3b914721b8ff2","modified":1515328664686},{"_id":"themes/next/docs/ru/README.md","hash":"95a904683da99daf1473fb7c1ffe8a6322ef8fb9","modified":1515328664687},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"43a70e456d15e9eab3753c22a5253ed2ff300ac8","modified":1515328664687},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1515328664687},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1515328664691},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1515328664690},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1515328664691},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1515328664691},{"_id":"themes/next/layout/_macro/reward.swig","hash":"aa620c582143f43ba1cb1a5e59240041a911185b","modified":1515328664692},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43f13d75cfb37ec4ed5386bee0f737641977200b","modified":1515328664692},{"_id":"themes/next/layout/_macro/post.swig","hash":"dc0fab8ebae5af50c446c22ac2e05a1fa7901996","modified":1518262641100},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"fea45ab314b9ea23edab25c2b8620f909d856b1d","modified":1515328664692},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1515328664692},{"_id":"themes/next/layout/_partials/head.swig","hash":"4dd4f0f4c6d8fbf2099419be6adcf7e3b051044d","modified":1515328664693},{"_id":"themes/next/layout/_partials/footer.swig","hash":"b6107f514f05463093155d243ee2851fab44b80d","modified":1518263278584},{"_id":"themes/next/layout/_partials/header.swig","hash":"aadbc692014e9eac821e36af6107406c476610b9","modified":1515328664693},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1515328664693},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1515328664694},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1515328664693},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1515328664700},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1515328664700},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1515328664700},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1515328664700},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1515328664700},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1515328664701},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1515328664700},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1515328664695},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1515328664695},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1515328664696},{"_id":"themes/next/layout/_scripts/.DS_Store","hash":"faba99ac180fdcb243d1bf54ebb095b6dc3fd1a6","modified":1518262547817},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1515328664704},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1515328664704},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1515328664704},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1515328664704},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1515328664704},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1515328664705},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1515328664705},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1515328664705},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1515328664705},{"_id":"themes/next/source/css/.DS_Store","hash":"cd7ddee7cabb2bbc8d45d548626ce4db3f2c199b","modified":1517757980827},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1515328664721},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1515328664721},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1515328664722},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1515328664721},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1515328664722},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1515328664722},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1515328664722},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1515328664722},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1515328664723},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1515328664722},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1515328664723},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1515328664723},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1515328664723},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515328664723},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1515328664723},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515328664723},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1515328664724},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1515328664724},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1515328664724},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664696},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664696},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664721},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664721},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664717},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664717},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1515328664717},{"_id":"themes/next/source/images/myicon.JPEG","hash":"fda640d647f76083c5dc732a4ef3ce48d7943e5a","modified":1515327869821},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1515328664693},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1515328664693},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1515328664694},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1515328664694},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1515328664694},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1515328664694},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1515328664695},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1515328664695},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1515328664695},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1515328664696},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1515328664696},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1515328664697},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1515328664698},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1515328664698},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1515328664698},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1515328664698},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1515328664698},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1515328664699},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1515328664700},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1515328664701},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1515328664701},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1515328664702},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1515328664702},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1515328664695},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1515328664696},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1515328664696},{"_id":"themes/next/source/css/_common/.DS_Store","hash":"e44538df9251afe60033443fd99bbd1fd9a3d539","modified":1517757980828},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1515328664720},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"1987af9e0821a58f65adc58ecc96859a064309bb","modified":1515330684538},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1515328664721},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1515328664721},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1515328664716},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1515328664717},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1515328664717},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1515328664717},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1515328664728},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1515328664728},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1515328664728},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1515328664729},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1515328664729},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1515328664735},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1515328664736},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1515328664725},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1515328664736},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1515328664725},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"531cdedd7fbe8cb1dab2e4328774a9f6b15b59da","modified":1515328664725},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1515328664726},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1515328664726},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1515328664726},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1515328664727},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1515328664726},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1515328664727},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1515328664727},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1515328664727},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1515328664733},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1515328664701},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1515328664701},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1515328664705},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1515328664705},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/.DS_Store","hash":"fb3778c07f7d2bcca4094ff134265a10e1346f8e","modified":1517757986271},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1515328664712},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1515328664715},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1515328664715},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1515328664716},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1515328664716},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1515328664716},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1515328664716},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1515328664716},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1515328664717},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1515328664717},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1515328664717},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"bb20cbd5df6433af2746a0a59aca0415c77bc17e","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"3bc53d98f7ab29b48f0d7a6212c6488c5458ed88","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1515328664720},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d204ef41e5f59aa102baf02e1751075a041ae7f4","modified":1515328664720},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1515328664720},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1515328664720},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1515328664720},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1515328664720},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1515328664729},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1515328664730},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1515328664729},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1515328664734},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1515328664734},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1515328664727},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1515328664735},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1515328664732},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1515328664706},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1515328664732},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1515328664706},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1515328664707},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1515328664708},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"0abb074afb6ab7242a22fa6dc3ac688251df708a","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"7a371ffa99a03cbc8051e3b04b5d2f7678e1767f","modified":1517758220075},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1515328664709},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1515328664710},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1515328664711},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1515328664712},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1515328664712},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1515328664712},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1515328664712},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1515328664714},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1515328664714},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1515328664713},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1515328664714},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1515328664714},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1515328664715},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1515328664714},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1515328664715},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1515328664715},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1515328664715},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1515328664715},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1515328664718},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1515328664719},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1515328664719},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1515328664731},{"_id":"public/baidu_verify_5OgYFbhfJP.html","hash":"74fe866c3ed7fc49318f99a7df8a3c918392e53b","modified":1522332892267},{"_id":"public/baidusitemap.xml","hash":"4207d64b084fdbe4d68dad4d59a082d9846e7e55","modified":1522332892319},{"_id":"public/atom.xml","hash":"476b82c5e07026bd439ab6b228606dbde5a2a96e","modified":1522332892326},{"_id":"public/search.xml","hash":"4dadf3310ee3b2f48a9a25ab70c15878ff2a5b40","modified":1522332892326},{"_id":"public/sitemap.xml","hash":"3fb1221786e96e6e6b7a54e4de49490b2d0a5317","modified":1522332892334},{"_id":"public/categories/index.html","hash":"f0a25fea811df4c59d8b7c805dcb90a39b26c8d9","modified":1522332892440},{"_id":"public/clustering-analysis/index.html","hash":"02a4ead6000850432980964a61f64e2c7cbacb13","modified":1522332892446},{"_id":"public/tags/index.html","hash":"3f80cb79203910efde52492a762bc8d6e3f98828","modified":1522332892447},{"_id":"public/聚类分析（四）：DBSCAN-算法/index.html","hash":"d2a0b83e1ec746a7f2538af2f9b02148eb7b1f89","modified":1522332892447},{"_id":"public/聚类分析（三）：高斯混合模型与EM算法/index.html","hash":"684bd6148ac63d37155f44f8350c502d75d1836e","modified":1522332892448},{"_id":"public/聚类分析（二）：k-means-算法/index.html","hash":"46930d08c25af9478eb4acea052d0254d97cff5a","modified":1522332892448},{"_id":"public/聚类分析（一）：层次聚类算法/index.html","hash":"7d9561791a836c56686d76724a29a27af48b351c","modified":1522332892448},{"_id":"public/Python-中的-Unicode-string-和-Byte-string/index.html","hash":"12849884c7e086a4ed40d3136f7d5688526daabe","modified":1522332892448},{"_id":"public/archives/index.html","hash":"78df0bd895cb21a92c4a7f108c5765980d549216","modified":1522332892448},{"_id":"public/archives/2017/index.html","hash":"e2575167a84e5be7c3d7dc3a384c7f4f64524c44","modified":1522332892448},{"_id":"public/archives/2017/10/index.html","hash":"9d5b800f8a9115c165d4d0b3b185ee00f1c83985","modified":1522332892449},{"_id":"public/archives/2017/12/index.html","hash":"a36ac185cc733edd7c1dedf01f15d6b7f0a30c34","modified":1522332892449},{"_id":"public/archives/2018/index.html","hash":"334cbc34b1f157cc4e5a3803ae3db53b832fe517","modified":1522332892449},{"_id":"public/archives/2018/02/index.html","hash":"fd48e866a84efb6b038d07ef7cad0c4f9148fc23","modified":1522332892449},{"_id":"public/archives/2018/03/index.html","hash":"6d0bb9caa50f2502f4823a4f69035763b0befe16","modified":1522332892449},{"_id":"public/categories/python-notes/index.html","hash":"2e755683d5fafd513a243c6ac10dc6db3c817608","modified":1522332892449},{"_id":"public/categories/machine-learning-algorithms/index.html","hash":"5908fd7bd22786d060520c97b39b28e9316c34ea","modified":1522332892449},{"_id":"public/index.html","hash":"2cb37d3a154ce9be0e38d010679982fd2d40f48b","modified":1522332892449},{"_id":"public/tags/python/index.html","hash":"d10ebe43fad46207b2e632b254b7c31730bc6990","modified":1522332892449},{"_id":"public/tags/unicode/index.html","hash":"72934f6adbe75ef571301b74c870fbb043e65feb","modified":1522332892449},{"_id":"public/tags/聚类/index.html","hash":"ab96e7037c495ee6d114b705eb114542fcd05994","modified":1522332892449},{"_id":"public/tags/非监督学习/index.html","hash":"f4ee65733104dfd152b0b543fd22b6105b344bf8","modified":1522332892449},{"_id":"public/tags/k-means-算法/index.html","hash":"359498c646a65184d7d34e89cfb6f6a179d25663","modified":1522332892449},{"_id":"public/tags/密度聚类/index.html","hash":"3ec13b4a415ff7196e95b08242c9c194e93bd832","modified":1522332892449},{"_id":"public/tags/层次聚类/index.html","hash":"0e76d75475495c20454f81d6d4070af68f01b287","modified":1522332892450},{"_id":"public/tags/高斯混合模型/index.html","hash":"8592e6eaf2b3f6ab517a53de98ef5cccc65e5c5f","modified":1522332892450},{"_id":"public/tags/GMM/index.html","hash":"ec800a0f03aa60cc0b61d58b06a160e545cc30a5","modified":1522332892450},{"_id":"public/tags/生成模型/index.html","hash":"8907a7974c1b3ce07873807e3df60e47c13e6854","modified":1522332892450},{"_id":"public/tags/EM-算法/index.html","hash":"5dd4dd27ebdac42b4089efa5a148bddd97af7ce7","modified":1522332892450},{"_id":"public/线性回归模型/index.html","hash":"450d5253821619af6a5f2e8976394258f3421c1f","modified":1522332892455},{"_id":"public/tags/线性模型/index.html","hash":"baa89f229808967334e35208efd8beb1f914cc85","modified":1522332892455},{"_id":"public/tags/线性回归/index.html","hash":"6e82ab513bf8f0674f81d6480d22e6848027f7eb","modified":1522332892456},{"_id":"public/tags/监督学习/index.html","hash":"fe66dc2448153c10b9195a4d753eeb1a1bcd48ca","modified":1522332892456},{"_id":"public/CNAME","hash":"d7a8fda1dc4460218cc07594a5411ba871221c11","modified":1522332892458},{"_id":"public/robots.txt","hash":"7eae66f5e4450beaee467197f6fcf992aa249660","modified":1522332892458},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1522332892458},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1522332892458},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1522332892458},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1522332892458},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1522332892458},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1522332892458},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1522332892458},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1522332892459},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1522332892459},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1522332892459},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1522332892459},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1522332892459},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1522332892459},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1522332892459},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1522332892459},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1522332892459},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1522332892459},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1522332892459},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1522332892459},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1522332892459},{"_id":"public/聚类分析（一）：层次聚类算法/a_dendrogram.PNG","hash":"73c06db16d026e6deed32ccd78e93fc0d048db6b","modified":1522332892459},{"_id":"public/images/myicon.JPEG","hash":"fda640d647f76083c5dc732a4ef3ce48d7943e5a","modified":1522332892911},{"_id":"public/聚类分析（一）：层次聚类算法/illustration_for_hierarchical_clustering.PNG","hash":"b6c33017e361be11206a07fdf34173fbcdafbe76","modified":1522332892912},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1522332892914},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1522332892914},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1522332892919},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1522332892919},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1522332892919},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1522332892919},{"_id":"public/js/src/bootstrap.js","hash":"531cdedd7fbe8cb1dab2e4328774a9f6b15b59da","modified":1522332892919},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1522332892919},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1522332892919},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1522332892919},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1522332892919},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1522332892919},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1522332892919},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1522332892919},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1522332892919},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1522332892919},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1522332892919},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1522332892919},{"_id":"public/css/main.css","hash":"9decec15dae087e34e5792c1a96568ee8cecb245","modified":1522332892920},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1522332892920},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1522332892920},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1522332892920},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1522332892920},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1522332892920},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1522332892920},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1522332892920}],"Category":[{"name":"python学习笔记","_id":"cjfclp3tt0003hws6adgqydny"},{"name":"机器学习算法","_id":"cjfclp3tz0008hws6natj7trl"}],"Data":[],"Page":[{"layout":"false","sitemap":false,"_content":"5OgYFbhfJP","source":"baidu_verify_5OgYFbhfJP.html","raw":"layout: false\nsitemap: false\n---\n5OgYFbhfJP","date":"2018-01-07T11:03:05.399Z","updated":"2018-01-07T11:03:05.399Z","path":"baidu_verify_5OgYFbhfJP.html","title":"","comments":1,"_id":"cjfclp3su0000hws6jx3rp2bl","content":"5OgYFbhfJP","site":{"data":{}},"excerpt":"","more":"5OgYFbhfJP"},{"title":"分类","date":"2017-10-27T12:57:03.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2017-10-27 20:57:03\ntype: 'categories'\ncomments: false\n---\n","updated":"2018-01-07T11:03:05.399Z","path":"categories/index.html","layout":"page","_id":"cjfclp3v1001mhws6yoqyoscx","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"聚类分析系列文章","date":"2018-03-12T13:50:41.000Z","_content":"- [聚类分析（一）：层次聚类算法][1]\n- [聚类分析（二）：k-means 算法][2]\n- [聚类分析（三）：高斯混合模型与 EM 算法][3]\n- [聚类分析（四）：DBSCAN 算法][4]\n\n[1]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\n[3]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[4]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95/","source":"clustering-analysis/index.md","raw":"---\ntitle: 聚类分析系列文章\ndate: 2018-03-12 21:50:41\n---\n- [聚类分析（一）：层次聚类算法][1]\n- [聚类分析（二）：k-means 算法][2]\n- [聚类分析（三）：高斯混合模型与 EM 算法][3]\n- [聚类分析（四）：DBSCAN 算法][4]\n\n[1]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\n[3]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[4]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95/","updated":"2018-03-22T12:23:10.787Z","path":"clustering-analysis/index.html","comments":1,"layout":"page","_id":"cjfclp3v2001nhws61kt8lg38","content":"<ul>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\">聚类分析（一）：层次聚类算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\">聚类分析（二）：k-means 算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\">聚类分析（三）：高斯混合模型与 EM 算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95/\">聚类分析（四）：DBSCAN 算法</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\">聚类分析（一）：层次聚类算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\">聚类分析（二）：k-means 算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\">聚类分析（三）：高斯混合模型与 EM 算法</a></li>\n<li><a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95/\">聚类分析（四）：DBSCAN 算法</a></li>\n</ul>\n"},{"title":"标签","date":"2017-10-27T12:56:41.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2017-10-27 20:56:41\ntype: 'tags'\ncomments: false\n---\n","updated":"2018-01-07T11:03:05.399Z","path":"tags/index.html","layout":"page","_id":"cjfclp3v2001ohws6gdh4789s","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Python 中的 Unicode String 和 Byte String","layout":"post","date":"2017-10-27T13:32:38.000Z","keywords":"python,unicode,string","_content":"\n# python 2.x 和 python 3.x 字符串类型的区别\npython 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。\n\n## python 2.x\n- `str` 类型：处理 binary 数据和 ASCII 文本数据。\n- `unicode` 类型：处理**非 ASCII** 文本数据。\n\n## python 3.x\n- `bytes` 类型：处理 binary 数据，同 `str` 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0\\~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。\n- `str` 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。\n- `bytearray` 类型：`bytes` 类型的变种，但是此类型是 **mutable** 的。\n\n---- \n\n# Unicode 简介\n包括 **ASCII 码**、**latin-1 编码** 和 **utf-8 编码** 等在内的码都被认为是 unicode 码。\n## 编码和解码的概念\n- 编码（encoding）：将字符串映射为一串原始的字节。\n- 解码（decoding）：将一串原始的字节翻译成字符串。\n\n## ASCII码\n- 编码长度为 1 个 byte.\n- 编码范围为 `0x00`\\~`0x7F`，只包含一些常见的字符。\n\n## latin-1码\n- 编码长度为 1 个 byte.\n- 编码范围为 `0x00`\\~`0xFF`，能支持更多的字符（如 accent character），兼容 ASCII 码。\n\n## utf-8码\n- 编码长度可变，为 1\\~4 个 byte。\n- 当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 `0x00` \\~ `0x7F`；当编码长度大于 1 个 byte 时，每个 byte 的取值为 `0x80` \\~ `0xFF`。\n\n## 其它编码\n- utf-16，编码长度为定长 2 个 byte。\n- utf-32，编码长度为定长 4 个 byte。\n\n---- \n\n# Unicode 字符串的存储方式\n## 在内存中的存储方式\nunicode 字符串中的字符在内存中以一种**与编码方式无关**的方式存储：[unicode code point][1]，它是一个数字，范围为 0\\~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，`a` 和 `b` 表示的是同一字符串（其中 `'\\uNNNN'` 即为 unicode code point，`N` 为一个十六进制位，十六进制位的个数为 4\\~6 位；当 unicode code point 的取值在 0\\~255 范围内时，也可以 `'\\xNN'` 的形式表示）：\n```python\n# python 2.7\n>>> a = u'\\u5a1c\\u5854\\u838e'\n>>> b = u'娜塔莎'\n>>> print a, b\n娜塔莎 娜塔莎\n>>> c = u'\\xe4'\n>>> print c\nä\n```\n## 在文件等外部媒介中的存储方式\nunicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。\n\n# 字符表示\n## python 3.x\n在 python 3.x 中，`str` 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：\n```python\n# python 3.6\n>>> a = 'Natasha, 娜塔莎'\n>>> type(a)\nstr\n>>> len(a)\n12\n```\n可以看到，python 3.x 中得到的 `a` 的长度为 12（包含空格），没有任何问题；我们可以对 `a` 进行编码，将其转换为 `bytes` 类型：\n```python\n# python 3.6\n>>> b = a.encode('utf-8')\n>>> b\nb'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> type(b)\nbytes\n```\n从上面可以看出，`bytes` 类型的对象中的某个字节的取值在 `0x00` \\~ `0x7F` 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。\n\n同理，我们也可以将一个 `bytes` 类型的对象译码为一个 `str` 类型的对象：\n```python\n# python 3.6\n>>> a = b.decode('utf-8')\n>>> a\n'Natasha, 娜塔莎'\n```\n\n## python 2.x\n在 python 2.x 中，如果还是用 `str` 类型来表示国际字符，就会有问题：\n```python \n# python 2.7\n>>> a = 'Natasha, 娜塔莎'\n>>> type(a)\nstr\n>>> a\n'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> len(a)\n18\n>>> print a\nNatasha, 娜塔莎\n```\n可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 `str` 字符串，但实际上 `a` 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 `a` 正好是字符串 `'Natasha, 娜塔莎'` 的 utf-8 编码的结果，且将 `a` 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:\n```python\n# python 2.7\n>>> import sys\n>>> sys.stdin.encoding  # 控制台的输入编码，可解释前例中 a 在内存中的表现形式\n'utf-8'\n>>> sys.stdout.encoding # 控制台的输出编码，可解释前例中打印 a 的显示结果\n'utf-8'\n```\n另外，`sys.getdefaultencoding()`函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 `'ascii'`, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 `json.dumps()` 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：\\>）。\n\n切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 `unicode` 类型字符串，如下所示：\n```python\n# python 2.7\n>>> a = u'Natasha, 娜塔莎'\n>>> type(a)\nunicode\n>>> len(a)\n12\n>>> b = a.encode('utf-8')\n>>> b\n'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> type(b)\nstr\n>>> a = b.decode('utf-8')\n>>> a\nu'Natasha, \\u5a1c\\u5854\\u838e'\n```\n另外，我们可以对 `unicode` 类型字符串进行编码操作，对 `str` 类型字符串进行译码操作。\n\n---- \n\n# 文本文件操作\n## python 3.x\n在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 `open` 函数的参数 `encoding` 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 `sys.getfilesystemencoding()`得到，如：\n```python\n# python 3.6\n>>> import sys\n>>> sys.getfilesystemencoding()\n'utf-8'\n>>> a = '娜塔莎'\n>>> f = open('data.txt', 'w', encoding='utf-8')\n>>> f.write(a)\n3\n>>> f.close()\n>>> f = open('data.txt', 'w', encoding='utf-8')\n>>> f.read()\n'娜塔莎'\n>>> f.close()\n```\n当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：\n```python\n# python 3.6\n>>> a = '娜塔莎'\n>>> b = a.encode('utf-8')\n>>> f = open('data.txt', 'wb')\n>>> f.write(b)\n9\n>>> f.close()\n>>> f.read().decode('utf-8')\n'娜塔莎'\n>>> f.close()\n```\n## python 2.x\n在 python 2.x 中，`open` 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 `str`；`codecs.open` 函数则支持自动读写 unicode 文本文件，如：\n```python\n# python 2.7\n>>> import codecs\n>>> a = u'安德烈'\n>>> f = codecs.open('data.txt', 'w', encoding='utf-8')\n>>> f.write(a)\n>>> f.close()\n>>> f = codecs.open('data.txt', 'r', encoding='utf-8') \n>>> print f.read()\n安德烈\n>>> f.close()\n```\n类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：\n```python\n# python 2.7\n>>> b = a.encode('utf-8')\n>>> f = open('data.txt', 'w')\n>>> f.write(b)\n>>> f.close()\n>>> f = open('data.txt', 'r')\n>>> print f.read().decode('utf-8')\n安德烈\n>>> f.close()\n```\n总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。\n\n---- \n\n# python 2.x 中的 json.dumps() 操作\njson 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。\n\n对于数据结构中的字符串类型为 `str`、 但实际上定义的是一个国际字符串的情况，`json.dumps()` 的结果如下：\n```python\n# python 2.7\n>>> a = {'Natasha': '娜塔莎'}\n>>> a_json_1 = json.dumps(a)\n>>> a_json_1\n'{\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"}'\n>>> a_json_2 = json.dumps(a, ensure_ascii=False)\n>>> a_json_2\n'{\"Natasha\": \"\\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e\"}'\n```\n可以看到，在这种情形下，当 `ensure_ascii` 为 `True` 时，`json.dumps()` 操作返回值的类型为 `str`，其会将 `a` 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 `'\\\\u5a1c'` 对应 `6` 个字符而非 `1` 个）；当 `ensure_ascii` 为 `False` 时，`json.dumps()` 操作的返回值类型仍然为 `str`，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 `a_json_2` 译码就可以得到我们想要的 json：\n```python\n# python 2.7\n>>> a_json_2.decode('utf-8')\nu'{\"Natasha\": \"\\u5a1c\\u5854\\u838e\"}'\n>>> print a_json_2.decode('utf-8')\n{\"Natasha\": \"娜塔莎\"}\n```\n对于数据结构中的字符串类型为 `unicode` 的情况，`json.dumps()` 的结果如下：\n```python\n# python 2.7\n>>> u = {u'Natasha': u'娜塔莎'}\n>>> u_json_1 = json.dumps(u)\n>>> u_json_1\n'{\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"}'\n>>> u_json_2 = json.dumps(u, ensure_ascii=False)\n>>> u_json_2\nu'{\"Natasha\": \"\\u5a1c\\u5854\\u838e\"}'\n>>> print u_json_2\n{\"Natasha\": \"娜塔莎\"}\n```\n在这种情形下，当 `ensure_ascii` 为 `True` 时，`json.dumps()` 操作返回值的类型为 `str`，其得到的结果和前面对 a 操作返回的结果完全一样；而当`ensure_ascii` 为 `False` 时，`json.dumps()` 操作的返回值类型变为 `unicode`，原始数据结构中的中文字符在返回值中完整地保留了下来。\n\n对于数据结构中的字符串类型既有 `unicode` 又有 `str` 的情形，运用 `json.dumps()` 时将 `ensure_ascii` 设为 `False` 的情况又会完全不同。\n\n当数据结构中的 ASCII 字符串为 `str` 类型，国际字符串为 `unicode` 类型时（如 `u = {'Natasha': u'娜塔莎'}`），`json.dumps()` 的返回值是正常的、符合预期的 `unicode` 字符串；当数据结构中有国际字符串为 `str` 类型，又存在其他字符串为 `unicode` 类型时（如 `u = {u'Natasha': '娜塔莎'}` 或 `u = {u'娜塔莉娅': '娜塔莎'}`），`json.dumps()` 会抛出异常 `UnicodeDecodeError`，这是因为系统会将数据结构中 `str` 类型字符串都转换为 `unicode` 类型，而系统的默认编译码方式为 ascii 编码，因而对 `str` 类型的国际字符串进行 ascii 译码就必然会出错。\n\n[1]:\thttps://www.wikiwand.com/en/List_of_Unicode_characters","source":"_posts/Python-中的-Unicode-string-和-Byte-string.md","raw":"---\ntitle: Python 中的 Unicode String 和 Byte String\nlayout: post\ndate: 2017-10-27 21:32:38\ntags: \n- python\n- unicode\ncategories: \n- python学习笔记\nkeywords: python,unicode,string\n---\n\n# python 2.x 和 python 3.x 字符串类型的区别\npython 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。\n\n## python 2.x\n- `str` 类型：处理 binary 数据和 ASCII 文本数据。\n- `unicode` 类型：处理**非 ASCII** 文本数据。\n\n## python 3.x\n- `bytes` 类型：处理 binary 数据，同 `str` 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0\\~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。\n- `str` 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。\n- `bytearray` 类型：`bytes` 类型的变种，但是此类型是 **mutable** 的。\n\n---- \n\n# Unicode 简介\n包括 **ASCII 码**、**latin-1 编码** 和 **utf-8 编码** 等在内的码都被认为是 unicode 码。\n## 编码和解码的概念\n- 编码（encoding）：将字符串映射为一串原始的字节。\n- 解码（decoding）：将一串原始的字节翻译成字符串。\n\n## ASCII码\n- 编码长度为 1 个 byte.\n- 编码范围为 `0x00`\\~`0x7F`，只包含一些常见的字符。\n\n## latin-1码\n- 编码长度为 1 个 byte.\n- 编码范围为 `0x00`\\~`0xFF`，能支持更多的字符（如 accent character），兼容 ASCII 码。\n\n## utf-8码\n- 编码长度可变，为 1\\~4 个 byte。\n- 当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 `0x00` \\~ `0x7F`；当编码长度大于 1 个 byte 时，每个 byte 的取值为 `0x80` \\~ `0xFF`。\n\n## 其它编码\n- utf-16，编码长度为定长 2 个 byte。\n- utf-32，编码长度为定长 4 个 byte。\n\n---- \n\n# Unicode 字符串的存储方式\n## 在内存中的存储方式\nunicode 字符串中的字符在内存中以一种**与编码方式无关**的方式存储：[unicode code point][1]，它是一个数字，范围为 0\\~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，`a` 和 `b` 表示的是同一字符串（其中 `'\\uNNNN'` 即为 unicode code point，`N` 为一个十六进制位，十六进制位的个数为 4\\~6 位；当 unicode code point 的取值在 0\\~255 范围内时，也可以 `'\\xNN'` 的形式表示）：\n```python\n# python 2.7\n>>> a = u'\\u5a1c\\u5854\\u838e'\n>>> b = u'娜塔莎'\n>>> print a, b\n娜塔莎 娜塔莎\n>>> c = u'\\xe4'\n>>> print c\nä\n```\n## 在文件等外部媒介中的存储方式\nunicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。\n\n# 字符表示\n## python 3.x\n在 python 3.x 中，`str` 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：\n```python\n# python 3.6\n>>> a = 'Natasha, 娜塔莎'\n>>> type(a)\nstr\n>>> len(a)\n12\n```\n可以看到，python 3.x 中得到的 `a` 的长度为 12（包含空格），没有任何问题；我们可以对 `a` 进行编码，将其转换为 `bytes` 类型：\n```python\n# python 3.6\n>>> b = a.encode('utf-8')\n>>> b\nb'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> type(b)\nbytes\n```\n从上面可以看出，`bytes` 类型的对象中的某个字节的取值在 `0x00` \\~ `0x7F` 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。\n\n同理，我们也可以将一个 `bytes` 类型的对象译码为一个 `str` 类型的对象：\n```python\n# python 3.6\n>>> a = b.decode('utf-8')\n>>> a\n'Natasha, 娜塔莎'\n```\n\n## python 2.x\n在 python 2.x 中，如果还是用 `str` 类型来表示国际字符，就会有问题：\n```python \n# python 2.7\n>>> a = 'Natasha, 娜塔莎'\n>>> type(a)\nstr\n>>> a\n'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> len(a)\n18\n>>> print a\nNatasha, 娜塔莎\n```\n可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 `str` 字符串，但实际上 `a` 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 `a` 正好是字符串 `'Natasha, 娜塔莎'` 的 utf-8 编码的结果，且将 `a` 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:\n```python\n# python 2.7\n>>> import sys\n>>> sys.stdin.encoding  # 控制台的输入编码，可解释前例中 a 在内存中的表现形式\n'utf-8'\n>>> sys.stdout.encoding # 控制台的输出编码，可解释前例中打印 a 的显示结果\n'utf-8'\n```\n另外，`sys.getdefaultencoding()`函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 `'ascii'`, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 `json.dumps()` 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：\\>）。\n\n切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 `unicode` 类型字符串，如下所示：\n```python\n# python 2.7\n>>> a = u'Natasha, 娜塔莎'\n>>> type(a)\nunicode\n>>> len(a)\n12\n>>> b = a.encode('utf-8')\n>>> b\n'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'\n>>> type(b)\nstr\n>>> a = b.decode('utf-8')\n>>> a\nu'Natasha, \\u5a1c\\u5854\\u838e'\n```\n另外，我们可以对 `unicode` 类型字符串进行编码操作，对 `str` 类型字符串进行译码操作。\n\n---- \n\n# 文本文件操作\n## python 3.x\n在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 `open` 函数的参数 `encoding` 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 `sys.getfilesystemencoding()`得到，如：\n```python\n# python 3.6\n>>> import sys\n>>> sys.getfilesystemencoding()\n'utf-8'\n>>> a = '娜塔莎'\n>>> f = open('data.txt', 'w', encoding='utf-8')\n>>> f.write(a)\n3\n>>> f.close()\n>>> f = open('data.txt', 'w', encoding='utf-8')\n>>> f.read()\n'娜塔莎'\n>>> f.close()\n```\n当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：\n```python\n# python 3.6\n>>> a = '娜塔莎'\n>>> b = a.encode('utf-8')\n>>> f = open('data.txt', 'wb')\n>>> f.write(b)\n9\n>>> f.close()\n>>> f.read().decode('utf-8')\n'娜塔莎'\n>>> f.close()\n```\n## python 2.x\n在 python 2.x 中，`open` 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 `str`；`codecs.open` 函数则支持自动读写 unicode 文本文件，如：\n```python\n# python 2.7\n>>> import codecs\n>>> a = u'安德烈'\n>>> f = codecs.open('data.txt', 'w', encoding='utf-8')\n>>> f.write(a)\n>>> f.close()\n>>> f = codecs.open('data.txt', 'r', encoding='utf-8') \n>>> print f.read()\n安德烈\n>>> f.close()\n```\n类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：\n```python\n# python 2.7\n>>> b = a.encode('utf-8')\n>>> f = open('data.txt', 'w')\n>>> f.write(b)\n>>> f.close()\n>>> f = open('data.txt', 'r')\n>>> print f.read().decode('utf-8')\n安德烈\n>>> f.close()\n```\n总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。\n\n---- \n\n# python 2.x 中的 json.dumps() 操作\njson 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。\n\n对于数据结构中的字符串类型为 `str`、 但实际上定义的是一个国际字符串的情况，`json.dumps()` 的结果如下：\n```python\n# python 2.7\n>>> a = {'Natasha': '娜塔莎'}\n>>> a_json_1 = json.dumps(a)\n>>> a_json_1\n'{\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"}'\n>>> a_json_2 = json.dumps(a, ensure_ascii=False)\n>>> a_json_2\n'{\"Natasha\": \"\\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e\"}'\n```\n可以看到，在这种情形下，当 `ensure_ascii` 为 `True` 时，`json.dumps()` 操作返回值的类型为 `str`，其会将 `a` 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 `'\\\\u5a1c'` 对应 `6` 个字符而非 `1` 个）；当 `ensure_ascii` 为 `False` 时，`json.dumps()` 操作的返回值类型仍然为 `str`，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 `a_json_2` 译码就可以得到我们想要的 json：\n```python\n# python 2.7\n>>> a_json_2.decode('utf-8')\nu'{\"Natasha\": \"\\u5a1c\\u5854\\u838e\"}'\n>>> print a_json_2.decode('utf-8')\n{\"Natasha\": \"娜塔莎\"}\n```\n对于数据结构中的字符串类型为 `unicode` 的情况，`json.dumps()` 的结果如下：\n```python\n# python 2.7\n>>> u = {u'Natasha': u'娜塔莎'}\n>>> u_json_1 = json.dumps(u)\n>>> u_json_1\n'{\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"}'\n>>> u_json_2 = json.dumps(u, ensure_ascii=False)\n>>> u_json_2\nu'{\"Natasha\": \"\\u5a1c\\u5854\\u838e\"}'\n>>> print u_json_2\n{\"Natasha\": \"娜塔莎\"}\n```\n在这种情形下，当 `ensure_ascii` 为 `True` 时，`json.dumps()` 操作返回值的类型为 `str`，其得到的结果和前面对 a 操作返回的结果完全一样；而当`ensure_ascii` 为 `False` 时，`json.dumps()` 操作的返回值类型变为 `unicode`，原始数据结构中的中文字符在返回值中完整地保留了下来。\n\n对于数据结构中的字符串类型既有 `unicode` 又有 `str` 的情形，运用 `json.dumps()` 时将 `ensure_ascii` 设为 `False` 的情况又会完全不同。\n\n当数据结构中的 ASCII 字符串为 `str` 类型，国际字符串为 `unicode` 类型时（如 `u = {'Natasha': u'娜塔莎'}`），`json.dumps()` 的返回值是正常的、符合预期的 `unicode` 字符串；当数据结构中有国际字符串为 `str` 类型，又存在其他字符串为 `unicode` 类型时（如 `u = {u'Natasha': '娜塔莎'}` 或 `u = {u'娜塔莉娅': '娜塔莎'}`），`json.dumps()` 会抛出异常 `UnicodeDecodeError`，这是因为系统会将数据结构中 `str` 类型字符串都转换为 `unicode` 类型，而系统的默认编译码方式为 ascii 编码，因而对 `str` 类型的国际字符串进行 ascii 译码就必然会出错。\n\n[1]:\thttps://www.wikiwand.com/en/List_of_Unicode_characters","slug":"Python-中的-Unicode-string-和-Byte-string","published":1,"updated":"2018-03-22T04:40:21.536Z","comments":1,"photos":[],"link":"","_id":"cjfclp3tm0001hws6wmpwdcn2","content":"<h1 id=\"python-2-x-和-python-3-x-字符串类型的区别\"><a href=\"#python-2-x-和-python-3-x-字符串类型的区别\" class=\"headerlink\" title=\"python 2.x 和 python 3.x 字符串类型的区别\"></a>python 2.x 和 python 3.x 字符串类型的区别</h1><p>python 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。</p>\n<h2 id=\"python-2-x\"><a href=\"#python-2-x\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><ul>\n<li><code>str</code> 类型：处理 binary 数据和 ASCII 文本数据。</li>\n<li><code>unicode</code> 类型：处理<strong>非 ASCII</strong> 文本数据。</li>\n</ul>\n<h2 id=\"python-3-x\"><a href=\"#python-3-x\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><ul>\n<li><code>bytes</code> 类型：处理 binary 数据，同 <code>str</code> 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。</li>\n<li><code>str</code> 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。</li>\n<li><code>bytearray</code> 类型：<code>bytes</code> 类型的变种，但是此类型是 <strong>mutable</strong> 的。</li>\n</ul>\n<hr>\n<h1 id=\"Unicode-简介\"><a href=\"#Unicode-简介\" class=\"headerlink\" title=\"Unicode 简介\"></a>Unicode 简介</h1><p>包括 <strong>ASCII 码</strong>、<strong>latin-1 编码</strong> 和 <strong>utf-8 编码</strong> 等在内的码都被认为是 unicode 码。</p>\n<h2 id=\"编码和解码的概念\"><a href=\"#编码和解码的概念\" class=\"headerlink\" title=\"编码和解码的概念\"></a>编码和解码的概念</h2><ul>\n<li>编码（encoding）：将字符串映射为一串原始的字节。</li>\n<li>解码（decoding）：将一串原始的字节翻译成字符串。</li>\n</ul>\n<h2 id=\"ASCII码\"><a href=\"#ASCII码\" class=\"headerlink\" title=\"ASCII码\"></a>ASCII码</h2><ul>\n<li>编码长度为 1 个 byte.</li>\n<li>编码范围为 <code>0x00</code>~<code>0x7F</code>，只包含一些常见的字符。</li>\n</ul>\n<h2 id=\"latin-1码\"><a href=\"#latin-1码\" class=\"headerlink\" title=\"latin-1码\"></a>latin-1码</h2><ul>\n<li>编码长度为 1 个 byte.</li>\n<li>编码范围为 <code>0x00</code>~<code>0xFF</code>，能支持更多的字符（如 accent character），兼容 ASCII 码。</li>\n</ul>\n<h2 id=\"utf-8码\"><a href=\"#utf-8码\" class=\"headerlink\" title=\"utf-8码\"></a>utf-8码</h2><ul>\n<li>编码长度可变，为 1~4 个 byte。</li>\n<li>当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 <code>0x00</code> ~ <code>0x7F</code>；当编码长度大于 1 个 byte 时，每个 byte 的取值为 <code>0x80</code> ~ <code>0xFF</code>。</li>\n</ul>\n<h2 id=\"其它编码\"><a href=\"#其它编码\" class=\"headerlink\" title=\"其它编码\"></a>其它编码</h2><ul>\n<li>utf-16，编码长度为定长 2 个 byte。</li>\n<li>utf-32，编码长度为定长 4 个 byte。</li>\n</ul>\n<hr>\n<h1 id=\"Unicode-字符串的存储方式\"><a href=\"#Unicode-字符串的存储方式\" class=\"headerlink\" title=\"Unicode 字符串的存储方式\"></a>Unicode 字符串的存储方式</h1><h2 id=\"在内存中的存储方式\"><a href=\"#在内存中的存储方式\" class=\"headerlink\" title=\"在内存中的存储方式\"></a>在内存中的存储方式</h2><p>unicode 字符串中的字符在内存中以一种<strong>与编码方式无关</strong>的方式存储：<a href=\"https://www.wikiwand.com/en/List_of_Unicode_characters\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">unicode code point</a>，它是一个数字，范围为 0~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，<code>a</code> 和 <code>b</code> 表示的是同一字符串（其中 <code>&#39;\\uNNNN&#39;</code> 即为 unicode code point，<code>N</code> 为一个十六进制位，十六进制位的个数为 4~6 位；当 unicode code point 的取值在 0~255 范围内时，也可以 <code>&#39;\\xNN&#39;</code> 的形式表示）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'\\u5a1c\\u5854\\u838e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = <span class=\"string\">u'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a, b</div><div class=\"line\">娜塔莎 娜塔莎</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = <span class=\"string\">u'\\xe4'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> c</div><div class=\"line\">ä</div></pre></td></tr></table></figure></p>\n<h2 id=\"在文件等外部媒介中的存储方式\"><a href=\"#在文件等外部媒介中的存储方式\" class=\"headerlink\" title=\"在文件等外部媒介中的存储方式\"></a>在文件等外部媒介中的存储方式</h2><p>unicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。</p>\n<h1 id=\"字符表示\"><a href=\"#字符表示\" class=\"headerlink\" title=\"字符表示\"></a>字符表示</h1><h2 id=\"python-3-x-1\"><a href=\"#python-3-x-1\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><p>在 python 3.x 中，<code>str</code> 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<p>可以看到，python 3.x 中得到的 <code>a</code> 的长度为 12（包含空格），没有任何问题；我们可以对 <code>a</code> 进行编码，将其转换为 <code>bytes</code> 类型：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</div><div class=\"line\"><span class=\"string\">b'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(b)</div><div class=\"line\">bytes</div></pre></td></tr></table></figure></p>\n<p>从上面可以看出，<code>bytes</code> 类型的对象中的某个字节的取值在 <code>0x00</code> ~ <code>0x7F</code> 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。</p>\n<p>同理，我们也可以将一个 <code>bytes</code> 类型的对象译码为一个 <code>str</code> 类型的对象：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = b.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">'Natasha, 娜塔莎'</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"python-2-x-1\"><a href=\"#python-2-x-1\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><p>在 python 2.x 中，如果还是用 <code>str</code> 类型来表示国际字符，就会有问题：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">18</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a</div><div class=\"line\">Natasha, 娜塔莎</div></pre></td></tr></table></figure></p>\n<p>可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 <code>str</code> 字符串，但实际上 <code>a</code> 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 <code>a</code> 正好是字符串 <code>&#39;Natasha, 娜塔莎&#39;</code> 的 utf-8 编码的结果，且将 <code>a</code> 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.stdin.encoding  <span class=\"comment\"># 控制台的输入编码，可解释前例中 a 在内存中的表现形式</span></div><div class=\"line\"><span class=\"string\">'utf-8'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.stdout.encoding <span class=\"comment\"># 控制台的输出编码，可解释前例中打印 a 的显示结果</span></div><div class=\"line\"><span class=\"string\">'utf-8'</span></div></pre></td></tr></table></figure></p>\n<p>另外，<code>sys.getdefaultencoding()</code>函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 <code>&#39;ascii&#39;</code>, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 <code>json.dumps()</code> 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：>）。</p>\n<p>切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 <code>unicode</code> 类型字符串，如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">unicode</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">12</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</div><div class=\"line\"><span class=\"string\">'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(b)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = b.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">u'Natasha, \\u5a1c\\u5854\\u838e'</span></div></pre></td></tr></table></figure></p>\n<p>另外，我们可以对 <code>unicode</code> 类型字符串进行编码操作，对 <code>str</code> 类型字符串进行译码操作。</p>\n<hr>\n<h1 id=\"文本文件操作\"><a href=\"#文本文件操作\" class=\"headerlink\" title=\"文本文件操作\"></a>文本文件操作</h1><h2 id=\"python-3-x-2\"><a href=\"#python-3-x-2\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><p>在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 <code>open</code> 函数的参数 <code>encoding</code> 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 <code>sys.getfilesystemencoding()</code>得到，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.getfilesystemencoding()</div><div class=\"line\"><span class=\"string\">'utf-8'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(a)</div><div class=\"line\"><span class=\"number\">3</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.read()</div><div class=\"line\"><span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'wb'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(b)</div><div class=\"line\"><span class=\"number\">9</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.read().decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<h2 id=\"python-2-x-2\"><a href=\"#python-2-x-2\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><p>在 python 2.x 中，<code>open</code> 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 <code>str</code>；<code>codecs.open</code> 函数则支持自动读写 unicode 文本文件，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> codecs</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'安德烈'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = codecs.open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(a)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = codecs.open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'r'</span>, encoding=<span class=\"string\">'utf-8'</span>) </div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> f.read()</div><div class=\"line\">安德烈</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(b)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'r'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> f.read().decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\">安德烈</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。</p>\n<hr>\n<h1 id=\"python-2-x-中的-json-dumps-操作\"><a href=\"#python-2-x-中的-json-dumps-操作\" class=\"headerlink\" title=\"python 2.x 中的 json.dumps() 操作\"></a>python 2.x 中的 json.dumps() 操作</h1><p>json 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。</p>\n<p>对于数据结构中的字符串类型为 <code>str</code>、 但实际上定义的是一个国际字符串的情况，<code>json.dumps()</code> 的结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = &#123;<span class=\"string\">'Natasha'</span>: <span class=\"string\">'娜塔莎'</span>&#125;</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_1 = json.dumps(a)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_1</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2 = json.dumps(a, ensure_ascii=<span class=\"keyword\">False</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e\"&#125;'</span></div></pre></td></tr></table></figure></p>\n<p>可以看到，在这种情形下，当 <code>ensure_ascii</code> 为 <code>True</code> 时，<code>json.dumps()</code> 操作返回值的类型为 <code>str</code>，其会将 <code>a</code> 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 <code>&#39;\\\\u5a1c&#39;</code> 对应 <code>6</code> 个字符而非 <code>1</code> 个）；当 <code>ensure_ascii</code> 为 <code>False</code> 时，<code>json.dumps()</code> 操作的返回值类型仍然为 <code>str</code>，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 <code>a_json_2</code> 译码就可以得到我们想要的 json：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"string\">u'&#123;\"Natasha\": \"\\u5a1c\\u5854\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a_json_2.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\">&#123;<span class=\"string\">\"Natasha\"</span>: <span class=\"string\">\"娜塔莎\"</span>&#125;</div></pre></td></tr></table></figure></p>\n<p>对于数据结构中的字符串类型为 <code>unicode</code> 的情况，<code>json.dumps()</code> 的结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u = &#123;<span class=\"string\">u'Natasha'</span>: <span class=\"string\">u'娜塔莎'</span>&#125;</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_1 = json.dumps(u)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_1</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_2 = json.dumps(u, ensure_ascii=<span class=\"keyword\">False</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_2</div><div class=\"line\"><span class=\"string\">u'&#123;\"Natasha\": \"\\u5a1c\\u5854\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> u_json_2</div><div class=\"line\">&#123;<span class=\"string\">\"Natasha\"</span>: <span class=\"string\">\"娜塔莎\"</span>&#125;</div></pre></td></tr></table></figure></p>\n<p>在这种情形下，当 <code>ensure_ascii</code> 为 <code>True</code> 时，<code>json.dumps()</code> 操作返回值的类型为 <code>str</code>，其得到的结果和前面对 a 操作返回的结果完全一样；而当<code>ensure_ascii</code> 为 <code>False</code> 时，<code>json.dumps()</code> 操作的返回值类型变为 <code>unicode</code>，原始数据结构中的中文字符在返回值中完整地保留了下来。</p>\n<p>对于数据结构中的字符串类型既有 <code>unicode</code> 又有 <code>str</code> 的情形，运用 <code>json.dumps()</code> 时将 <code>ensure_ascii</code> 设为 <code>False</code> 的情况又会完全不同。</p>\n<p>当数据结构中的 ASCII 字符串为 <code>str</code> 类型，国际字符串为 <code>unicode</code> 类型时（如 <code>u = {&#39;Natasha&#39;: u&#39;娜塔莎&#39;}</code>），<code>json.dumps()</code> 的返回值是正常的、符合预期的 <code>unicode</code> 字符串；当数据结构中有国际字符串为 <code>str</code> 类型，又存在其他字符串为 <code>unicode</code> 类型时（如 <code>u = {u&#39;Natasha&#39;: &#39;娜塔莎&#39;}</code> 或 <code>u = {u&#39;娜塔莉娅&#39;: &#39;娜塔莎&#39;}</code>），<code>json.dumps()</code> 会抛出异常 <code>UnicodeDecodeError</code>，这是因为系统会将数据结构中 <code>str</code> 类型字符串都转换为 <code>unicode</code> 类型，而系统的默认编译码方式为 ascii 编码，因而对 <code>str</code> 类型的国际字符串进行 ascii 译码就必然会出错。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"python-2-x-和-python-3-x-字符串类型的区别\"><a href=\"#python-2-x-和-python-3-x-字符串类型的区别\" class=\"headerlink\" title=\"python 2.x 和 python 3.x 字符串类型的区别\"></a>python 2.x 和 python 3.x 字符串类型的区别</h1><p>python 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。</p>\n<h2 id=\"python-2-x\"><a href=\"#python-2-x\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><ul>\n<li><code>str</code> 类型：处理 binary 数据和 ASCII 文本数据。</li>\n<li><code>unicode</code> 类型：处理<strong>非 ASCII</strong> 文本数据。</li>\n</ul>\n<h2 id=\"python-3-x\"><a href=\"#python-3-x\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><ul>\n<li><code>bytes</code> 类型：处理 binary 数据，同 <code>str</code> 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。</li>\n<li><code>str</code> 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。</li>\n<li><code>bytearray</code> 类型：<code>bytes</code> 类型的变种，但是此类型是 <strong>mutable</strong> 的。</li>\n</ul>\n<hr>\n<h1 id=\"Unicode-简介\"><a href=\"#Unicode-简介\" class=\"headerlink\" title=\"Unicode 简介\"></a>Unicode 简介</h1><p>包括 <strong>ASCII 码</strong>、<strong>latin-1 编码</strong> 和 <strong>utf-8 编码</strong> 等在内的码都被认为是 unicode 码。</p>\n<h2 id=\"编码和解码的概念\"><a href=\"#编码和解码的概念\" class=\"headerlink\" title=\"编码和解码的概念\"></a>编码和解码的概念</h2><ul>\n<li>编码（encoding）：将字符串映射为一串原始的字节。</li>\n<li>解码（decoding）：将一串原始的字节翻译成字符串。</li>\n</ul>\n<h2 id=\"ASCII码\"><a href=\"#ASCII码\" class=\"headerlink\" title=\"ASCII码\"></a>ASCII码</h2><ul>\n<li>编码长度为 1 个 byte.</li>\n<li>编码范围为 <code>0x00</code>~<code>0x7F</code>，只包含一些常见的字符。</li>\n</ul>\n<h2 id=\"latin-1码\"><a href=\"#latin-1码\" class=\"headerlink\" title=\"latin-1码\"></a>latin-1码</h2><ul>\n<li>编码长度为 1 个 byte.</li>\n<li>编码范围为 <code>0x00</code>~<code>0xFF</code>，能支持更多的字符（如 accent character），兼容 ASCII 码。</li>\n</ul>\n<h2 id=\"utf-8码\"><a href=\"#utf-8码\" class=\"headerlink\" title=\"utf-8码\"></a>utf-8码</h2><ul>\n<li>编码长度可变，为 1~4 个 byte。</li>\n<li>当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 <code>0x00</code> ~ <code>0x7F</code>；当编码长度大于 1 个 byte 时，每个 byte 的取值为 <code>0x80</code> ~ <code>0xFF</code>。</li>\n</ul>\n<h2 id=\"其它编码\"><a href=\"#其它编码\" class=\"headerlink\" title=\"其它编码\"></a>其它编码</h2><ul>\n<li>utf-16，编码长度为定长 2 个 byte。</li>\n<li>utf-32，编码长度为定长 4 个 byte。</li>\n</ul>\n<hr>\n<h1 id=\"Unicode-字符串的存储方式\"><a href=\"#Unicode-字符串的存储方式\" class=\"headerlink\" title=\"Unicode 字符串的存储方式\"></a>Unicode 字符串的存储方式</h1><h2 id=\"在内存中的存储方式\"><a href=\"#在内存中的存储方式\" class=\"headerlink\" title=\"在内存中的存储方式\"></a>在内存中的存储方式</h2><p>unicode 字符串中的字符在内存中以一种<strong>与编码方式无关</strong>的方式存储：<a href=\"https://www.wikiwand.com/en/List_of_Unicode_characters\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">unicode code point</a>，它是一个数字，范围为 0~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，<code>a</code> 和 <code>b</code> 表示的是同一字符串（其中 <code>&#39;\\uNNNN&#39;</code> 即为 unicode code point，<code>N</code> 为一个十六进制位，十六进制位的个数为 4~6 位；当 unicode code point 的取值在 0~255 范围内时，也可以 <code>&#39;\\xNN&#39;</code> 的形式表示）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'\\u5a1c\\u5854\\u838e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = <span class=\"string\">u'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a, b</div><div class=\"line\">娜塔莎 娜塔莎</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = <span class=\"string\">u'\\xe4'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> c</div><div class=\"line\">ä</div></pre></td></tr></table></figure></p>\n<h2 id=\"在文件等外部媒介中的存储方式\"><a href=\"#在文件等外部媒介中的存储方式\" class=\"headerlink\" title=\"在文件等外部媒介中的存储方式\"></a>在文件等外部媒介中的存储方式</h2><p>unicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。</p>\n<h1 id=\"字符表示\"><a href=\"#字符表示\" class=\"headerlink\" title=\"字符表示\"></a>字符表示</h1><h2 id=\"python-3-x-1\"><a href=\"#python-3-x-1\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><p>在 python 3.x 中，<code>str</code> 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<p>可以看到，python 3.x 中得到的 <code>a</code> 的长度为 12（包含空格），没有任何问题；我们可以对 <code>a</code> 进行编码，将其转换为 <code>bytes</code> 类型：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</div><div class=\"line\"><span class=\"string\">b'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(b)</div><div class=\"line\">bytes</div></pre></td></tr></table></figure></p>\n<p>从上面可以看出，<code>bytes</code> 类型的对象中的某个字节的取值在 <code>0x00</code> ~ <code>0x7F</code> 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。</p>\n<p>同理，我们也可以将一个 <code>bytes</code> 类型的对象译码为一个 <code>str</code> 类型的对象：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = b.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">'Natasha, 娜塔莎'</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"python-2-x-1\"><a href=\"#python-2-x-1\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><p>在 python 2.x 中，如果还是用 <code>str</code> 类型来表示国际字符，就会有问题：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">18</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a</div><div class=\"line\">Natasha, 娜塔莎</div></pre></td></tr></table></figure></p>\n<p>可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 <code>str</code> 字符串，但实际上 <code>a</code> 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 <code>a</code> 正好是字符串 <code>&#39;Natasha, 娜塔莎&#39;</code> 的 utf-8 编码的结果，且将 <code>a</code> 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.stdin.encoding  <span class=\"comment\"># 控制台的输入编码，可解释前例中 a 在内存中的表现形式</span></div><div class=\"line\"><span class=\"string\">'utf-8'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.stdout.encoding <span class=\"comment\"># 控制台的输出编码，可解释前例中打印 a 的显示结果</span></div><div class=\"line\"><span class=\"string\">'utf-8'</span></div></pre></td></tr></table></figure></p>\n<p>另外，<code>sys.getdefaultencoding()</code>函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 <code>&#39;ascii&#39;</code>, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 <code>json.dumps()</code> 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：>）。</p>\n<p>切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 <code>unicode</code> 类型字符串，如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'Natasha, 娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(a)</div><div class=\"line\">unicode</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(a)</div><div class=\"line\"><span class=\"number\">12</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</div><div class=\"line\"><span class=\"string\">'Natasha, \\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(b)</div><div class=\"line\">str</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = b.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</div><div class=\"line\"><span class=\"string\">u'Natasha, \\u5a1c\\u5854\\u838e'</span></div></pre></td></tr></table></figure></p>\n<p>另外，我们可以对 <code>unicode</code> 类型字符串进行编码操作，对 <code>str</code> 类型字符串进行译码操作。</p>\n<hr>\n<h1 id=\"文本文件操作\"><a href=\"#文本文件操作\" class=\"headerlink\" title=\"文本文件操作\"></a>文本文件操作</h1><h2 id=\"python-3-x-2\"><a href=\"#python-3-x-2\" class=\"headerlink\" title=\"python 3.x\"></a>python 3.x</h2><p>在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 <code>open</code> 函数的参数 <code>encoding</code> 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 <code>sys.getfilesystemencoding()</code>得到，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> sys</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>sys.getfilesystemencoding()</div><div class=\"line\"><span class=\"string\">'utf-8'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(a)</div><div class=\"line\"><span class=\"number\">3</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.read()</div><div class=\"line\"><span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'wb'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(b)</div><div class=\"line\"><span class=\"number\">9</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.read().decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"string\">'娜塔莎'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<h2 id=\"python-2-x-2\"><a href=\"#python-2-x-2\" class=\"headerlink\" title=\"python 2.x\"></a>python 2.x</h2><p>在 python 2.x 中，<code>open</code> 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 <code>str</code>；<code>codecs.open</code> 函数则支持自动读写 unicode 文本文件，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> codecs</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = <span class=\"string\">u'安德烈'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = codecs.open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>, encoding=<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(a)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = codecs.open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'r'</span>, encoding=<span class=\"string\">'utf-8'</span>) </div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> f.read()</div><div class=\"line\">安德烈</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.encode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'w'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.write(b)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = open(<span class=\"string\">'data.txt'</span>, <span class=\"string\">'r'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> f.read().decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\">安德烈</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.close()</div></pre></td></tr></table></figure></p>\n<p>总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。</p>\n<hr>\n<h1 id=\"python-2-x-中的-json-dumps-操作\"><a href=\"#python-2-x-中的-json-dumps-操作\" class=\"headerlink\" title=\"python 2.x 中的 json.dumps() 操作\"></a>python 2.x 中的 json.dumps() 操作</h1><p>json 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。</p>\n<p>对于数据结构中的字符串类型为 <code>str</code>、 但实际上定义的是一个国际字符串的情况，<code>json.dumps()</code> 的结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = &#123;<span class=\"string\">'Natasha'</span>: <span class=\"string\">'娜塔莎'</span>&#125;</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_1 = json.dumps(a)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_1</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2 = json.dumps(a, ensure_ascii=<span class=\"keyword\">False</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\xe5\\xa8\\x9c\\xe5\\xa1\\x94\\xe8\\x8e\\x8e\"&#125;'</span></div></pre></td></tr></table></figure></p>\n<p>可以看到，在这种情形下，当 <code>ensure_ascii</code> 为 <code>True</code> 时，<code>json.dumps()</code> 操作返回值的类型为 <code>str</code>，其会将 <code>a</code> 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 <code>&#39;\\\\u5a1c&#39;</code> 对应 <code>6</code> 个字符而非 <code>1</code> 个）；当 <code>ensure_ascii</code> 为 <code>False</code> 时，<code>json.dumps()</code> 操作的返回值类型仍然为 <code>str</code>，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 <code>a_json_2</code> 译码就可以得到我们想要的 json：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a_json_2.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\"><span class=\"string\">u'&#123;\"Natasha\": \"\\u5a1c\\u5854\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> a_json_2.decode(<span class=\"string\">'utf-8'</span>)</div><div class=\"line\">&#123;<span class=\"string\">\"Natasha\"</span>: <span class=\"string\">\"娜塔莎\"</span>&#125;</div></pre></td></tr></table></figure></p>\n<p>对于数据结构中的字符串类型为 <code>unicode</code> 的情况，<code>json.dumps()</code> 的结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 2.7</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u = &#123;<span class=\"string\">u'Natasha'</span>: <span class=\"string\">u'娜塔莎'</span>&#125;</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_1 = json.dumps(u)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_1</div><div class=\"line\"><span class=\"string\">'&#123;\"Natasha\": \"\\\\u5a1c\\\\u5854\\\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_2 = json.dumps(u, ensure_ascii=<span class=\"keyword\">False</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>u_json_2</div><div class=\"line\"><span class=\"string\">u'&#123;\"Natasha\": \"\\u5a1c\\u5854\\u838e\"&#125;'</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">print</span> u_json_2</div><div class=\"line\">&#123;<span class=\"string\">\"Natasha\"</span>: <span class=\"string\">\"娜塔莎\"</span>&#125;</div></pre></td></tr></table></figure></p>\n<p>在这种情形下，当 <code>ensure_ascii</code> 为 <code>True</code> 时，<code>json.dumps()</code> 操作返回值的类型为 <code>str</code>，其得到的结果和前面对 a 操作返回的结果完全一样；而当<code>ensure_ascii</code> 为 <code>False</code> 时，<code>json.dumps()</code> 操作的返回值类型变为 <code>unicode</code>，原始数据结构中的中文字符在返回值中完整地保留了下来。</p>\n<p>对于数据结构中的字符串类型既有 <code>unicode</code> 又有 <code>str</code> 的情形，运用 <code>json.dumps()</code> 时将 <code>ensure_ascii</code> 设为 <code>False</code> 的情况又会完全不同。</p>\n<p>当数据结构中的 ASCII 字符串为 <code>str</code> 类型，国际字符串为 <code>unicode</code> 类型时（如 <code>u = {&#39;Natasha&#39;: u&#39;娜塔莎&#39;}</code>），<code>json.dumps()</code> 的返回值是正常的、符合预期的 <code>unicode</code> 字符串；当数据结构中有国际字符串为 <code>str</code> 类型，又存在其他字符串为 <code>unicode</code> 类型时（如 <code>u = {u&#39;Natasha&#39;: &#39;娜塔莎&#39;}</code> 或 <code>u = {u&#39;娜塔莉娅&#39;: &#39;娜塔莎&#39;}</code>），<code>json.dumps()</code> 会抛出异常 <code>UnicodeDecodeError</code>，这是因为系统会将数据结构中 <code>str</code> 类型字符串都转换为 <code>unicode</code> 类型，而系统的默认编译码方式为 ascii 编码，因而对 <code>str</code> 类型的国际字符串进行 ascii 译码就必然会出错。</p>\n"},{"title":"线性回归模型","date":"2018-03-29T14:10:14.000Z","keywords":"线性模型,回归,监督学习,regression,判别模型,最大似然估计,最大后验概率估计","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","source":"_posts/线性回归模型.md","raw":"---\ntitle: 线性回归模型\ndate: 2018-03-29 22:10:14\ntags:\n- 线性模型\n- 线性回归\n- 监督学习\ncategories:\n- 机器学习算法\nkeywords: 线性模型,回归,监督学习,regression,判别模型,最大似然估计,最大后验概率估计\n\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","slug":"线性回归模型","published":1,"updated":"2018-03-29T14:14:32.096Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfclp3tr0002hws6qq92bhcw","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n"},{"title":"聚类分析（二）：k-means 算法","layout":"post","date":"2018-02-10T12:33:10.000Z","keywords":"聚类,k-means,中心点,非监督学习,clustering,machine learning","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# k-means 聚类算法\nk-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 [*聚类分析（一）：层次聚类算法*][2] 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 `cluster` 都维持一个中心点（为属于该 `cluster` 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 `cluster`。在该算法中，`cluster` 的数目 \\\\( K \\\\) 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 \\\\( K \\\\) 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 `cluster` 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。\n\n## 表示为求解特定的优化问题\n假定数据集为 \\\\( \\\\lbrace {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N \\\\rbrace \\\\)，其包含 \\\\( N \\\\) 个样本点，每个样本点的维度为 \\\\( D \\\\)。我们的目的是将该数据集划分为 \\\\( K \\\\) 个 `cluster`，其中 \\\\( K \\\\) 是一个预先给定的值。假设每个 `cluster` 的中心点为向量 \\\\( {\\\\bf \\\\mu}\\_k  \\\\in \\\\Bbb{R}^{d} \\\\)，其中 \\\\( k = 1, …, K \\\\)。如前面所述，我们的目的是找到中心点 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\)，以及每个样本点所属的类别，以使每个样本点距其被分配的 `cluster` 所对应的中心点的平方 Euclidean 距离之和最小。\n\n为方便用数学符号描述该优化问题，我们以变量 \\\\( r\\_{nk} \\\\in \\\\lbrace 0, 1 \\\\rbrace \\\\) 来表示样本点 \\\\( {\\\\bf x}\\_n \\\\) 是否被分配至第 \\\\( k \\\\) 个 `cluster`，若样本点 \\\\( {\\\\bf x}\\_n \\\\) 被分配至第 \\\\( k \\\\) 个 `cluster`，则 \\\\( r\\_{nk} = 1 \\\\) 且 \\\\( r\\_{nj} = 0  \\\\) \\\\( (j \\\\neq k) \\\\)。由此我们可以写出目标函数\n$$  J = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} r\\_{nk} \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_k \\\\|^{2} $$\n它表示的即是每个样本点与其被分配的 `cluster` 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 和 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 的值，以使得目标函数 \\\\( J \\\\) 最小。\n\n我们可以看到，由于 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。\n\nk-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 选定初始值；然后在第一个迭代步骤中固定 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 的值，对目标函数 \\\\( J \\\\) 根据  \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 求最小值；再在第二个迭代步骤中固定 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 的值，对  \\\\( J \\\\) 根据 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求最小值；如此交替迭代，直至目标函数收敛。\n\n考虑迭代过程中的两个优化问题。首先考虑固定 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求解 \\\\( r\\_{nk} \\\\) 的问题，可以看到 \\\\( J \\\\) 是关于 \\\\( r\\_{nk} \\\\) 的线性函数，因此我们很容易给出一个闭式解：\\\\( J \\\\) 包含 \\\\( N \\\\) 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，\\\\( r\\_{nk} \\\\) 的解为\n$$ r\\_{nk} = \\\\begin{cases} 1, & \\\\text {if \\\\( k = \\\\rm {arg}  \\\\rm {min}\\_{j}  \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_j \\\\|^{2} \\\\) } \\\\\\\\ 0, & \\\\text {otherwise} \\\\end{cases} $$\n从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 `cluster`。\n\n再来考虑固定  \\\\( r\\_{nk} \\\\) 求解 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的问题，目标函数 \\\\( J \\\\) 是关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的二次函数，因此可以通过将 \\\\( J \\\\) 关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的导数置为 0 来求解 \\\\( J \\\\) 关于  \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的最小值：\n$$ 2 \\\\sum\\_{n = 1}^{N} r\\_{nk}({\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_k) = 0 $$\n容易求出 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的值为\n$$ {\\\\bf \\\\mu}\\_k = \\\\frac {\\\\sum\\_{n} r\\_{nk} {\\\\bf x}\\_n} {\\\\sum\\_{n} r\\_{nk} } $$\n该式表明，这一步迭代是将中心点更新为所有被分配至该 `cluster` 的样本点的均值。\n\nk-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 \\\\( J \\\\) 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。\n\n虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程：\n\n1.  初始化每个 `cluster` 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果；\n2. 将每个样本点分配给距离其最近的中心点所对应的 `cluster`；\n3. 更新每个 `cluster` 的中心点为被分配给该 `cluster` 的所有样本点的均值；\n4. 交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。\n\n[ PRML 教材 ][3]中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 \\\\( K = 2 \\\\)，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_k-means_algorithm.png\" width = \"660\" height = \"550\" alt = \"k-means 算法运行图示\" align = center />\n</div>\n\n## 算法复杂度及其优缺点\n### 算法复杂度\nk-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 \\\\( O(NK) \\\\) 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 \\\\(  O(D) \\\\)，其中 \\\\( N \\\\) 为样本点的个数，\\\\( K \\\\) 为指定的聚类个数，\\\\( D \\\\) 为样本点的维度；因此，一次迭代过程的时间复杂度为 \\\\( O(NKD) \\\\)，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 \\\\( O(NKD) \\\\)。\n  \n实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。\n\n### 缺点\nk-means 算法虽简单易用，但其有一些很明显的缺点，总结如下：\n- 由于其假设每个 `cluster` 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 `cluster`；但事实上的数据的 `cluster` 有可能并不是如此。\n- 由于其假设每一个 `cluster` 的分布形状都为球形（spherical），（“球形分布”表明一个 `cluster` 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 `cluster` ，对于具有非凸的或者形状很特别的 `cluster` 的数据集，其聚类效果往往很差。\n- 由于其假设不同的 `cluster` 具有相似的密度，因此对于具有密度差别较大的 `cluster` 的数据集，其聚类效果不好。\n- 其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。\n- `cluster` 的数目 \\\\( K \\\\) 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 \\\\( K \\\\) 值也是一个问题。一般确定 \\\\( K \\\\) 的值的方法有以下几种：a）选定一个 \\\\( K \\\\) 的区间，例如 2～10，对每一个 \\\\( K \\\\) 值分别运行多次 k-means 算法，取目标函数 \\\\( J \\\\) 的值最小的 \\\\( K \\\\) 作为聚类数目；b）利用 [ Elbow 方法 ][4] 来确定 \\\\( K \\\\) 的值；c）利用 [ gap statistics ][5] 来确定 \\\\( K \\\\) 的值；d）根据问题的目的和对数据的粗略了解来确定 \\\\( K \\\\) 的值。\n- 其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 [ k-means++ 算法 ][6]）来克服此问题。\n- 其仅适用于数值类型的样本。但其扩展算法 [ k-modes 算法 ][7]（专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。\n\n其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 `cluster`，而关于 `cluster` 的其他信息一概没有做限制，那么根据 [ Occam 剃刀原理 ][8]，k-means 算法中的 `cluster` 应是最简单的那一种，即对应这三个假设。在博文 [*聚类分析（三）：高斯混合模型与 EM 算法*][9] 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。\n\n### 优点\n尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下：\n- 实现起来简单，总是可以收敛，算法复杂度低。\n- 其产生的聚类结果容易阐释。\n- 在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。\n\n---- \n# 实现 k-means 聚类\n在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 `KMeans` 类来展示 k-means 算法对不同类型的数据集的聚类效果。\n\n## 利用 python 实现 k-means 聚类\n首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy\nimport time\n\nfrom sklearn.datasets import make_blobs\n\nclass KMeansClust():\n    def __init__(self, n_clust=2, max_iter=50, tol=1e-10):\n        self.data_set = None\n        self.centers_his = []\n        self.pred_label = None\n        self.pred_label_his = []\n        self.n_clust = n_clust\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        n_samples, n_features = self.data_set.shape\n        self.pred_label = np.zeros(n_samples, dtype=int)\n \n    start_time = time.time()\n \n        # 初始化中心点\n        centers = np.random.rand(self.n_clust, n_features)\n        for i in range(n_features):\n            dim_min = np.min(self.data_set[:, i])\n            dim_max = np.max(self.data_set[:, i])\n            centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i]\n        self.centers_his.append(copy.deepcopy(centers))\n        self.pred_label_his.append(copy.deepcopy(self.pred_label))\n\n        print(\"The initializing cluster centers are: %s\" % centers)\n\n        # 开始迭代\n        pre_J = 1e10\n        iter_cnt = 0\n        while iter_cnt < self.max_iter:\n            iter_cnt += 1\n            # E 步：将各个样本点分配给距其最近的中心点所对应的 cluster\n            for i in range(n_samples):\n                self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** 2, axis=1))\n            # M 步：更新中心点\n            for i in range(self.n_clust):\n                centers[i] = np.mean(self.data_set[self.pred_label == i], axis=0)\n            self.centers_his.append(copy.deepcopy(centers))\n            self.pred_label_his.append(copy.deepcopy(self.pred_label))\n            # 重新计算目标函数 J\n            crt_J = np.sum((self.data_set - centers[self.pred_label]) ** 2) / n_samples\n            print(\"iteration %s, current value of J: %.4f\" % (iter_cnt, crt_J))\n            # 若前后两次迭代产生的目标函数的值变化不大，则结束迭代\n            if np.abs(pre_J - crt_J) < self.tol:\n                break\n            pre_J = crt_J\n\n        print(\"total iteration num: %s, final value of J: %.4f, time used: %.4f seconds\" \n                % (iter_cnt, crt_J, time.time() - start_time))\n\n    # 可视化算法每次迭代产生的结果\n    def plot_clustering(self, iter_cnt=-1, title=None):\n        if iter_cnt >= len(self.centers_his) or iter_cnt < -1:\n            raise Exception(\"iter_cnt is not valid!\")\n        plt.scatter(self.data_set[:, 0], self.data_set[:, 1],\n                        c=self.pred_label_his[iter_cnt], alpha=0.8)\n        plt.scatter(self.centers_his[iter_cnt][:, 0], self.centers_his[iter_cnt][:, 1],\n                        c='r', marker='x')\n        if title is not None:\n            plt.title(title, size=14)\n        plt.axis('on')\n        plt.tight_layout()\n```\n\n创建一个 `KMeansClust` 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 `predict` 即可对给定的数据集进行 k-means 聚类；方法 `plot_clustering` 则可以可视化每一次迭代所产生的结果。利用 `KMeansClust` 类进行 k-means 聚类的代码如下所示：\n```python\nif __name__ == '__main__':\n    # 生成数据集\n    n_samples = 1500\n    centers = [[0, 0], [5, 6], [8, 3.5]]\n    cluster_std = [2, 1.0, 0.5]\n    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)\n\n    # 运行 k-means 算法\n    kmeans_cluster = KMeansClust(n_clust=3)\n    kmeans_cluster.predict(X)\n\n    # 可视化中心点的初始化以及算法的聚类结果\n    plt.subplots(1, 2)\n    plt.subplot(1, 2, 1)\n    kmeans_cluster.plot_clustering(iter_cnt=0, title='initialization centers')\n    plt.subplot(1, 2, 2)\n    kmeans_cluster.plot_clustering(iter_cnt=-1, title='k-means clustering result')\n    plt.show()\n```\n\n以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：\n```\nThe initializing cluster centers are: \n[[-6.12152378  2.14971475]\n [ 6.71575768 -5.41421872]\n [-1.30016464 -2.3824513 ]]\niteration 1, current value of J: 12.5459\niteration 2, current value of J: 7.3479\niteration 3, current value of J: 5.2928\niteration 4, current value of J: 5.1493\niteration 5, current value of J: 5.1152\niteration 6, current value of J: 5.1079\niteration 7, current value of J: 5.1065\niteration 8, current value of J: 5.1063\niteration 9, current value of J: 5.1052\niteration 10, current value of J: 5.0970\niteration 11, current value of J: 5.0592\niteration 12, current value of J: 4.9402\niteration 13, current value of J: 4.5036\niteration 14, current value of J: 3.6246\niteration 15, current value of J: 3.2003\niteration 16, current value of J: 3.1678\niteration 17, current value of J: 3.1658\niteration 18, current value of J: 3.1657\niteration 19, current value of J: 3.1657\ntotal iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(good_initialization).png\" width = \"1000\" height = \"500\" alt = \"k-means 算法运行结果（好的初始化）\" align = center />\n</div>\n\n可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(bad_initialization).png\" width = \"1000\" height = \"500\" alt = \"k-means 算法运行结果（不好的初始化）\" align = center />\n</div>\n\n## 利用 sklearn 实现 k-means 聚类\nsklearn 中的 `KMeans` 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 [demo][10] 来展示 `KMeans` 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# 设定一个不合理的 K 值\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(\"Incorrect Number of Blobs\")\n\n# 产生一个非球形分布的数据集\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title(\"Anisotropicly Distributed Blobs\")\n\n# 产生一个各 cluster 的密度不一致的数据集\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(\"Unequal Variance\")\n\n# 产生一个各 cluster 的样本数目不一致的数据集\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))\ny_pred = KMeans(n_clusters=3,\n                random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(\"Unevenly Sized Blobs\")\n\nplt.show()\n```\n\n运行结果如下图所示：\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_different_datasets.png\" width = \"780\" height = \"650\" alt = \"k-means 算法在不同数据集下的表现\" align = center />\n</div>\n\n上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 \\\\( K \\\\) 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 `cluster` 的密度相近的假设条件；第四个数据集则破坏了各 `cluster` 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。\n\n\n\n\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[3]:\thttp://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n[4]:\thttps://www.wikipedia.com/en/Determining_the_number_of_clusters_in_a_data_set\n[5]:\thttps://datasciencelab.wordpress.com/tag/gap-statistic/\n[6]:\thttp://ilpubs.stanford.edu:8090/778/1/2006-13.pdf\n[7]:\thttp://www.irma-international.org/viewtitle/10828/\n[8]:\thttps://www.wikipedia.com/en/Occam's_razor\n[9]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[10]:\thttp://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py","source":"_posts/聚类分析（二）：k-means-算法.md","raw":"---\ntitle: 聚类分析（二）：k-means 算法\nlayout: post\ndate: 2018-02-10 20:33:10\ntags:\n- 聚类\n- 非监督学习\n- k-means 算法\ncategories:\n- 机器学习算法\nkeywords: 聚类,k-means,中心点,非监督学习,clustering,machine learning\n\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# k-means 聚类算法\nk-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 [*聚类分析（一）：层次聚类算法*][2] 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 `cluster` 都维持一个中心点（为属于该 `cluster` 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 `cluster`。在该算法中，`cluster` 的数目 \\\\( K \\\\) 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 \\\\( K \\\\) 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 `cluster` 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。\n\n## 表示为求解特定的优化问题\n假定数据集为 \\\\( \\\\lbrace {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N \\\\rbrace \\\\)，其包含 \\\\( N \\\\) 个样本点，每个样本点的维度为 \\\\( D \\\\)。我们的目的是将该数据集划分为 \\\\( K \\\\) 个 `cluster`，其中 \\\\( K \\\\) 是一个预先给定的值。假设每个 `cluster` 的中心点为向量 \\\\( {\\\\bf \\\\mu}\\_k  \\\\in \\\\Bbb{R}^{d} \\\\)，其中 \\\\( k = 1, …, K \\\\)。如前面所述，我们的目的是找到中心点 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\)，以及每个样本点所属的类别，以使每个样本点距其被分配的 `cluster` 所对应的中心点的平方 Euclidean 距离之和最小。\n\n为方便用数学符号描述该优化问题，我们以变量 \\\\( r\\_{nk} \\\\in \\\\lbrace 0, 1 \\\\rbrace \\\\) 来表示样本点 \\\\( {\\\\bf x}\\_n \\\\) 是否被分配至第 \\\\( k \\\\) 个 `cluster`，若样本点 \\\\( {\\\\bf x}\\_n \\\\) 被分配至第 \\\\( k \\\\) 个 `cluster`，则 \\\\( r\\_{nk} = 1 \\\\) 且 \\\\( r\\_{nj} = 0  \\\\) \\\\( (j \\\\neq k) \\\\)。由此我们可以写出目标函数\n$$  J = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} r\\_{nk} \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_k \\\\|^{2} $$\n它表示的即是每个样本点与其被分配的 `cluster` 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 和 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 的值，以使得目标函数 \\\\( J \\\\) 最小。\n\n我们可以看到，由于 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。\n\nk-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 选定初始值；然后在第一个迭代步骤中固定 \\\\( \\\\lbrace {\\\\bf \\\\mu}\\_k \\\\rbrace \\\\) 的值，对目标函数 \\\\( J \\\\) 根据  \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 求最小值；再在第二个迭代步骤中固定 \\\\( \\\\lbrace r\\_{nk} \\\\rbrace \\\\) 的值，对  \\\\( J \\\\) 根据 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求最小值；如此交替迭代，直至目标函数收敛。\n\n考虑迭代过程中的两个优化问题。首先考虑固定 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求解 \\\\( r\\_{nk} \\\\) 的问题，可以看到 \\\\( J \\\\) 是关于 \\\\( r\\_{nk} \\\\) 的线性函数，因此我们很容易给出一个闭式解：\\\\( J \\\\) 包含 \\\\( N \\\\) 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，\\\\( r\\_{nk} \\\\) 的解为\n$$ r\\_{nk} = \\\\begin{cases} 1, & \\\\text {if \\\\( k = \\\\rm {arg}  \\\\rm {min}\\_{j}  \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_j \\\\|^{2} \\\\) } \\\\\\\\ 0, & \\\\text {otherwise} \\\\end{cases} $$\n从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 `cluster`。\n\n再来考虑固定  \\\\( r\\_{nk} \\\\) 求解 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的问题，目标函数 \\\\( J \\\\) 是关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的二次函数，因此可以通过将 \\\\( J \\\\) 关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的导数置为 0 来求解 \\\\( J \\\\) 关于  \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的最小值：\n$$ 2 \\\\sum\\_{n = 1}^{N} r\\_{nk}({\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_k) = 0 $$\n容易求出 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的值为\n$$ {\\\\bf \\\\mu}\\_k = \\\\frac {\\\\sum\\_{n} r\\_{nk} {\\\\bf x}\\_n} {\\\\sum\\_{n} r\\_{nk} } $$\n该式表明，这一步迭代是将中心点更新为所有被分配至该 `cluster` 的样本点的均值。\n\nk-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 \\\\( J \\\\) 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。\n\n虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程：\n\n1.  初始化每个 `cluster` 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果；\n2. 将每个样本点分配给距离其最近的中心点所对应的 `cluster`；\n3. 更新每个 `cluster` 的中心点为被分配给该 `cluster` 的所有样本点的均值；\n4. 交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。\n\n[ PRML 教材 ][3]中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 \\\\( K = 2 \\\\)，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_k-means_algorithm.png\" width = \"660\" height = \"550\" alt = \"k-means 算法运行图示\" align = center />\n</div>\n\n## 算法复杂度及其优缺点\n### 算法复杂度\nk-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 \\\\( O(NK) \\\\) 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 \\\\(  O(D) \\\\)，其中 \\\\( N \\\\) 为样本点的个数，\\\\( K \\\\) 为指定的聚类个数，\\\\( D \\\\) 为样本点的维度；因此，一次迭代过程的时间复杂度为 \\\\( O(NKD) \\\\)，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 \\\\( O(NKD) \\\\)。\n  \n实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。\n\n### 缺点\nk-means 算法虽简单易用，但其有一些很明显的缺点，总结如下：\n- 由于其假设每个 `cluster` 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 `cluster`；但事实上的数据的 `cluster` 有可能并不是如此。\n- 由于其假设每一个 `cluster` 的分布形状都为球形（spherical），（“球形分布”表明一个 `cluster` 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 `cluster` ，对于具有非凸的或者形状很特别的 `cluster` 的数据集，其聚类效果往往很差。\n- 由于其假设不同的 `cluster` 具有相似的密度，因此对于具有密度差别较大的 `cluster` 的数据集，其聚类效果不好。\n- 其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。\n- `cluster` 的数目 \\\\( K \\\\) 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 \\\\( K \\\\) 值也是一个问题。一般确定 \\\\( K \\\\) 的值的方法有以下几种：a）选定一个 \\\\( K \\\\) 的区间，例如 2～10，对每一个 \\\\( K \\\\) 值分别运行多次 k-means 算法，取目标函数 \\\\( J \\\\) 的值最小的 \\\\( K \\\\) 作为聚类数目；b）利用 [ Elbow 方法 ][4] 来确定 \\\\( K \\\\) 的值；c）利用 [ gap statistics ][5] 来确定 \\\\( K \\\\) 的值；d）根据问题的目的和对数据的粗略了解来确定 \\\\( K \\\\) 的值。\n- 其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 [ k-means++ 算法 ][6]）来克服此问题。\n- 其仅适用于数值类型的样本。但其扩展算法 [ k-modes 算法 ][7]（专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。\n\n其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 `cluster`，而关于 `cluster` 的其他信息一概没有做限制，那么根据 [ Occam 剃刀原理 ][8]，k-means 算法中的 `cluster` 应是最简单的那一种，即对应这三个假设。在博文 [*聚类分析（三）：高斯混合模型与 EM 算法*][9] 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。\n\n### 优点\n尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下：\n- 实现起来简单，总是可以收敛，算法复杂度低。\n- 其产生的聚类结果容易阐释。\n- 在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。\n\n---- \n# 实现 k-means 聚类\n在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 `KMeans` 类来展示 k-means 算法对不同类型的数据集的聚类效果。\n\n## 利用 python 实现 k-means 聚类\n首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy\nimport time\n\nfrom sklearn.datasets import make_blobs\n\nclass KMeansClust():\n    def __init__(self, n_clust=2, max_iter=50, tol=1e-10):\n        self.data_set = None\n        self.centers_his = []\n        self.pred_label = None\n        self.pred_label_his = []\n        self.n_clust = n_clust\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        n_samples, n_features = self.data_set.shape\n        self.pred_label = np.zeros(n_samples, dtype=int)\n \n    start_time = time.time()\n \n        # 初始化中心点\n        centers = np.random.rand(self.n_clust, n_features)\n        for i in range(n_features):\n            dim_min = np.min(self.data_set[:, i])\n            dim_max = np.max(self.data_set[:, i])\n            centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i]\n        self.centers_his.append(copy.deepcopy(centers))\n        self.pred_label_his.append(copy.deepcopy(self.pred_label))\n\n        print(\"The initializing cluster centers are: %s\" % centers)\n\n        # 开始迭代\n        pre_J = 1e10\n        iter_cnt = 0\n        while iter_cnt < self.max_iter:\n            iter_cnt += 1\n            # E 步：将各个样本点分配给距其最近的中心点所对应的 cluster\n            for i in range(n_samples):\n                self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** 2, axis=1))\n            # M 步：更新中心点\n            for i in range(self.n_clust):\n                centers[i] = np.mean(self.data_set[self.pred_label == i], axis=0)\n            self.centers_his.append(copy.deepcopy(centers))\n            self.pred_label_his.append(copy.deepcopy(self.pred_label))\n            # 重新计算目标函数 J\n            crt_J = np.sum((self.data_set - centers[self.pred_label]) ** 2) / n_samples\n            print(\"iteration %s, current value of J: %.4f\" % (iter_cnt, crt_J))\n            # 若前后两次迭代产生的目标函数的值变化不大，则结束迭代\n            if np.abs(pre_J - crt_J) < self.tol:\n                break\n            pre_J = crt_J\n\n        print(\"total iteration num: %s, final value of J: %.4f, time used: %.4f seconds\" \n                % (iter_cnt, crt_J, time.time() - start_time))\n\n    # 可视化算法每次迭代产生的结果\n    def plot_clustering(self, iter_cnt=-1, title=None):\n        if iter_cnt >= len(self.centers_his) or iter_cnt < -1:\n            raise Exception(\"iter_cnt is not valid!\")\n        plt.scatter(self.data_set[:, 0], self.data_set[:, 1],\n                        c=self.pred_label_his[iter_cnt], alpha=0.8)\n        plt.scatter(self.centers_his[iter_cnt][:, 0], self.centers_his[iter_cnt][:, 1],\n                        c='r', marker='x')\n        if title is not None:\n            plt.title(title, size=14)\n        plt.axis('on')\n        plt.tight_layout()\n```\n\n创建一个 `KMeansClust` 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 `predict` 即可对给定的数据集进行 k-means 聚类；方法 `plot_clustering` 则可以可视化每一次迭代所产生的结果。利用 `KMeansClust` 类进行 k-means 聚类的代码如下所示：\n```python\nif __name__ == '__main__':\n    # 生成数据集\n    n_samples = 1500\n    centers = [[0, 0], [5, 6], [8, 3.5]]\n    cluster_std = [2, 1.0, 0.5]\n    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)\n\n    # 运行 k-means 算法\n    kmeans_cluster = KMeansClust(n_clust=3)\n    kmeans_cluster.predict(X)\n\n    # 可视化中心点的初始化以及算法的聚类结果\n    plt.subplots(1, 2)\n    plt.subplot(1, 2, 1)\n    kmeans_cluster.plot_clustering(iter_cnt=0, title='initialization centers')\n    plt.subplot(1, 2, 2)\n    kmeans_cluster.plot_clustering(iter_cnt=-1, title='k-means clustering result')\n    plt.show()\n```\n\n以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：\n```\nThe initializing cluster centers are: \n[[-6.12152378  2.14971475]\n [ 6.71575768 -5.41421872]\n [-1.30016464 -2.3824513 ]]\niteration 1, current value of J: 12.5459\niteration 2, current value of J: 7.3479\niteration 3, current value of J: 5.2928\niteration 4, current value of J: 5.1493\niteration 5, current value of J: 5.1152\niteration 6, current value of J: 5.1079\niteration 7, current value of J: 5.1065\niteration 8, current value of J: 5.1063\niteration 9, current value of J: 5.1052\niteration 10, current value of J: 5.0970\niteration 11, current value of J: 5.0592\niteration 12, current value of J: 4.9402\niteration 13, current value of J: 4.5036\niteration 14, current value of J: 3.6246\niteration 15, current value of J: 3.2003\niteration 16, current value of J: 3.1678\niteration 17, current value of J: 3.1658\niteration 18, current value of J: 3.1657\niteration 19, current value of J: 3.1657\ntotal iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(good_initialization).png\" width = \"1000\" height = \"500\" alt = \"k-means 算法运行结果（好的初始化）\" align = center />\n</div>\n\n可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(bad_initialization).png\" width = \"1000\" height = \"500\" alt = \"k-means 算法运行结果（不好的初始化）\" align = center />\n</div>\n\n## 利用 sklearn 实现 k-means 聚类\nsklearn 中的 `KMeans` 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 [demo][10] 来展示 `KMeans` 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# 设定一个不合理的 K 值\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(\"Incorrect Number of Blobs\")\n\n# 产生一个非球形分布的数据集\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title(\"Anisotropicly Distributed Blobs\")\n\n# 产生一个各 cluster 的密度不一致的数据集\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(\"Unequal Variance\")\n\n# 产生一个各 cluster 的样本数目不一致的数据集\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))\ny_pred = KMeans(n_clusters=3,\n                random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(\"Unevenly Sized Blobs\")\n\nplt.show()\n```\n\n运行结果如下图所示：\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_different_datasets.png\" width = \"780\" height = \"650\" alt = \"k-means 算法在不同数据集下的表现\" align = center />\n</div>\n\n上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 \\\\( K \\\\) 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 `cluster` 的密度相近的假设条件；第四个数据集则破坏了各 `cluster` 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。\n\n\n\n\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[3]:\thttp://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n[4]:\thttps://www.wikipedia.com/en/Determining_the_number_of_clusters_in_a_data_set\n[5]:\thttps://datasciencelab.wordpress.com/tag/gap-statistic/\n[6]:\thttp://ilpubs.stanford.edu:8090/778/1/2006-13.pdf\n[7]:\thttp://www.irma-international.org/viewtitle/10828/\n[8]:\thttps://www.wikipedia.com/en/Occam's_razor\n[9]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[10]:\thttp://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py","slug":"聚类分析（二）：k-means-算法","published":1,"updated":"2018-03-22T12:14:54.345Z","comments":1,"photos":[],"link":"","_id":"cjfclp3tv0005hws6lwu23jhe","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"k-means-聚类算法\"><a href=\"#k-means-聚类算法\" class=\"headerlink\" title=\"k-means 聚类算法\"></a>k-means 聚类算法</h1><p>k-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\"><em>聚类分析（一）：层次聚类算法</em></a> 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 <code>cluster</code> 都维持一个中心点（为属于该 <code>cluster</code> 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 <code>cluster</code>。在该算法中，<code>cluster</code> 的数目 \\( K \\) 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 \\( K \\) 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 <code>cluster</code> 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。</p>\n<h2 id=\"表示为求解特定的优化问题\"><a href=\"#表示为求解特定的优化问题\" class=\"headerlink\" title=\"表示为求解特定的优化问题\"></a>表示为求解特定的优化问题</h2><p>假定数据集为 \\( \\lbrace {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N \\rbrace \\)，其包含 \\( N \\) 个样本点，每个样本点的维度为 \\( D \\)。我们的目的是将该数据集划分为 \\( K \\) 个 <code>cluster</code>，其中 \\( K \\) 是一个预先给定的值。假设每个 <code>cluster</code> 的中心点为向量 \\( {\\bf \\mu}_k  \\in \\Bbb{R}^{d} \\)，其中 \\( k = 1, …, K \\)。如前面所述，我们的目的是找到中心点 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\)，以及每个样本点所属的类别，以使每个样本点距其被分配的 <code>cluster</code> 所对应的中心点的平方 Euclidean 距离之和最小。</p>\n<p>为方便用数学符号描述该优化问题，我们以变量 \\( r_{nk} \\in \\lbrace 0, 1 \\rbrace \\) 来表示样本点 \\( {\\bf x}_n \\) 是否被分配至第 \\( k \\) 个 <code>cluster</code>，若样本点 \\( {\\bf x}_n \\) 被分配至第 \\( k \\) 个 <code>cluster</code>，则 \\( r_{nk} = 1 \\) 且 \\( r_{nj} = 0  \\) \\( (j \\neq k) \\)。由此我们可以写出目标函数<br>$$  J = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} r_{nk} \\| {\\bf x}_n -  {\\bf \\mu}_k \\|^{2} $$<br>它表示的即是每个样本点与其被分配的 <code>cluster</code> 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 \\( \\lbrace r_{nk} \\rbrace \\) 和 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 的值，以使得目标函数 \\( J \\) 最小。</p>\n<p>我们可以看到，由于 \\( \\lbrace r_{nk} \\rbrace \\) 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。</p>\n<p>k-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 选定初始值；然后在第一个迭代步骤中固定 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 的值，对目标函数 \\( J \\) 根据  \\( \\lbrace r_{nk} \\rbrace \\) 求最小值；再在第二个迭代步骤中固定 \\( \\lbrace r_{nk} \\rbrace \\) 的值，对  \\( J \\) 根据 \\( {\\bf \\mu}_k \\) 求最小值；如此交替迭代，直至目标函数收敛。</p>\n<p>考虑迭代过程中的两个优化问题。首先考虑固定 \\( {\\bf \\mu}_k \\) 求解 \\( r_{nk} \\) 的问题，可以看到 \\( J \\) 是关于 \\( r_{nk} \\) 的线性函数，因此我们很容易给出一个闭式解：\\( J \\) 包含 \\( N \\) 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，\\( r_{nk} \\) 的解为<br>$$ r_{nk} = \\begin{cases} 1, &amp; \\text {if \\( k = \\rm {arg}  \\rm {min}_{j}  \\| {\\bf x}_n -  {\\bf \\mu}_j \\|^{2} \\) } \\\\ 0, &amp; \\text {otherwise} \\end{cases} $$<br>从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 <code>cluster</code>。</p>\n<p>再来考虑固定  \\( r_{nk} \\) 求解 \\( {\\bf \\mu}_k \\) 的问题，目标函数 \\( J \\) 是关于 \\( {\\bf \\mu}_k \\) 的二次函数，因此可以通过将 \\( J \\) 关于 \\( {\\bf \\mu}_k \\) 的导数置为 0 来求解 \\( J \\) 关于  \\( {\\bf \\mu}_k \\) 的最小值：<br>$$ 2 \\sum_{n = 1}^{N} r_{nk}({\\bf x}_n -  {\\bf \\mu}_k) = 0 $$<br>容易求出 \\( {\\bf \\mu}_k \\) 的值为<br>$$ {\\bf \\mu}_k = \\frac {\\sum_{n} r_{nk} {\\bf x}_n} {\\sum_{n} r_{nk} } $$<br>该式表明，这一步迭代是将中心点更新为所有被分配至该 <code>cluster</code> 的样本点的均值。</p>\n<p>k-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 \\( J \\) 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。</p>\n<p>虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程：</p>\n<ol>\n<li>初始化每个 <code>cluster</code> 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果；</li>\n<li>将每个样本点分配给距离其最近的中心点所对应的 <code>cluster</code>；</li>\n<li>更新每个 <code>cluster</code> 的中心点为被分配给该 <code>cluster</code> 的所有样本点的均值；</li>\n<li>交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。</li>\n</ol>\n<p><a href=\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> PRML 教材 </a>中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 \\( K = 2 \\)，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_k-means_algorithm.png\" width=\"660\" height=\"550\" alt=\"k-means 算法运行图示\" align=\"center\"><br></div>\n\n<h2 id=\"算法复杂度及其优缺点\"><a href=\"#算法复杂度及其优缺点\" class=\"headerlink\" title=\"算法复杂度及其优缺点\"></a>算法复杂度及其优缺点</h2><h3 id=\"算法复杂度\"><a href=\"#算法复杂度\" class=\"headerlink\" title=\"算法复杂度\"></a>算法复杂度</h3><p>k-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 \\( O(NK) \\) 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 \\(  O(D) \\)，其中 \\( N \\) 为样本点的个数，\\( K \\) 为指定的聚类个数，\\( D \\) 为样本点的维度；因此，一次迭代过程的时间复杂度为 \\( O(NKD) \\)，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 \\( O(NKD) \\)。</p>\n<p>实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。</p>\n<h3 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><p>k-means 算法虽简单易用，但其有一些很明显的缺点，总结如下：</p>\n<ul>\n<li>由于其假设每个 <code>cluster</code> 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 <code>cluster</code>；但事实上的数据的 <code>cluster</code> 有可能并不是如此。</li>\n<li>由于其假设每一个 <code>cluster</code> 的分布形状都为球形（spherical），（“球形分布”表明一个 <code>cluster</code> 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 <code>cluster</code> ，对于具有非凸的或者形状很特别的 <code>cluster</code> 的数据集，其聚类效果往往很差。</li>\n<li>由于其假设不同的 <code>cluster</code> 具有相似的密度，因此对于具有密度差别较大的 <code>cluster</code> 的数据集，其聚类效果不好。</li>\n<li>其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。</li>\n<li><code>cluster</code> 的数目 \\( K \\) 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 \\( K \\) 值也是一个问题。一般确定 \\( K \\) 的值的方法有以下几种：a）选定一个 \\( K \\) 的区间，例如 2～10，对每一个 \\( K \\) 值分别运行多次 k-means 算法，取目标函数 \\( J \\) 的值最小的 \\( K \\) 作为聚类数目；b）利用 <a href=\"https://www.wikipedia.com/en/Determining_the_number_of_clusters_in_a_data_set\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Elbow 方法 </a> 来确定 \\( K \\) 的值；c）利用 <a href=\"https://datasciencelab.wordpress.com/tag/gap-statistic/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> gap statistics </a> 来确定 \\( K \\) 的值；d）根据问题的目的和对数据的粗略了解来确定 \\( K \\) 的值。</li>\n<li>其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 <a href=\"http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> k-means++ 算法 </a>）来克服此问题。</li>\n<li>其仅适用于数值类型的样本。但其扩展算法 <a href=\"http://www.irma-international.org/viewtitle/10828/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> k-modes 算法 </a>（专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。</li>\n</ul>\n<p>其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 <code>cluster</code>，而关于 <code>cluster</code> 的其他信息一概没有做限制，那么根据 <a href=\"https://www.wikipedia.com/en/Occam&#39;s_razor\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Occam 剃刀原理 </a>，k-means 算法中的 <code>cluster</code> 应是最简单的那一种，即对应这三个假设。在博文 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\"><em>聚类分析（三）：高斯混合模型与 EM 算法</em></a> 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。</p>\n<h3 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h3><p>尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下：</p>\n<ul>\n<li>实现起来简单，总是可以收敛，算法复杂度低。</li>\n<li>其产生的聚类结果容易阐释。</li>\n<li>在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。</li>\n</ul>\n<hr>\n<h1 id=\"实现-k-means-聚类\"><a href=\"#实现-k-means-聚类\" class=\"headerlink\" title=\"实现 k-means 聚类\"></a>实现 k-means 聚类</h1><p>在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 <code>KMeans</code> 类来展示 k-means 算法对不同类型的数据集的聚类效果。</p>\n<h2 id=\"利用-python-实现-k-means-聚类\"><a href=\"#利用-python-实现-k-means-聚类\" class=\"headerlink\" title=\"利用 python 实现 k-means 聚类\"></a>利用 python 实现 k-means 聚类</h2><p>首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">import</span> copy</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KMeansClust</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_clust=<span class=\"number\">2</span>, max_iter=<span class=\"number\">50</span>, tol=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.centers_his = []</div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.pred_label_his = []</div><div class=\"line\">        self.n_clust = n_clust</div><div class=\"line\">        self.max_iter = max_iter</div><div class=\"line\">        self.tol = tol</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        n_samples, n_features = self.data_set.shape</div><div class=\"line\">        self.pred_label = np.zeros(n_samples, dtype=int)</div><div class=\"line\"> </div><div class=\"line\">    start_time = time.time()</div><div class=\"line\"> </div><div class=\"line\">        <span class=\"comment\"># 初始化中心点</span></div><div class=\"line\">        centers = np.random.rand(self.n_clust, n_features)</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_features):</div><div class=\"line\">            dim_min = np.min(self.data_set[:, i])</div><div class=\"line\">            dim_max = np.max(self.data_set[:, i])</div><div class=\"line\">            centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i]</div><div class=\"line\">        self.centers_his.append(copy.deepcopy(centers))</div><div class=\"line\">        self.pred_label_his.append(copy.deepcopy(self.pred_label))</div><div class=\"line\"></div><div class=\"line\">        print(<span class=\"string\">\"The initializing cluster centers are: %s\"</span> % centers)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始迭代</span></div><div class=\"line\">        pre_J = <span class=\"number\">1e10</span></div><div class=\"line\">        iter_cnt = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">while</span> iter_cnt &lt; self.max_iter:</div><div class=\"line\">            iter_cnt += <span class=\"number\">1</span></div><div class=\"line\">            <span class=\"comment\"># E 步：将各个样本点分配给距其最近的中心点所对应的 cluster</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_samples):</div><div class=\"line\">                self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** <span class=\"number\">2</span>, axis=<span class=\"number\">1</span>))</div><div class=\"line\">            <span class=\"comment\"># M 步：更新中心点</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_clust):</div><div class=\"line\">                centers[i] = np.mean(self.data_set[self.pred_label == i], axis=<span class=\"number\">0</span>)</div><div class=\"line\">            self.centers_his.append(copy.deepcopy(centers))</div><div class=\"line\">            self.pred_label_his.append(copy.deepcopy(self.pred_label))</div><div class=\"line\">            <span class=\"comment\"># 重新计算目标函数 J</span></div><div class=\"line\">            crt_J = np.sum((self.data_set - centers[self.pred_label]) ** <span class=\"number\">2</span>) / n_samples</div><div class=\"line\">            print(<span class=\"string\">\"iteration %s, current value of J: %.4f\"</span> % (iter_cnt, crt_J))</div><div class=\"line\">            <span class=\"comment\"># 若前后两次迭代产生的目标函数的值变化不大，则结束迭代</span></div><div class=\"line\">            <span class=\"keyword\">if</span> np.abs(pre_J - crt_J) &lt; self.tol:</div><div class=\"line\">                <span class=\"keyword\">break</span></div><div class=\"line\">            pre_J = crt_J</div><div class=\"line\"></div><div class=\"line\">        print(<span class=\"string\">\"total iteration num: %s, final value of J: %.4f, time used: %.4f seconds\"</span> </div><div class=\"line\">                % (iter_cnt, crt_J, time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化算法每次迭代产生的结果</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(self, iter_cnt=<span class=\"number\">-1</span>, title=None)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> iter_cnt &gt;= len(self.centers_his) <span class=\"keyword\">or</span> iter_cnt &lt; <span class=\"number\">-1</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> Exception(<span class=\"string\">\"iter_cnt is not valid!\"</span>)</div><div class=\"line\">        plt.scatter(self.data_set[:, <span class=\"number\">0</span>], self.data_set[:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=self.pred_label_his[iter_cnt], alpha=<span class=\"number\">0.8</span>)</div><div class=\"line\">        plt.scatter(self.centers_his[iter_cnt][:, <span class=\"number\">0</span>], self.centers_his[iter_cnt][:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=<span class=\"string\">'r'</span>, marker=<span class=\"string\">'x'</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">            plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">        plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">        plt.tight_layout()</div></pre></td></tr></table></figure></p>\n<p>创建一个 <code>KMeansClust</code> 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 <code>predict</code> 即可对给定的数据集进行 k-means 聚类；方法 <code>plot_clustering</code> 则可以可视化每一次迭代所产生的结果。利用 <code>KMeansClust</code> 类进行 k-means 聚类的代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    <span class=\"comment\"># 生成数据集</span></div><div class=\"line\">    n_samples = <span class=\"number\">1500</span></div><div class=\"line\">    centers = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">8</span>, <span class=\"number\">3.5</span>]]</div><div class=\"line\">    cluster_std = [<span class=\"number\">2</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>]</div><div class=\"line\">    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 运行 k-means 算法</span></div><div class=\"line\">    kmeans_cluster = KMeansClust(n_clust=<span class=\"number\">3</span>)</div><div class=\"line\">    kmeans_cluster.predict(X)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化中心点的初始化以及算法的聚类结果</span></div><div class=\"line\">    plt.subplots(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">    kmeans_cluster.plot_clustering(iter_cnt=<span class=\"number\">0</span>, title=<span class=\"string\">'initialization centers'</span>)</div><div class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">    kmeans_cluster.plot_clustering(iter_cnt=<span class=\"number\">-1</span>, title=<span class=\"string\">'k-means clustering result'</span>)</div><div class=\"line\">    plt.show()</div></pre></td></tr></table></figure></p>\n<p>以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">The initializing cluster centers are: </div><div class=\"line\">[[-6.12152378  2.14971475]</div><div class=\"line\"> [ 6.71575768 -5.41421872]</div><div class=\"line\"> [-1.30016464 -2.3824513 ]]</div><div class=\"line\">iteration 1, current value of J: 12.5459</div><div class=\"line\">iteration 2, current value of J: 7.3479</div><div class=\"line\">iteration 3, current value of J: 5.2928</div><div class=\"line\">iteration 4, current value of J: 5.1493</div><div class=\"line\">iteration 5, current value of J: 5.1152</div><div class=\"line\">iteration 6, current value of J: 5.1079</div><div class=\"line\">iteration 7, current value of J: 5.1065</div><div class=\"line\">iteration 8, current value of J: 5.1063</div><div class=\"line\">iteration 9, current value of J: 5.1052</div><div class=\"line\">iteration 10, current value of J: 5.0970</div><div class=\"line\">iteration 11, current value of J: 5.0592</div><div class=\"line\">iteration 12, current value of J: 4.9402</div><div class=\"line\">iteration 13, current value of J: 4.5036</div><div class=\"line\">iteration 14, current value of J: 3.6246</div><div class=\"line\">iteration 15, current value of J: 3.2003</div><div class=\"line\">iteration 16, current value of J: 3.1678</div><div class=\"line\">iteration 17, current value of J: 3.1658</div><div class=\"line\">iteration 18, current value of J: 3.1657</div><div class=\"line\">iteration 19, current value of J: 3.1657</div><div class=\"line\">total iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(good_initialization).png\" width=\"1000\" height=\"500\" alt=\"k-means 算法运行结果（好的初始化）\" align=\"center\"><br></div>\n\n<p>可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(bad_initialization).png\" width=\"1000\" height=\"500\" alt=\"k-means 算法运行结果（不好的初始化）\" align=\"center\"><br></div>\n\n<h2 id=\"利用-sklearn-实现-k-means-聚类\"><a href=\"#利用-sklearn-实现-k-means-聚类\" class=\"headerlink\" title=\"利用 sklearn 实现 k-means 聚类\"></a>利用 sklearn 实现 k-means 聚类</h2><p>sklearn 中的 <code>KMeans</code> 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">demo</a> 来展示 <code>KMeans</code> 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>, <span class=\"number\">12</span>))</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 设定一个不合理的 K 值</span></div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">2</span>, random_state=random_state).fit_predict(X)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">221</span>)</div><div class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Incorrect Number of Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个非球形分布的数据集</span></div><div class=\"line\">transformation = [[<span class=\"number\">0.60834549</span>, <span class=\"number\">-0.63667341</span>], [<span class=\"number\">-0.40887718</span>, <span class=\"number\">0.85253229</span>]]</div><div class=\"line\">X_aniso = np.dot(X, transformation)</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>, random_state=random_state).fit_predict(X_aniso)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">222</span>)</div><div class=\"line\">plt.scatter(X_aniso[:, <span class=\"number\">0</span>], X_aniso[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Anisotropicly Distributed Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的密度不一致的数据集</span></div><div class=\"line\">X_varied, y_varied = make_blobs(n_samples=n_samples,</div><div class=\"line\">                                cluster_std=[<span class=\"number\">1.0</span>, <span class=\"number\">2.5</span>, <span class=\"number\">0.5</span>],</div><div class=\"line\">                                random_state=random_state)</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>, random_state=random_state).fit_predict(X_varied)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">223</span>)</div><div class=\"line\">plt.scatter(X_varied[:, <span class=\"number\">0</span>], X_varied[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unequal Variance\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的样本数目不一致的数据集</span></div><div class=\"line\">X_filtered = np.vstack((X[y == <span class=\"number\">0</span>][:<span class=\"number\">500</span>], X[y == <span class=\"number\">1</span>][:<span class=\"number\">100</span>], X[y == <span class=\"number\">2</span>][:<span class=\"number\">50</span>]))</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>,</div><div class=\"line\">                random_state=random_state).fit_predict(X_filtered)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">224</span>)</div><div class=\"line\">plt.scatter(X_filtered[:, <span class=\"number\">0</span>], X_filtered[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unevenly Sized Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_different_datasets.png\" width=\"780\" height=\"650\" alt=\"k-means 算法在不同数据集下的表现\" align=\"center\"><br></div>\n\n<p>上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 \\( K \\) 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 <code>cluster</code> 的密度相近的假设条件；第四个数据集则破坏了各 <code>cluster</code> 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"k-means-聚类算法\"><a href=\"#k-means-聚类算法\" class=\"headerlink\" title=\"k-means 聚类算法\"></a>k-means 聚类算法</h1><p>k-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\"><em>聚类分析（一）：层次聚类算法</em></a> 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 <code>cluster</code> 都维持一个中心点（为属于该 <code>cluster</code> 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 <code>cluster</code>。在该算法中，<code>cluster</code> 的数目 \\( K \\) 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 \\( K \\) 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 <code>cluster</code> 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。</p>\n<h2 id=\"表示为求解特定的优化问题\"><a href=\"#表示为求解特定的优化问题\" class=\"headerlink\" title=\"表示为求解特定的优化问题\"></a>表示为求解特定的优化问题</h2><p>假定数据集为 \\( \\lbrace {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N \\rbrace \\)，其包含 \\( N \\) 个样本点，每个样本点的维度为 \\( D \\)。我们的目的是将该数据集划分为 \\( K \\) 个 <code>cluster</code>，其中 \\( K \\) 是一个预先给定的值。假设每个 <code>cluster</code> 的中心点为向量 \\( {\\bf \\mu}_k  \\in \\Bbb{R}^{d} \\)，其中 \\( k = 1, …, K \\)。如前面所述，我们的目的是找到中心点 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\)，以及每个样本点所属的类别，以使每个样本点距其被分配的 <code>cluster</code> 所对应的中心点的平方 Euclidean 距离之和最小。</p>\n<p>为方便用数学符号描述该优化问题，我们以变量 \\( r_{nk} \\in \\lbrace 0, 1 \\rbrace \\) 来表示样本点 \\( {\\bf x}_n \\) 是否被分配至第 \\( k \\) 个 <code>cluster</code>，若样本点 \\( {\\bf x}_n \\) 被分配至第 \\( k \\) 个 <code>cluster</code>，则 \\( r_{nk} = 1 \\) 且 \\( r_{nj} = 0  \\) \\( (j \\neq k) \\)。由此我们可以写出目标函数<br>$$  J = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} r_{nk} \\| {\\bf x}_n -  {\\bf \\mu}_k \\|^{2} $$<br>它表示的即是每个样本点与其被分配的 <code>cluster</code> 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 \\( \\lbrace r_{nk} \\rbrace \\) 和 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 的值，以使得目标函数 \\( J \\) 最小。</p>\n<p>我们可以看到，由于 \\( \\lbrace r_{nk} \\rbrace \\) 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。</p>\n<p>k-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 选定初始值；然后在第一个迭代步骤中固定 \\( \\lbrace {\\bf \\mu}_k \\rbrace \\) 的值，对目标函数 \\( J \\) 根据  \\( \\lbrace r_{nk} \\rbrace \\) 求最小值；再在第二个迭代步骤中固定 \\( \\lbrace r_{nk} \\rbrace \\) 的值，对  \\( J \\) 根据 \\( {\\bf \\mu}_k \\) 求最小值；如此交替迭代，直至目标函数收敛。</p>\n<p>考虑迭代过程中的两个优化问题。首先考虑固定 \\( {\\bf \\mu}_k \\) 求解 \\( r_{nk} \\) 的问题，可以看到 \\( J \\) 是关于 \\( r_{nk} \\) 的线性函数，因此我们很容易给出一个闭式解：\\( J \\) 包含 \\( N \\) 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，\\( r_{nk} \\) 的解为<br>$$ r_{nk} = \\begin{cases} 1, &amp; \\text {if \\( k = \\rm {arg}  \\rm {min}_{j}  \\| {\\bf x}_n -  {\\bf \\mu}_j \\|^{2} \\) } \\\\ 0, &amp; \\text {otherwise} \\end{cases} $$<br>从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 <code>cluster</code>。</p>\n<p>再来考虑固定  \\( r_{nk} \\) 求解 \\( {\\bf \\mu}_k \\) 的问题，目标函数 \\( J \\) 是关于 \\( {\\bf \\mu}_k \\) 的二次函数，因此可以通过将 \\( J \\) 关于 \\( {\\bf \\mu}_k \\) 的导数置为 0 来求解 \\( J \\) 关于  \\( {\\bf \\mu}_k \\) 的最小值：<br>$$ 2 \\sum_{n = 1}^{N} r_{nk}({\\bf x}_n -  {\\bf \\mu}_k) = 0 $$<br>容易求出 \\( {\\bf \\mu}_k \\) 的值为<br>$$ {\\bf \\mu}_k = \\frac {\\sum_{n} r_{nk} {\\bf x}_n} {\\sum_{n} r_{nk} } $$<br>该式表明，这一步迭代是将中心点更新为所有被分配至该 <code>cluster</code> 的样本点的均值。</p>\n<p>k-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 \\( J \\) 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。</p>\n<p>虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程：</p>\n<ol>\n<li>初始化每个 <code>cluster</code> 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果；</li>\n<li>将每个样本点分配给距离其最近的中心点所对应的 <code>cluster</code>；</li>\n<li>更新每个 <code>cluster</code> 的中心点为被分配给该 <code>cluster</code> 的所有样本点的均值；</li>\n<li>交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。</li>\n</ol>\n<p><a href=\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> PRML 教材 </a>中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 \\( K = 2 \\)，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_k-means_algorithm.png\" width=\"660\" height=\"550\" alt=\"k-means 算法运行图示\" align=\"center\"><br></div>\n\n<h2 id=\"算法复杂度及其优缺点\"><a href=\"#算法复杂度及其优缺点\" class=\"headerlink\" title=\"算法复杂度及其优缺点\"></a>算法复杂度及其优缺点</h2><h3 id=\"算法复杂度\"><a href=\"#算法复杂度\" class=\"headerlink\" title=\"算法复杂度\"></a>算法复杂度</h3><p>k-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 \\( O(NK) \\) 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 \\(  O(D) \\)，其中 \\( N \\) 为样本点的个数，\\( K \\) 为指定的聚类个数，\\( D \\) 为样本点的维度；因此，一次迭代过程的时间复杂度为 \\( O(NKD) \\)，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 \\( O(NKD) \\)。</p>\n<p>实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。</p>\n<h3 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h3><p>k-means 算法虽简单易用，但其有一些很明显的缺点，总结如下：</p>\n<ul>\n<li>由于其假设每个 <code>cluster</code> 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 <code>cluster</code>；但事实上的数据的 <code>cluster</code> 有可能并不是如此。</li>\n<li>由于其假设每一个 <code>cluster</code> 的分布形状都为球形（spherical），（“球形分布”表明一个 <code>cluster</code> 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 <code>cluster</code> ，对于具有非凸的或者形状很特别的 <code>cluster</code> 的数据集，其聚类效果往往很差。</li>\n<li>由于其假设不同的 <code>cluster</code> 具有相似的密度，因此对于具有密度差别较大的 <code>cluster</code> 的数据集，其聚类效果不好。</li>\n<li>其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。</li>\n<li><code>cluster</code> 的数目 \\( K \\) 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 \\( K \\) 值也是一个问题。一般确定 \\( K \\) 的值的方法有以下几种：a）选定一个 \\( K \\) 的区间，例如 2～10，对每一个 \\( K \\) 值分别运行多次 k-means 算法，取目标函数 \\( J \\) 的值最小的 \\( K \\) 作为聚类数目；b）利用 <a href=\"https://www.wikipedia.com/en/Determining_the_number_of_clusters_in_a_data_set\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Elbow 方法 </a> 来确定 \\( K \\) 的值；c）利用 <a href=\"https://datasciencelab.wordpress.com/tag/gap-statistic/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> gap statistics </a> 来确定 \\( K \\) 的值；d）根据问题的目的和对数据的粗略了解来确定 \\( K \\) 的值。</li>\n<li>其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 <a href=\"http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> k-means++ 算法 </a>）来克服此问题。</li>\n<li>其仅适用于数值类型的样本。但其扩展算法 <a href=\"http://www.irma-international.org/viewtitle/10828/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> k-modes 算法 </a>（专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。</li>\n</ul>\n<p>其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 <code>cluster</code>，而关于 <code>cluster</code> 的其他信息一概没有做限制，那么根据 <a href=\"https://www.wikipedia.com/en/Occam&#39;s_razor\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Occam 剃刀原理 </a>，k-means 算法中的 <code>cluster</code> 应是最简单的那一种，即对应这三个假设。在博文 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\"><em>聚类分析（三）：高斯混合模型与 EM 算法</em></a> 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。</p>\n<h3 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h3><p>尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下：</p>\n<ul>\n<li>实现起来简单，总是可以收敛，算法复杂度低。</li>\n<li>其产生的聚类结果容易阐释。</li>\n<li>在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。</li>\n</ul>\n<hr>\n<h1 id=\"实现-k-means-聚类\"><a href=\"#实现-k-means-聚类\" class=\"headerlink\" title=\"实现 k-means 聚类\"></a>实现 k-means 聚类</h1><p>在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 <code>KMeans</code> 类来展示 k-means 算法对不同类型的数据集的聚类效果。</p>\n<h2 id=\"利用-python-实现-k-means-聚类\"><a href=\"#利用-python-实现-k-means-聚类\" class=\"headerlink\" title=\"利用 python 实现 k-means 聚类\"></a>利用 python 实现 k-means 聚类</h2><p>首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">import</span> copy</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">KMeansClust</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_clust=<span class=\"number\">2</span>, max_iter=<span class=\"number\">50</span>, tol=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.centers_his = []</div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.pred_label_his = []</div><div class=\"line\">        self.n_clust = n_clust</div><div class=\"line\">        self.max_iter = max_iter</div><div class=\"line\">        self.tol = tol</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        n_samples, n_features = self.data_set.shape</div><div class=\"line\">        self.pred_label = np.zeros(n_samples, dtype=int)</div><div class=\"line\"> </div><div class=\"line\">    start_time = time.time()</div><div class=\"line\"> </div><div class=\"line\">        <span class=\"comment\"># 初始化中心点</span></div><div class=\"line\">        centers = np.random.rand(self.n_clust, n_features)</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_features):</div><div class=\"line\">            dim_min = np.min(self.data_set[:, i])</div><div class=\"line\">            dim_max = np.max(self.data_set[:, i])</div><div class=\"line\">            centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i]</div><div class=\"line\">        self.centers_his.append(copy.deepcopy(centers))</div><div class=\"line\">        self.pred_label_his.append(copy.deepcopy(self.pred_label))</div><div class=\"line\"></div><div class=\"line\">        print(<span class=\"string\">\"The initializing cluster centers are: %s\"</span> % centers)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始迭代</span></div><div class=\"line\">        pre_J = <span class=\"number\">1e10</span></div><div class=\"line\">        iter_cnt = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">while</span> iter_cnt &lt; self.max_iter:</div><div class=\"line\">            iter_cnt += <span class=\"number\">1</span></div><div class=\"line\">            <span class=\"comment\"># E 步：将各个样本点分配给距其最近的中心点所对应的 cluster</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n_samples):</div><div class=\"line\">                self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** <span class=\"number\">2</span>, axis=<span class=\"number\">1</span>))</div><div class=\"line\">            <span class=\"comment\"># M 步：更新中心点</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_clust):</div><div class=\"line\">                centers[i] = np.mean(self.data_set[self.pred_label == i], axis=<span class=\"number\">0</span>)</div><div class=\"line\">            self.centers_his.append(copy.deepcopy(centers))</div><div class=\"line\">            self.pred_label_his.append(copy.deepcopy(self.pred_label))</div><div class=\"line\">            <span class=\"comment\"># 重新计算目标函数 J</span></div><div class=\"line\">            crt_J = np.sum((self.data_set - centers[self.pred_label]) ** <span class=\"number\">2</span>) / n_samples</div><div class=\"line\">            print(<span class=\"string\">\"iteration %s, current value of J: %.4f\"</span> % (iter_cnt, crt_J))</div><div class=\"line\">            <span class=\"comment\"># 若前后两次迭代产生的目标函数的值变化不大，则结束迭代</span></div><div class=\"line\">            <span class=\"keyword\">if</span> np.abs(pre_J - crt_J) &lt; self.tol:</div><div class=\"line\">                <span class=\"keyword\">break</span></div><div class=\"line\">            pre_J = crt_J</div><div class=\"line\"></div><div class=\"line\">        print(<span class=\"string\">\"total iteration num: %s, final value of J: %.4f, time used: %.4f seconds\"</span> </div><div class=\"line\">                % (iter_cnt, crt_J, time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化算法每次迭代产生的结果</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(self, iter_cnt=<span class=\"number\">-1</span>, title=None)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> iter_cnt &gt;= len(self.centers_his) <span class=\"keyword\">or</span> iter_cnt &lt; <span class=\"number\">-1</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> Exception(<span class=\"string\">\"iter_cnt is not valid!\"</span>)</div><div class=\"line\">        plt.scatter(self.data_set[:, <span class=\"number\">0</span>], self.data_set[:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=self.pred_label_his[iter_cnt], alpha=<span class=\"number\">0.8</span>)</div><div class=\"line\">        plt.scatter(self.centers_his[iter_cnt][:, <span class=\"number\">0</span>], self.centers_his[iter_cnt][:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=<span class=\"string\">'r'</span>, marker=<span class=\"string\">'x'</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">            plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">        plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">        plt.tight_layout()</div></pre></td></tr></table></figure></p>\n<p>创建一个 <code>KMeansClust</code> 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 <code>predict</code> 即可对给定的数据集进行 k-means 聚类；方法 <code>plot_clustering</code> 则可以可视化每一次迭代所产生的结果。利用 <code>KMeansClust</code> 类进行 k-means 聚类的代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</div><div class=\"line\">    <span class=\"comment\"># 生成数据集</span></div><div class=\"line\">    n_samples = <span class=\"number\">1500</span></div><div class=\"line\">    centers = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">8</span>, <span class=\"number\">3.5</span>]]</div><div class=\"line\">    cluster_std = [<span class=\"number\">2</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>]</div><div class=\"line\">    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 运行 k-means 算法</span></div><div class=\"line\">    kmeans_cluster = KMeansClust(n_clust=<span class=\"number\">3</span>)</div><div class=\"line\">    kmeans_cluster.predict(X)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化中心点的初始化以及算法的聚类结果</span></div><div class=\"line\">    plt.subplots(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">    kmeans_cluster.plot_clustering(iter_cnt=<span class=\"number\">0</span>, title=<span class=\"string\">'initialization centers'</span>)</div><div class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">    kmeans_cluster.plot_clustering(iter_cnt=<span class=\"number\">-1</span>, title=<span class=\"string\">'k-means clustering result'</span>)</div><div class=\"line\">    plt.show()</div></pre></td></tr></table></figure></p>\n<p>以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">The initializing cluster centers are: </div><div class=\"line\">[[-6.12152378  2.14971475]</div><div class=\"line\"> [ 6.71575768 -5.41421872]</div><div class=\"line\"> [-1.30016464 -2.3824513 ]]</div><div class=\"line\">iteration 1, current value of J: 12.5459</div><div class=\"line\">iteration 2, current value of J: 7.3479</div><div class=\"line\">iteration 3, current value of J: 5.2928</div><div class=\"line\">iteration 4, current value of J: 5.1493</div><div class=\"line\">iteration 5, current value of J: 5.1152</div><div class=\"line\">iteration 6, current value of J: 5.1079</div><div class=\"line\">iteration 7, current value of J: 5.1065</div><div class=\"line\">iteration 8, current value of J: 5.1063</div><div class=\"line\">iteration 9, current value of J: 5.1052</div><div class=\"line\">iteration 10, current value of J: 5.0970</div><div class=\"line\">iteration 11, current value of J: 5.0592</div><div class=\"line\">iteration 12, current value of J: 4.9402</div><div class=\"line\">iteration 13, current value of J: 4.5036</div><div class=\"line\">iteration 14, current value of J: 3.6246</div><div class=\"line\">iteration 15, current value of J: 3.2003</div><div class=\"line\">iteration 16, current value of J: 3.1678</div><div class=\"line\">iteration 17, current value of J: 3.1658</div><div class=\"line\">iteration 18, current value of J: 3.1657</div><div class=\"line\">iteration 19, current value of J: 3.1657</div><div class=\"line\">total iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(good_initialization).png\" width=\"1000\" height=\"500\" alt=\"k-means 算法运行结果（好的初始化）\" align=\"center\"><br></div>\n\n<p>可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_result(bad_initialization).png\" width=\"1000\" height=\"500\" alt=\"k-means 算法运行结果（不好的初始化）\" align=\"center\"><br></div>\n\n<h2 id=\"利用-sklearn-实现-k-means-聚类\"><a href=\"#利用-sklearn-实现-k-means-聚类\" class=\"headerlink\" title=\"利用 sklearn 实现 k-means 聚类\"></a>利用 sklearn 实现 k-means 聚类</h2><p>sklearn 中的 <code>KMeans</code> 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">demo</a> 来展示 <code>KMeans</code> 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>, <span class=\"number\">12</span>))</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 设定一个不合理的 K 值</span></div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">2</span>, random_state=random_state).fit_predict(X)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">221</span>)</div><div class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Incorrect Number of Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个非球形分布的数据集</span></div><div class=\"line\">transformation = [[<span class=\"number\">0.60834549</span>, <span class=\"number\">-0.63667341</span>], [<span class=\"number\">-0.40887718</span>, <span class=\"number\">0.85253229</span>]]</div><div class=\"line\">X_aniso = np.dot(X, transformation)</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>, random_state=random_state).fit_predict(X_aniso)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">222</span>)</div><div class=\"line\">plt.scatter(X_aniso[:, <span class=\"number\">0</span>], X_aniso[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Anisotropicly Distributed Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的密度不一致的数据集</span></div><div class=\"line\">X_varied, y_varied = make_blobs(n_samples=n_samples,</div><div class=\"line\">                                cluster_std=[<span class=\"number\">1.0</span>, <span class=\"number\">2.5</span>, <span class=\"number\">0.5</span>],</div><div class=\"line\">                                random_state=random_state)</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>, random_state=random_state).fit_predict(X_varied)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">223</span>)</div><div class=\"line\">plt.scatter(X_varied[:, <span class=\"number\">0</span>], X_varied[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unequal Variance\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的样本数目不一致的数据集</span></div><div class=\"line\">X_filtered = np.vstack((X[y == <span class=\"number\">0</span>][:<span class=\"number\">500</span>], X[y == <span class=\"number\">1</span>][:<span class=\"number\">100</span>], X[y == <span class=\"number\">2</span>][:<span class=\"number\">50</span>]))</div><div class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">3</span>,</div><div class=\"line\">                random_state=random_state).fit_predict(X_filtered)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">224</span>)</div><div class=\"line\">plt.scatter(X_filtered[:, <span class=\"number\">0</span>], X_filtered[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unevenly Sized Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/kmeans_different_datasets.png\" width=\"780\" height=\"650\" alt=\"k-means 算法在不同数据集下的表现\" align=\"center\"><br></div>\n\n<p>上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 \\( K \\) 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 <code>cluster</code> 的密度相近的假设条件；第四个数据集则破坏了各 <code>cluster</code> 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。</p>\n"},{"title":"聚类分析（四）：DBSCAN 算法","date":"2018-03-22T12:18:35.000Z","keywords":"聚类,非监督学习,密度聚类,clustering,machine learning","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# DBSCAN 算法\nDBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的**密度聚类算法**。密度聚类算法认为各个 `cluster` 是样本点密度高的区域，而 `cluster` 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。\n\n要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。\n\n## 几个定义\n在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 `cluster`，而噪声点则存在于各 `cluster` 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。\n\n>**定义1：** （样本点的 \\\\( \\\\text {Eps} \\\\)-邻域）假设数据集为 \\\\( \\\\bf X \\\\)，则样本点 \\\\( \\\\bf p \\\\) 的 \\\\( \\\\text {Eps} \\\\)-邻域定义为 \\\\( N\\_{\\\\text {Eps}} ({\\\\bf p}) = \\\\lbrace {\\\\bf q} \\\\in {\\\\bf X} | d({\\\\bf p}, {\\\\bf q}) \\\\le {\\\\text {Eps}} \\rbrace \\\\). \n\n我们再给定一个参数 \\\\( \\\\text {MinPts} \\\\)，并定义核心点须满足的条件为：其 \\\\( \\\\text {Eps} \\\\)-邻域内包含的样本点的数目不小于 \\\\( \\\\text {MinPts} \\\\) ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 `cluster` 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 `cluster` 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 `cluster` 所具有的形式。\n\n>**定义2： **（密度直达）我们称样本点 \\\\( \\\\bf p \\\\) 是由样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 密度直达的，如果它们满足 \\\\( {\\\\bf p} \\\\in N\\_{\\\\text {Eps}} ({\\\\bf q}) \\\\) 且 \\\\( |N\\_{\\\\text {Eps}}({\\\\bf q})| \\\\ge \\\\text {MinPts} \\\\) （即样本点 \\\\( \\\\bf q \\\\) 是核心点）. \n\n很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 \\\\( \\\\text {Eps} = 5 \\\\)，可以看到，样本点 \\\\( \\\\bf q \\\\) 为核心点，样本点 \\\\( \\\\bf p \\\\) 不是核心点，且 \\\\( \\\\bf p \\\\) 在 \\\\( \\\\bf q \\\\) 的 \\\\( \\\\text {Eps} \\\\)-邻域内，因而 \\\\( \\\\bf p \\\\) 可由 \\\\( \\\\bf q \\\\) 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_definition_in_dbscan.png\" width = \"900\" height = \"600\" alt = \"DBSCAN 中的相关概念的图示\" align = center />\n</div>\n\n>**定义3：**（密度可达）我们称样本点 \\\\( \\\\bf p \\\\) 是由样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 密度可达的，如果存在一系列的样本点 \\\\( {\\\\bf p}\\_{1}, ..., {\\\\bf p}\\_n \\\\)（其中 \\\\( {\\\\bf p}\\_1 = {\\\\bf q}, {\\\\bf p}\\_n = {\\\\bf p} \\\\)）使得对于 \\\\( i = 1, ..., n-1 \\\\)，样本点 \\\\( {\\\\bf p}\\_{i + 1} \\\\) 可由样本点 \\\\( {\\\\bf p}\\_{i} \\\\) 密度可达.\n\n我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 `cluster` 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。\n\n>**定义4：**（密度相连）我们称样本点 \\\\( \\\\bf p \\\\) 与样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 是密度相连的，如果存在一个样本点 \\\\( {\\\\bf o} \\\\)，使得 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\) 均由样本点 \\\\( \\\\bf o \\\\) 密度可达。\n\n密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 `cluster` ，使得每一个 `cluster` 中的任意两个样本点都是密度相连的，且每一个 `cluster` 在密度可达的意义上都是最大的。`cluster` 的定义如下：\n\n>**定义5：**（`cluster`）假设数据集为 \\\\( \\\\bf X \\\\)，给定参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\)，则某个 `cluster` \\\\( C \\\\) 是数据集  \\\\( \\\\bf X \\\\) 的一个非空子集，且满足如下条件：\n\t\t>1）对于任意的样本点 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\)，如果 \\\\( {\\\\bf p} \\\\in C \\\\) 且 \\\\( \\\\bf q \\\\) 可由 \\\\( \\\\bf p \\\\) 密度可达，则 \\\\( {\\\\bf q} \\\\in C \\\\) .（最大性）\n\t\t>2）对于 \\\\( C \\\\) 中的任意样本点 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\)， \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\) 关于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 是密度相连的.（连接性）\n\n这样我们就定义出了 DBSCAN 算法最终产生的 `cluster` 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 `cluster` 的样本点。根据以上定义，由于一个 `cluster` 中的任意两个样本点都是密度相连的，每一个 `cluster` 至少包含 \\\\( \\\\text {MinPts} \\\\) 个样本点。 \n\n## 算法描述\nDBSCAN 算法就是为了寻找以上定义 5 中定义的 `cluster`，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 `cluster`，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 `cluster`，…，依此过程，直至没有未被聚类的核心点为止。依照 `cluster` 的“最大性”和“连接性”，在给定参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 的情况下，最终产生的 `cluster` 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 `cluster` 的边缘的情况除外，这种情况下，这些临界位置的样本点的 `cluster` 归属与种子的选取顺序有关）。\n\n## 合理地选取参数\nDBSCAN 的聚类结果和效果取决于参数 \\\\( \\\\text {Eps} \\\\) 和 \\\\( \\\\text {MinPts} \\\\) 以及距离衡量方法的选取。\n\n由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 [*聚类分析（一）：层次聚类算法*][2]。\n\n然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 `cluster` 所对应的 \\\\( \\\\text {Eps} \\\\) 和 \\\\( \\\\text {MinPts} \\\\)。这里我们给出一种启发式的参数选取方法。假设 \\\\( d \\\\) 是某个样本点 \\\\( \\\\bf p \\\\) 距离它的第 \\\\( k \\\\) 近邻的距离，则一般情况下 \\\\( \\\\bf p \\\\) 的 \\\\( d \\\\)-邻域内正好包含 \\\\( k + 1 \\\\) 个样本点。我们可以推断，在一个合理的 `cluster` 内，改变 \\\\( k \\\\) 的值不应该导致 \\\\( d \\\\) 值有较大的变化，除非 \\\\( \\\\bf p \\\\) 的第 \\\\( k \\\\) 近邻们（\\\\( k = 1, 2, 3,… \\\\) ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 `cluster`。\n\n因而我们一般将 \\\\( k \\\\) 的值固定下来，一个合理的选择是令 \\\\( k = 3 \\\\) 或 \\\\( k = 4 \\\\)，那么 \\\\( \\\\text {MinPts} \\\\) 的值也确定了（为 \\\\( k + 1 \\\\)）；然后再来看每个样本点的 \\\\( \\\\text {k-dist} \\\\) 距离（即该样本点距离它的第 \\\\( k \\\\) 近邻的距离）的分布情况，我们把每个样本点的 \\\\( \\\\text {k-dist} \\\\) 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 \\\\( \\\\text {Eps} \\\\)，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 \\\\( \\\\text {k-dist} \\\\) 距离作为 \\\\( \\\\text {Eps} \\\\)，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 \\\\( \\\\text {k-dist} \\\\) 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 \\\\( \\\\text {k-dist} \\\\) 距离作为 \\\\( \\\\text {k-dist} \\\\) ，如下图中的图（b）所示。\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_parameter_selection_for_dbscan.png\" width = \"900\" height = \"500\" alt = \"DBSCAN 中数据集的 k-dist 图\" align = center />\n</div>\n\n## 算法的复杂度及其优缺点\n### 算法复杂度\nDBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 \\\\( \\\\text {Eps} \\\\)-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 \\\\( O(N) \\\\)，其中 \\\\( N \\\\) 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 [R\\* tree][3] 建立索引时，查询邻域的平均时间复杂度为 \\\\( O(\\\\log N) \\\\)。因而，DBSCAN 算法的平均时间复杂度为 \\\\( O(N\\\\log N) \\\\)；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 \\\\( O(N) \\\\)。\n\n### 优缺点\nDBSCAN 算法有很多优点，总结如下：\n- DBSCAN 不需要事先指定最终需要生成的 `cluster` 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 `cluster` 的数目，实际上我们对数据集是有相当多的了解的。\n- DBSCAN 可以找到具有任意形状的 `cluster`，如非凸的 `cluster`，这基于其对 `cluster` 的定义（`cluster` 是由密度低的区域所隔开的密度高的区域）。\n- DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。\n- DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 `cluster` 的边缘的情况，这种情况下，这些临界位置的样本点的 `cluster` 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。\n- DBSCAN 的运行速度快，当采用索引时，其复杂度仅为  \\\\( O(N\\\\log N) \\\\)。\n\n当然，它也有一个主要缺点，即对于具有密度相差较大的 `cluster` 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 \\\\( \\\\text {MinPts} \\\\) 和 \\\\( \\\\text {Eps} \\\\) 是参照数据集中最稀疏的 `cluster` 所选取的，那么很有可能最终所有的样本最终都被归为一个 `cluster`，因为可能数据集中的 `cluster` 之间的区域的密度和最稀疏的 `cluster` 的密度相当；如果选取的参数 \\\\( \\\\text {MinPts} \\\\) 和 \\\\( \\\\text {Eps} \\\\) 倾向于聚出密度比较大的 `cluster`，那么极有可能，比较稀疏的这些 `cluster` 都被归为噪声。[ OPTICS][4] 算法一般被用来解决这一问题。\n\n---- \n# 实现 DBSCAN 聚类\n现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 `DBSCAN`，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 `min_pts` 和 `eps` 以及距离衡量方法（默认为 `euclidean`），对数据集进行聚类时，我们对构造出来的实例调用方法 `predict`。`predict` 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 `rtree` 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：\n```python\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nimport rtree\n\nUNCLASSIFIED = -2\nNOISE = -1\n\n\nclass DBSCAN():\n    def __init__(self, min_pts, eps, metric='euclidean', index_flag=True):\n        self.min_pts = min_pts\n        self.eps = eps\n        self.metric = metric\n        self.index_flag = index_flag\n        self.data_set = None\n        self.pred_label = None\n        self.core_points = set()\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        self.n_samples, self.n_features = self.data_set.shape\n\n        self.data_index = None\n        self.dist_matrix = None\n\n        start_time = time.time()\n        if self.n_features == 2 and self.metric == 'euclidean' \\\n            and self.index_flag:\n            # 此种情形下对数据集建立空间索引\n            self.construct_index()\n        else:\n            # 其它情形下对数据集计算距离矩阵\n            self.cal_dist_matrix()\n\n        self.pred_label = np.array([UNCLASSIFIED] * self.n_samples)\n\n        # 开始 DBSCAN 聚类\n        crt_cluster_label = -1\n        for i in range(self.n_samples):\n            if self.pred_label[i] == UNCLASSIFIED:\n                query_result = self.query_eps_region_data(i)\n                if len(query_result) < self.min_pts:\n                    self.pred_label[i] = NOISE\n                else:\n                    crt_cluster_label += 1\n                    self.core_points.add(i)\n                    for j in query_result:\n                        self.pred_label[j] = crt_cluster_label\n                    query_result.discard(i)\n                    self.generate_cluster_by_seed(query_result, crt_cluster_label)\n        print(\"time used: %.4f seconds\" % (time.time() - start_time))\n\n    def construct_index(self):\n        self.data_index = rtree.index.Index()\n        for i in range(self.n_samples):\n            data = self.data_set[i]\n            self.data_index.insert(i, (data[0], data[1], data[0], data[1]))\n\n    @staticmethod\n    def distance(data1, data2, metric='euclidean'):\n        if metric == 'euclidean':\n            dist = np.sqrt(np.dot(data1 - data2, data1 - data2))\n        elif metric == 'manhattan':\n            dist = np.sum(np.abs(data1 - data2))\n        elif metric == 'chebyshev':\n            dist = np.max(np.abs(data1 - data2))\n        else:\n            raise Exception(\"invalid or unsupported distance metric!\")\n        return dist\n\n    def cal_dist_matrix(self):\n        self.dist_matrix = np.zeros((self.n_samples, self.n_samples))\n        for i in range(self.n_samples):\n            for j in range(i + 1, self.n_samples):\n                dist = self.distance(self.data_set[i], self.data_set[j], self.metric)\n                self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist\n\n    def query_eps_region_data(self, i):\n        if self.data_index:\n            data = self.data_set[i]\n            query_result = set()\n            buff_polygon = Point(data[0], data[1]).buffer(self.eps)\n            xmin, ymin, xmax, ymax = buff_polygon.bounds\n            for idx in self.data_index.intersection((xmin, ymin, xmax, ymax)):\n                if Point(self.data_set[idx][0], self.data_set[idx][1]).intersects(buff_polygon):\n                    query_result.add(idx)\n        else:\n            query_result = set(item[0] for item in np.argwhere(self.dist_matrix[i] <= self.eps))\n        return query_result\n\n    def generate_cluster_by_seed(self, seed_set, cluster_label):\n        while seed_set:\n            crt_data_index = seed_set.pop()\n            crt_query_result = self.query_eps_region_data(crt_data_index)\n            if len(crt_query_result) >= self.min_pts:\n                self.core_points.add(crt_data_index)\n                for i in crt_query_result:\n                    if self.pred_label[i] == UNCLASSIFIED:\n                        seed_set.add(i)\n                    self.pred_label[i] = cluster_label\n```\n\n我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 `sklearn` 中实现的 `DBSCAN` 类对同一份数据集进行了 DBSCAN 聚类，代码如下：\n```python\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN as DBSCAN_SKLEARN\n\ndef plot_clustering(X, y, core_pts_idx=None, title=None):\n    if core_pts_idx is not None:\n        core_pts_idx = np.array(list(core_pts_idx), dtype=int)\n        core_sample_mask = np.zeros_like(y, dtype=bool)\n        core_sample_mask[core_pts_idx] = True\n\n        unique_labels = set(y)\n        colors = [plt.cm.Spectral(item) for item in np.linspace(0, 1, len(unique_labels))]\n\n        for k, col in zip(unique_labels, colors):\n            if k == -1:\n                col = [0, 0, 0, 1]\n            class_member_mask = (y == k)\n            xy = X[class_member_mask & core_sample_mask]\n            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                    markeredgecolor='k', markersize=12, alpha=0.6)\n            xy = X[class_member_mask & ~core_sample_mask]\n            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                    markeredgecolor='k', markersize=6, alpha=0.6)\n    else:\n        plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6)\n    if title is not None:\n        plt.title(title, size=14)\n    plt.axis('on')\n    plt.tight_layout()\n\n# 构造数据集\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# 利用我自己手写的 DBSCAN 算法对数据集进行聚类\ndbscan_diy = DBSCAN(min_pts=20, eps=0.5)\ndbscan_diy.predict(X)\nn_clusters = len(set(dbscan_diy.pred_label)) - (1 if -1 in dbscan_diy.pred_label else 0)\nprint(\"count of clusters generated: %s\" % n_clusters)\nprint(\"propotion of noise data for dbscan_diy: %.4f\" % (np.sum(dbscan_diy.pred_label == -1) / n_samples))\nplt.subplot(1, 2, 1)\nplot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points,\n                title=\"DBSCAN(DIY) Results\")\n\n# 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类\ndbscan_sklearn = DBSCAN_SKLEARN(min_samples=20, eps=0.5)\ndbscan_sklearn.fit(X)\nprint(\"propotion of noise data for dbscan_sklearn: %.4f\" % (np.sum(dbscan_sklearn.labels_ == -1) / n_samples))\nplt.subplot(1, 2, 2)\nplot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_,\n                title=\"DBSCAN(SKLEARN) Results\")\n\nplt.show()\n```\n\n\n运行得到的输出和可视化结果如下所示：\n```\ntime used: 4.2602 seconds\ncount of clusters generated: 3\npropotion of noise data for dbscan_diy: 0.1220\npropotion of noise data for dbscan_sklearn: 0.1220\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_results.png\" width = \"1000\" height = \"500\" alt = \"DBSCAN 的运行结果\" align = center />\n</div>\n\n上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。\n\n我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：\n```python\nfrom sklearn.datasets import make_circles, make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nn_samples = 1500\nnoisy_circles, _ = make_circles(n_samples=n_samples, factor=.5, noise=.05)\nnoisy_circles = StandardScaler().fit_transform(noisy_circles)\nnoisy_moons, _ = make_moons(n_samples=n_samples, noise=.05)\nnoisy_moons = StandardScaler().fit_transform(noisy_moons)\ndbscan = DBSCAN(min_pts=5, eps=0.22)\ndbscan.predict(noisy_circles)\nplt.subplot(1, 2, 1)\nplot_clustering(noisy_circles, dbscan.pred_label, title=\"Concentric Circles Dataset\")\n\ndbscan.predict(noisy_moons)\nplt.subplot(1, 2, 2)\nplot_clustering(noisy_moons, dbscan.pred_label, title=\"Interleaved Moons DataSet\")\n\nplt.show()\n```\n\n\n运行的结果如下图所示：\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_result_for_nonconvex_datasets.png\" width = \"1000\" height = \"500\" alt = \"DBSCAN 在非凸数据集下的运行结果\" align = center />\n</div>\n\n可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。\n\n\n \n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[3]:\thttps://en.wikipedia.org/wiki/R*_tree?oldformat=true\n[4]:\thttp://suraj.lums.edu.pk/~cs536a04/handouts/OPTICS.pdf","source":"_posts/聚类分析（四）：DBSCAN-算法.md","raw":"---\ntitle: 聚类分析（四）：DBSCAN 算法\ndate: 2018-03-22 20:18:35\ntags:\n- 聚类\n- 非监督学习\n- 密度聚类\ncategories:\n- 机器学习算法\nkeywords: 聚类,非监督学习,密度聚类,clustering,machine learning\n\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# DBSCAN 算法\nDBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的**密度聚类算法**。密度聚类算法认为各个 `cluster` 是样本点密度高的区域，而 `cluster` 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。\n\n要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。\n\n## 几个定义\n在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 `cluster`，而噪声点则存在于各 `cluster` 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。\n\n>**定义1：** （样本点的 \\\\( \\\\text {Eps} \\\\)-邻域）假设数据集为 \\\\( \\\\bf X \\\\)，则样本点 \\\\( \\\\bf p \\\\) 的 \\\\( \\\\text {Eps} \\\\)-邻域定义为 \\\\( N\\_{\\\\text {Eps}} ({\\\\bf p}) = \\\\lbrace {\\\\bf q} \\\\in {\\\\bf X} | d({\\\\bf p}, {\\\\bf q}) \\\\le {\\\\text {Eps}} \\rbrace \\\\). \n\n我们再给定一个参数 \\\\( \\\\text {MinPts} \\\\)，并定义核心点须满足的条件为：其 \\\\( \\\\text {Eps} \\\\)-邻域内包含的样本点的数目不小于 \\\\( \\\\text {MinPts} \\\\) ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 `cluster` 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 `cluster` 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 `cluster` 所具有的形式。\n\n>**定义2： **（密度直达）我们称样本点 \\\\( \\\\bf p \\\\) 是由样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 密度直达的，如果它们满足 \\\\( {\\\\bf p} \\\\in N\\_{\\\\text {Eps}} ({\\\\bf q}) \\\\) 且 \\\\( |N\\_{\\\\text {Eps}}({\\\\bf q})| \\\\ge \\\\text {MinPts} \\\\) （即样本点 \\\\( \\\\bf q \\\\) 是核心点）. \n\n很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 \\\\( \\\\text {Eps} = 5 \\\\)，可以看到，样本点 \\\\( \\\\bf q \\\\) 为核心点，样本点 \\\\( \\\\bf p \\\\) 不是核心点，且 \\\\( \\\\bf p \\\\) 在 \\\\( \\\\bf q \\\\) 的 \\\\( \\\\text {Eps} \\\\)-邻域内，因而 \\\\( \\\\bf p \\\\) 可由 \\\\( \\\\bf q \\\\) 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_definition_in_dbscan.png\" width = \"900\" height = \"600\" alt = \"DBSCAN 中的相关概念的图示\" align = center />\n</div>\n\n>**定义3：**（密度可达）我们称样本点 \\\\( \\\\bf p \\\\) 是由样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 密度可达的，如果存在一系列的样本点 \\\\( {\\\\bf p}\\_{1}, ..., {\\\\bf p}\\_n \\\\)（其中 \\\\( {\\\\bf p}\\_1 = {\\\\bf q}, {\\\\bf p}\\_n = {\\\\bf p} \\\\)）使得对于 \\\\( i = 1, ..., n-1 \\\\)，样本点 \\\\( {\\\\bf p}\\_{i + 1} \\\\) 可由样本点 \\\\( {\\\\bf p}\\_{i} \\\\) 密度可达.\n\n我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 `cluster` 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。\n\n>**定义4：**（密度相连）我们称样本点 \\\\( \\\\bf p \\\\) 与样本点 \\\\( \\\\bf q \\\\) 对于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 是密度相连的，如果存在一个样本点 \\\\( {\\\\bf o} \\\\)，使得 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\) 均由样本点 \\\\( \\\\bf o \\\\) 密度可达。\n\n密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 `cluster` ，使得每一个 `cluster` 中的任意两个样本点都是密度相连的，且每一个 `cluster` 在密度可达的意义上都是最大的。`cluster` 的定义如下：\n\n>**定义5：**（`cluster`）假设数据集为 \\\\( \\\\bf X \\\\)，给定参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\)，则某个 `cluster` \\\\( C \\\\) 是数据集  \\\\( \\\\bf X \\\\) 的一个非空子集，且满足如下条件：\n\t\t>1）对于任意的样本点 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\)，如果 \\\\( {\\\\bf p} \\\\in C \\\\) 且 \\\\( \\\\bf q \\\\) 可由 \\\\( \\\\bf p \\\\) 密度可达，则 \\\\( {\\\\bf q} \\\\in C \\\\) .（最大性）\n\t\t>2）对于 \\\\( C \\\\) 中的任意样本点 \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\)， \\\\( \\\\bf p \\\\) 和 \\\\( \\\\bf q \\\\) 关于参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 是密度相连的.（连接性）\n\n这样我们就定义出了 DBSCAN 算法最终产生的 `cluster` 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 `cluster` 的样本点。根据以上定义，由于一个 `cluster` 中的任意两个样本点都是密度相连的，每一个 `cluster` 至少包含 \\\\( \\\\text {MinPts} \\\\) 个样本点。 \n\n## 算法描述\nDBSCAN 算法就是为了寻找以上定义 5 中定义的 `cluster`，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 `cluster`，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 `cluster`，…，依此过程，直至没有未被聚类的核心点为止。依照 `cluster` 的“最大性”和“连接性”，在给定参数 \\\\( \\\\lbrace \\\\text {Eps}, \\\\text {MinPts} \\\\rbrace \\\\) 的情况下，最终产生的 `cluster` 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 `cluster` 的边缘的情况除外，这种情况下，这些临界位置的样本点的 `cluster` 归属与种子的选取顺序有关）。\n\n## 合理地选取参数\nDBSCAN 的聚类结果和效果取决于参数 \\\\( \\\\text {Eps} \\\\) 和 \\\\( \\\\text {MinPts} \\\\) 以及距离衡量方法的选取。\n\n由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 [*聚类分析（一）：层次聚类算法*][2]。\n\n然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 `cluster` 所对应的 \\\\( \\\\text {Eps} \\\\) 和 \\\\( \\\\text {MinPts} \\\\)。这里我们给出一种启发式的参数选取方法。假设 \\\\( d \\\\) 是某个样本点 \\\\( \\\\bf p \\\\) 距离它的第 \\\\( k \\\\) 近邻的距离，则一般情况下 \\\\( \\\\bf p \\\\) 的 \\\\( d \\\\)-邻域内正好包含 \\\\( k + 1 \\\\) 个样本点。我们可以推断，在一个合理的 `cluster` 内，改变 \\\\( k \\\\) 的值不应该导致 \\\\( d \\\\) 值有较大的变化，除非 \\\\( \\\\bf p \\\\) 的第 \\\\( k \\\\) 近邻们（\\\\( k = 1, 2, 3,… \\\\) ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 `cluster`。\n\n因而我们一般将 \\\\( k \\\\) 的值固定下来，一个合理的选择是令 \\\\( k = 3 \\\\) 或 \\\\( k = 4 \\\\)，那么 \\\\( \\\\text {MinPts} \\\\) 的值也确定了（为 \\\\( k + 1 \\\\)）；然后再来看每个样本点的 \\\\( \\\\text {k-dist} \\\\) 距离（即该样本点距离它的第 \\\\( k \\\\) 近邻的距离）的分布情况，我们把每个样本点的 \\\\( \\\\text {k-dist} \\\\) 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 \\\\( \\\\text {Eps} \\\\)，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 \\\\( \\\\text {k-dist} \\\\) 距离作为 \\\\( \\\\text {Eps} \\\\)，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 \\\\( \\\\text {k-dist} \\\\) 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 \\\\( \\\\text {k-dist} \\\\) 距离作为 \\\\( \\\\text {k-dist} \\\\) ，如下图中的图（b）所示。\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_parameter_selection_for_dbscan.png\" width = \"900\" height = \"500\" alt = \"DBSCAN 中数据集的 k-dist 图\" align = center />\n</div>\n\n## 算法的复杂度及其优缺点\n### 算法复杂度\nDBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 \\\\( \\\\text {Eps} \\\\)-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 \\\\( O(N) \\\\)，其中 \\\\( N \\\\) 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 [R\\* tree][3] 建立索引时，查询邻域的平均时间复杂度为 \\\\( O(\\\\log N) \\\\)。因而，DBSCAN 算法的平均时间复杂度为 \\\\( O(N\\\\log N) \\\\)；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 \\\\( O(N) \\\\)。\n\n### 优缺点\nDBSCAN 算法有很多优点，总结如下：\n- DBSCAN 不需要事先指定最终需要生成的 `cluster` 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 `cluster` 的数目，实际上我们对数据集是有相当多的了解的。\n- DBSCAN 可以找到具有任意形状的 `cluster`，如非凸的 `cluster`，这基于其对 `cluster` 的定义（`cluster` 是由密度低的区域所隔开的密度高的区域）。\n- DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。\n- DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 `cluster` 的边缘的情况，这种情况下，这些临界位置的样本点的 `cluster` 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。\n- DBSCAN 的运行速度快，当采用索引时，其复杂度仅为  \\\\( O(N\\\\log N) \\\\)。\n\n当然，它也有一个主要缺点，即对于具有密度相差较大的 `cluster` 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 \\\\( \\\\text {MinPts} \\\\) 和 \\\\( \\\\text {Eps} \\\\) 是参照数据集中最稀疏的 `cluster` 所选取的，那么很有可能最终所有的样本最终都被归为一个 `cluster`，因为可能数据集中的 `cluster` 之间的区域的密度和最稀疏的 `cluster` 的密度相当；如果选取的参数 \\\\( \\\\text {MinPts} \\\\) 和 \\\\( \\\\text {Eps} \\\\) 倾向于聚出密度比较大的 `cluster`，那么极有可能，比较稀疏的这些 `cluster` 都被归为噪声。[ OPTICS][4] 算法一般被用来解决这一问题。\n\n---- \n# 实现 DBSCAN 聚类\n现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 `DBSCAN`，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 `min_pts` 和 `eps` 以及距离衡量方法（默认为 `euclidean`），对数据集进行聚类时，我们对构造出来的实例调用方法 `predict`。`predict` 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 `rtree` 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：\n```python\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nimport rtree\n\nUNCLASSIFIED = -2\nNOISE = -1\n\n\nclass DBSCAN():\n    def __init__(self, min_pts, eps, metric='euclidean', index_flag=True):\n        self.min_pts = min_pts\n        self.eps = eps\n        self.metric = metric\n        self.index_flag = index_flag\n        self.data_set = None\n        self.pred_label = None\n        self.core_points = set()\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        self.n_samples, self.n_features = self.data_set.shape\n\n        self.data_index = None\n        self.dist_matrix = None\n\n        start_time = time.time()\n        if self.n_features == 2 and self.metric == 'euclidean' \\\n            and self.index_flag:\n            # 此种情形下对数据集建立空间索引\n            self.construct_index()\n        else:\n            # 其它情形下对数据集计算距离矩阵\n            self.cal_dist_matrix()\n\n        self.pred_label = np.array([UNCLASSIFIED] * self.n_samples)\n\n        # 开始 DBSCAN 聚类\n        crt_cluster_label = -1\n        for i in range(self.n_samples):\n            if self.pred_label[i] == UNCLASSIFIED:\n                query_result = self.query_eps_region_data(i)\n                if len(query_result) < self.min_pts:\n                    self.pred_label[i] = NOISE\n                else:\n                    crt_cluster_label += 1\n                    self.core_points.add(i)\n                    for j in query_result:\n                        self.pred_label[j] = crt_cluster_label\n                    query_result.discard(i)\n                    self.generate_cluster_by_seed(query_result, crt_cluster_label)\n        print(\"time used: %.4f seconds\" % (time.time() - start_time))\n\n    def construct_index(self):\n        self.data_index = rtree.index.Index()\n        for i in range(self.n_samples):\n            data = self.data_set[i]\n            self.data_index.insert(i, (data[0], data[1], data[0], data[1]))\n\n    @staticmethod\n    def distance(data1, data2, metric='euclidean'):\n        if metric == 'euclidean':\n            dist = np.sqrt(np.dot(data1 - data2, data1 - data2))\n        elif metric == 'manhattan':\n            dist = np.sum(np.abs(data1 - data2))\n        elif metric == 'chebyshev':\n            dist = np.max(np.abs(data1 - data2))\n        else:\n            raise Exception(\"invalid or unsupported distance metric!\")\n        return dist\n\n    def cal_dist_matrix(self):\n        self.dist_matrix = np.zeros((self.n_samples, self.n_samples))\n        for i in range(self.n_samples):\n            for j in range(i + 1, self.n_samples):\n                dist = self.distance(self.data_set[i], self.data_set[j], self.metric)\n                self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist\n\n    def query_eps_region_data(self, i):\n        if self.data_index:\n            data = self.data_set[i]\n            query_result = set()\n            buff_polygon = Point(data[0], data[1]).buffer(self.eps)\n            xmin, ymin, xmax, ymax = buff_polygon.bounds\n            for idx in self.data_index.intersection((xmin, ymin, xmax, ymax)):\n                if Point(self.data_set[idx][0], self.data_set[idx][1]).intersects(buff_polygon):\n                    query_result.add(idx)\n        else:\n            query_result = set(item[0] for item in np.argwhere(self.dist_matrix[i] <= self.eps))\n        return query_result\n\n    def generate_cluster_by_seed(self, seed_set, cluster_label):\n        while seed_set:\n            crt_data_index = seed_set.pop()\n            crt_query_result = self.query_eps_region_data(crt_data_index)\n            if len(crt_query_result) >= self.min_pts:\n                self.core_points.add(crt_data_index)\n                for i in crt_query_result:\n                    if self.pred_label[i] == UNCLASSIFIED:\n                        seed_set.add(i)\n                    self.pred_label[i] = cluster_label\n```\n\n我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 `sklearn` 中实现的 `DBSCAN` 类对同一份数据集进行了 DBSCAN 聚类，代码如下：\n```python\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN as DBSCAN_SKLEARN\n\ndef plot_clustering(X, y, core_pts_idx=None, title=None):\n    if core_pts_idx is not None:\n        core_pts_idx = np.array(list(core_pts_idx), dtype=int)\n        core_sample_mask = np.zeros_like(y, dtype=bool)\n        core_sample_mask[core_pts_idx] = True\n\n        unique_labels = set(y)\n        colors = [plt.cm.Spectral(item) for item in np.linspace(0, 1, len(unique_labels))]\n\n        for k, col in zip(unique_labels, colors):\n            if k == -1:\n                col = [0, 0, 0, 1]\n            class_member_mask = (y == k)\n            xy = X[class_member_mask & core_sample_mask]\n            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                    markeredgecolor='k', markersize=12, alpha=0.6)\n            xy = X[class_member_mask & ~core_sample_mask]\n            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                    markeredgecolor='k', markersize=6, alpha=0.6)\n    else:\n        plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6)\n    if title is not None:\n        plt.title(title, size=14)\n    plt.axis('on')\n    plt.tight_layout()\n\n# 构造数据集\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# 利用我自己手写的 DBSCAN 算法对数据集进行聚类\ndbscan_diy = DBSCAN(min_pts=20, eps=0.5)\ndbscan_diy.predict(X)\nn_clusters = len(set(dbscan_diy.pred_label)) - (1 if -1 in dbscan_diy.pred_label else 0)\nprint(\"count of clusters generated: %s\" % n_clusters)\nprint(\"propotion of noise data for dbscan_diy: %.4f\" % (np.sum(dbscan_diy.pred_label == -1) / n_samples))\nplt.subplot(1, 2, 1)\nplot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points,\n                title=\"DBSCAN(DIY) Results\")\n\n# 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类\ndbscan_sklearn = DBSCAN_SKLEARN(min_samples=20, eps=0.5)\ndbscan_sklearn.fit(X)\nprint(\"propotion of noise data for dbscan_sklearn: %.4f\" % (np.sum(dbscan_sklearn.labels_ == -1) / n_samples))\nplt.subplot(1, 2, 2)\nplot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_,\n                title=\"DBSCAN(SKLEARN) Results\")\n\nplt.show()\n```\n\n\n运行得到的输出和可视化结果如下所示：\n```\ntime used: 4.2602 seconds\ncount of clusters generated: 3\npropotion of noise data for dbscan_diy: 0.1220\npropotion of noise data for dbscan_sklearn: 0.1220\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_results.png\" width = \"1000\" height = \"500\" alt = \"DBSCAN 的运行结果\" align = center />\n</div>\n\n上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。\n\n我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：\n```python\nfrom sklearn.datasets import make_circles, make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nn_samples = 1500\nnoisy_circles, _ = make_circles(n_samples=n_samples, factor=.5, noise=.05)\nnoisy_circles = StandardScaler().fit_transform(noisy_circles)\nnoisy_moons, _ = make_moons(n_samples=n_samples, noise=.05)\nnoisy_moons = StandardScaler().fit_transform(noisy_moons)\ndbscan = DBSCAN(min_pts=5, eps=0.22)\ndbscan.predict(noisy_circles)\nplt.subplot(1, 2, 1)\nplot_clustering(noisy_circles, dbscan.pred_label, title=\"Concentric Circles Dataset\")\n\ndbscan.predict(noisy_moons)\nplt.subplot(1, 2, 2)\nplot_clustering(noisy_moons, dbscan.pred_label, title=\"Interleaved Moons DataSet\")\n\nplt.show()\n```\n\n\n运行的结果如下图所示：\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_result_for_nonconvex_datasets.png\" width = \"1000\" height = \"500\" alt = \"DBSCAN 在非凸数据集下的运行结果\" align = center />\n</div>\n\n可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。\n\n\n \n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\n[3]:\thttps://en.wikipedia.org/wiki/R*_tree?oldformat=true\n[4]:\thttp://suraj.lums.edu.pk/~cs536a04/handouts/OPTICS.pdf","slug":"聚类分析（四）：DBSCAN-算法","published":1,"updated":"2018-03-27T00:44:24.377Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfclp3tw0006hws6orb33bmd","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"DBSCAN-算法\"><a href=\"#DBSCAN-算法\" class=\"headerlink\" title=\"DBSCAN 算法\"></a>DBSCAN 算法</h1><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的<strong>密度聚类算法</strong>。密度聚类算法认为各个 <code>cluster</code> 是样本点密度高的区域，而 <code>cluster</code> 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。</p>\n<p>要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。</p>\n<h2 id=\"几个定义\"><a href=\"#几个定义\" class=\"headerlink\" title=\"几个定义\"></a>几个定义</h2><p>在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 <code>cluster</code>，而噪声点则存在于各 <code>cluster</code> 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。</p>\n<blockquote>\n<p><strong>定义1：</strong> （样本点的 \\( \\text {Eps} \\)-邻域）假设数据集为 \\( \\bf X \\)，则样本点 \\( \\bf p \\) 的 \\( \\text {Eps} \\)-邻域定义为 \\( N_{\\text {Eps}} ({\\bf p}) = \\lbrace {\\bf q} \\in {\\bf X} | d({\\bf p}, {\\bf q}) \\le {\\text {Eps}} \\rbrace \\). </p>\n</blockquote>\n<p>我们再给定一个参数 \\( \\text {MinPts} \\)，并定义核心点须满足的条件为：其 \\( \\text {Eps} \\)-邻域内包含的样本点的数目不小于 \\( \\text {MinPts} \\) ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 <code>cluster</code> 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 <code>cluster</code> 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 <code>cluster</code> 所具有的形式。</p>\n<blockquote>\n<p><strong>定义2： </strong>（密度直达）我们称样本点 \\( \\bf p \\) 是由样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 密度直达的，如果它们满足 \\( {\\bf p} \\in N_{\\text {Eps}} ({\\bf q}) \\) 且 \\( |N_{\\text {Eps}}({\\bf q})| \\ge \\text {MinPts} \\) （即样本点 \\( \\bf q \\) 是核心点）. </p>\n</blockquote>\n<p>很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 \\( \\text {Eps} = 5 \\)，可以看到，样本点 \\( \\bf q \\) 为核心点，样本点 \\( \\bf p \\) 不是核心点，且 \\( \\bf p \\) 在 \\( \\bf q \\) 的 \\( \\text {Eps} \\)-邻域内，因而 \\( \\bf p \\) 可由 \\( \\bf q \\) 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_definition_in_dbscan.png\" width=\"900\" height=\"600\" alt=\"DBSCAN 中的相关概念的图示\" align=\"center\"><br></div>\n\n<blockquote>\n<p><strong>定义3：</strong>（密度可达）我们称样本点 \\( \\bf p \\) 是由样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 密度可达的，如果存在一系列的样本点 \\( {\\bf p}_{1}, …, {\\bf p}_n \\)（其中 \\( {\\bf p}_1 = {\\bf q}, {\\bf p}_n = {\\bf p} \\)）使得对于 \\( i = 1, …, n-1 \\)，样本点 \\( {\\bf p}_{i + 1} \\) 可由样本点 \\( {\\bf p}_{i} \\) 密度可达.</p>\n</blockquote>\n<p>我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 <code>cluster</code> 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。</p>\n<blockquote>\n<p><strong>定义4：</strong>（密度相连）我们称样本点 \\( \\bf p \\) 与样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 是密度相连的，如果存在一个样本点 \\( {\\bf o} \\)，使得 \\( \\bf p \\) 和 \\( \\bf q \\) 均由样本点 \\( \\bf o \\) 密度可达。</p>\n</blockquote>\n<p>密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 <code>cluster</code> ，使得每一个 <code>cluster</code> 中的任意两个样本点都是密度相连的，且每一个 <code>cluster</code> 在密度可达的意义上都是最大的。<code>cluster</code> 的定义如下：</p>\n<blockquote>\n<p><strong>定义5：</strong>（<code>cluster</code>）假设数据集为 \\( \\bf X \\)，给定参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\)，则某个 <code>cluster</code> \\( C \\) 是数据集  \\( \\bf X \\) 的一个非空子集，且满足如下条件：<br>1）对于任意的样本点 \\( \\bf p \\) 和 \\( \\bf q \\)，如果 \\( {\\bf p} \\in C \\) 且 \\( \\bf q \\) 可由 \\( \\bf p \\) 密度可达，则 \\( {\\bf q} \\in C \\) .（最大性）<br>2）对于 \\( C \\) 中的任意样本点 \\( \\bf p \\) 和 \\( \\bf q \\)， \\( \\bf p \\) 和 \\( \\bf q \\) 关于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 是密度相连的.（连接性）</p>\n</blockquote>\n<p>这样我们就定义出了 DBSCAN 算法最终产生的 <code>cluster</code> 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 <code>cluster</code> 的样本点。根据以上定义，由于一个 <code>cluster</code> 中的任意两个样本点都是密度相连的，每一个 <code>cluster</code> 至少包含 \\( \\text {MinPts} \\) 个样本点。 </p>\n<h2 id=\"算法描述\"><a href=\"#算法描述\" class=\"headerlink\" title=\"算法描述\"></a>算法描述</h2><p>DBSCAN 算法就是为了寻找以上定义 5 中定义的 <code>cluster</code>，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 <code>cluster</code>，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 <code>cluster</code>，…，依此过程，直至没有未被聚类的核心点为止。依照 <code>cluster</code> 的“最大性”和“连接性”，在给定参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 的情况下，最终产生的 <code>cluster</code> 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 <code>cluster</code> 的边缘的情况除外，这种情况下，这些临界位置的样本点的 <code>cluster</code> 归属与种子的选取顺序有关）。</p>\n<h2 id=\"合理地选取参数\"><a href=\"#合理地选取参数\" class=\"headerlink\" title=\"合理地选取参数\"></a>合理地选取参数</h2><p>DBSCAN 的聚类结果和效果取决于参数 \\( \\text {Eps} \\) 和 \\( \\text {MinPts} \\) 以及距离衡量方法的选取。</p>\n<p>由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\"><em>聚类分析（一）：层次聚类算法</em></a>。</p>\n<p>然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 <code>cluster</code> 所对应的 \\( \\text {Eps} \\) 和 \\( \\text {MinPts} \\)。这里我们给出一种启发式的参数选取方法。假设 \\( d \\) 是某个样本点 \\( \\bf p \\) 距离它的第 \\( k \\) 近邻的距离，则一般情况下 \\( \\bf p \\) 的 \\( d \\)-邻域内正好包含 \\( k + 1 \\) 个样本点。我们可以推断，在一个合理的 <code>cluster</code> 内，改变 \\( k \\) 的值不应该导致 \\( d \\) 值有较大的变化，除非 \\( \\bf p \\) 的第 \\( k \\) 近邻们（\\( k = 1, 2, 3,… \\) ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 <code>cluster</code>。</p>\n<p>因而我们一般将 \\( k \\) 的值固定下来，一个合理的选择是令 \\( k = 3 \\) 或 \\( k = 4 \\)，那么 \\( \\text {MinPts} \\) 的值也确定了（为 \\( k + 1 \\)）；然后再来看每个样本点的 \\( \\text {k-dist} \\) 距离（即该样本点距离它的第 \\( k \\) 近邻的距离）的分布情况，我们把每个样本点的 \\( \\text {k-dist} \\) 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 \\( \\text {Eps} \\)，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 \\( \\text {k-dist} \\) 距离作为 \\( \\text {Eps} \\)，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 \\( \\text {k-dist} \\) 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 \\( \\text {k-dist} \\) 距离作为 \\( \\text {k-dist} \\) ，如下图中的图（b）所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_parameter_selection_for_dbscan.png\" width=\"900\" height=\"500\" alt=\"DBSCAN 中数据集的 k-dist 图\" align=\"center\"><br></div>\n\n<h2 id=\"算法的复杂度及其优缺点\"><a href=\"#算法的复杂度及其优缺点\" class=\"headerlink\" title=\"算法的复杂度及其优缺点\"></a>算法的复杂度及其优缺点</h2><h3 id=\"算法复杂度\"><a href=\"#算法复杂度\" class=\"headerlink\" title=\"算法复杂度\"></a>算法复杂度</h3><p>DBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 \\( \\text {Eps} \\)-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 \\( O(N) \\)，其中 \\( N \\) 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 <a href=\"https://en.wikipedia.org/wiki/R*_tree?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">R* tree</a> 建立索引时，查询邻域的平均时间复杂度为 \\( O(\\log N) \\)。因而，DBSCAN 算法的平均时间复杂度为 \\( O(N\\log N) \\)；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 \\( O(N) \\)。</p>\n<h3 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a>优缺点</h3><p>DBSCAN 算法有很多优点，总结如下：</p>\n<ul>\n<li>DBSCAN 不需要事先指定最终需要生成的 <code>cluster</code> 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 <code>cluster</code> 的数目，实际上我们对数据集是有相当多的了解的。</li>\n<li>DBSCAN 可以找到具有任意形状的 <code>cluster</code>，如非凸的 <code>cluster</code>，这基于其对 <code>cluster</code> 的定义（<code>cluster</code> 是由密度低的区域所隔开的密度高的区域）。</li>\n<li>DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。</li>\n<li>DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 <code>cluster</code> 的边缘的情况，这种情况下，这些临界位置的样本点的 <code>cluster</code> 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。</li>\n<li>DBSCAN 的运行速度快，当采用索引时，其复杂度仅为  \\( O(N\\log N) \\)。</li>\n</ul>\n<p>当然，它也有一个主要缺点，即对于具有密度相差较大的 <code>cluster</code> 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 \\( \\text {MinPts} \\) 和 \\( \\text {Eps} \\) 是参照数据集中最稀疏的 <code>cluster</code> 所选取的，那么很有可能最终所有的样本最终都被归为一个 <code>cluster</code>，因为可能数据集中的 <code>cluster</code> 之间的区域的密度和最稀疏的 <code>cluster</code> 的密度相当；如果选取的参数 \\( \\text {MinPts} \\) 和 \\( \\text {Eps} \\) 倾向于聚出密度比较大的 <code>cluster</code>，那么极有可能，比较稀疏的这些 <code>cluster</code> 都被归为噪声。<a href=\"http://suraj.lums.edu.pk/~cs536a04/handouts/OPTICS.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> OPTICS</a> 算法一般被用来解决这一问题。</p>\n<hr>\n<h1 id=\"实现-DBSCAN-聚类\"><a href=\"#实现-DBSCAN-聚类\" class=\"headerlink\" title=\"实现 DBSCAN 聚类\"></a>实现 DBSCAN 聚类</h1><p>现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 <code>DBSCAN</code>，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 <code>min_pts</code> 和 <code>eps</code> 以及距离衡量方法（默认为 <code>euclidean</code>），对数据集进行聚类时，我们对构造出来的实例调用方法 <code>predict</code>。<code>predict</code> 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 <code>rtree</code> 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">from</span> shapely.geometry <span class=\"keyword\">import</span> Point</div><div class=\"line\"><span class=\"keyword\">import</span> rtree</div><div class=\"line\"></div><div class=\"line\">UNCLASSIFIED = <span class=\"number\">-2</span></div><div class=\"line\">NOISE = <span class=\"number\">-1</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DBSCAN</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, min_pts, eps, metric=<span class=\"string\">'euclidean'</span>, index_flag=True)</span>:</span></div><div class=\"line\">        self.min_pts = min_pts</div><div class=\"line\">        self.eps = eps</div><div class=\"line\">        self.metric = metric</div><div class=\"line\">        self.index_flag = index_flag</div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.core_points = set()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        self.n_samples, self.n_features = self.data_set.shape</div><div class=\"line\"></div><div class=\"line\">        self.data_index = <span class=\"keyword\">None</span></div><div class=\"line\">        self.dist_matrix = <span class=\"keyword\">None</span></div><div class=\"line\"></div><div class=\"line\">        start_time = time.time()</div><div class=\"line\">        <span class=\"keyword\">if</span> self.n_features == <span class=\"number\">2</span> <span class=\"keyword\">and</span> self.metric == <span class=\"string\">'euclidean'</span> \\</div><div class=\"line\">            <span class=\"keyword\">and</span> self.index_flag:</div><div class=\"line\">            <span class=\"comment\"># 此种情形下对数据集建立空间索引</span></div><div class=\"line\">            self.construct_index()</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"comment\"># 其它情形下对数据集计算距离矩阵</span></div><div class=\"line\">            self.cal_dist_matrix()</div><div class=\"line\"></div><div class=\"line\">        self.pred_label = np.array([UNCLASSIFIED] * self.n_samples)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始 DBSCAN 聚类</span></div><div class=\"line\">        crt_cluster_label = <span class=\"number\">-1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            <span class=\"keyword\">if</span> self.pred_label[i] == UNCLASSIFIED:</div><div class=\"line\">                query_result = self.query_eps_region_data(i)</div><div class=\"line\">                <span class=\"keyword\">if</span> len(query_result) &lt; self.min_pts:</div><div class=\"line\">                    self.pred_label[i] = NOISE</div><div class=\"line\">                <span class=\"keyword\">else</span>:</div><div class=\"line\">                    crt_cluster_label += <span class=\"number\">1</span></div><div class=\"line\">                    self.core_points.add(i)</div><div class=\"line\">                    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> query_result:</div><div class=\"line\">                        self.pred_label[j] = crt_cluster_label</div><div class=\"line\">                    query_result.discard(i)</div><div class=\"line\">                    self.generate_cluster_by_seed(query_result, crt_cluster_label)</div><div class=\"line\">        print(<span class=\"string\">\"time used: %.4f seconds\"</span> % (time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">construct_index</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.data_index = rtree.index.Index()</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            data = self.data_set[i]</div><div class=\"line\">            self.data_index.insert(i, (data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>], data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>]))</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">    @staticmethod</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distance</span><span class=\"params\">(data1, data2, metric=<span class=\"string\">'euclidean'</span>)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> metric == <span class=\"string\">'euclidean'</span>:</div><div class=\"line\">            dist = np.sqrt(np.dot(data1 - data2, data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">elif</span> metric == <span class=\"string\">'manhattan'</span>:</div><div class=\"line\">            dist = np.sum(np.abs(data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">elif</span> metric == <span class=\"string\">'chebyshev'</span>:</div><div class=\"line\">            dist = np.max(np.abs(data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> Exception(<span class=\"string\">\"invalid or unsupported distance metric!\"</span>)</div><div class=\"line\">        <span class=\"keyword\">return</span> dist</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_dist_matrix</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.dist_matrix = np.zeros((self.n_samples, self.n_samples))</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i + <span class=\"number\">1</span>, self.n_samples):</div><div class=\"line\">                dist = self.distance(self.data_set[i], self.data_set[j], self.metric)</div><div class=\"line\">                self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">query_eps_region_data</span><span class=\"params\">(self, i)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.data_index:</div><div class=\"line\">            data = self.data_set[i]</div><div class=\"line\">            query_result = set()</div><div class=\"line\">            buff_polygon = Point(data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>]).buffer(self.eps)</div><div class=\"line\">            xmin, ymin, xmax, ymax = buff_polygon.bounds</div><div class=\"line\">            <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> self.data_index.intersection((xmin, ymin, xmax, ymax)):</div><div class=\"line\">                <span class=\"keyword\">if</span> Point(self.data_set[idx][<span class=\"number\">0</span>], self.data_set[idx][<span class=\"number\">1</span>]).intersects(buff_polygon):</div><div class=\"line\">                    query_result.add(idx)</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            query_result = set(item[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> np.argwhere(self.dist_matrix[i] &lt;= self.eps))</div><div class=\"line\">        <span class=\"keyword\">return</span> query_result</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_cluster_by_seed</span><span class=\"params\">(self, seed_set, cluster_label)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">while</span> seed_set:</div><div class=\"line\">            crt_data_index = seed_set.pop()</div><div class=\"line\">            crt_query_result = self.query_eps_region_data(crt_data_index)</div><div class=\"line\">            <span class=\"keyword\">if</span> len(crt_query_result) &gt;= self.min_pts:</div><div class=\"line\">                self.core_points.add(crt_data_index)</div><div class=\"line\">                <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> crt_query_result:</div><div class=\"line\">                    <span class=\"keyword\">if</span> self.pred_label[i] == UNCLASSIFIED:</div><div class=\"line\">                        seed_set.add(i)</div><div class=\"line\">                    self.pred_label[i] = cluster_label</div></pre></td></tr></table></figure></p>\n<p>我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 <code>sklearn</code> 中实现的 <code>DBSCAN</code> 类对同一份数据集进行了 DBSCAN 聚类，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> DBSCAN <span class=\"keyword\">as</span> DBSCAN_SKLEARN</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(X, y, core_pts_idx=None, title=None)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">if</span> core_pts_idx <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        core_pts_idx = np.array(list(core_pts_idx), dtype=int)</div><div class=\"line\">        core_sample_mask = np.zeros_like(y, dtype=bool)</div><div class=\"line\">        core_sample_mask[core_pts_idx] = <span class=\"keyword\">True</span></div><div class=\"line\"></div><div class=\"line\">        unique_labels = set(y)</div><div class=\"line\">        colors = [plt.cm.Spectral(item) <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> np.linspace(<span class=\"number\">0</span>, <span class=\"number\">1</span>, len(unique_labels))]</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> k, col <span class=\"keyword\">in</span> zip(unique_labels, colors):</div><div class=\"line\">            <span class=\"keyword\">if</span> k == <span class=\"number\">-1</span>:</div><div class=\"line\">                col = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</div><div class=\"line\">            class_member_mask = (y == k)</div><div class=\"line\">            xy = X[class_member_mask &amp; core_sample_mask]</div><div class=\"line\">            plt.plot(xy[:, <span class=\"number\">0</span>], xy[:, <span class=\"number\">1</span>], <span class=\"string\">'o'</span>, markerfacecolor=tuple(col),</div><div class=\"line\">                    markeredgecolor=<span class=\"string\">'k'</span>, markersize=<span class=\"number\">12</span>, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">            xy = X[class_member_mask &amp; ~core_sample_mask]</div><div class=\"line\">            plt.plot(xy[:, <span class=\"number\">0</span>], xy[:, <span class=\"number\">1</span>], <span class=\"string\">'o'</span>, markerfacecolor=tuple(col),</div><div class=\"line\">                    markeredgecolor=<span class=\"string\">'k'</span>, markersize=<span class=\"number\">6</span>, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">    <span class=\"keyword\">else</span>:</div><div class=\"line\">        plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">    <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">    plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">    plt.tight_layout()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 构造数据集</span></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 利用我自己手写的 DBSCAN 算法对数据集进行聚类</span></div><div class=\"line\">dbscan_diy = DBSCAN(min_pts=<span class=\"number\">20</span>, eps=<span class=\"number\">0.5</span>)</div><div class=\"line\">dbscan_diy.predict(X)</div><div class=\"line\">n_clusters = len(set(dbscan_diy.pred_label)) - (<span class=\"number\">1</span> <span class=\"keyword\">if</span> <span class=\"number\">-1</span> <span class=\"keyword\">in</span> dbscan_diy.pred_label <span class=\"keyword\">else</span> <span class=\"number\">0</span>)</div><div class=\"line\">print(<span class=\"string\">\"count of clusters generated: %s\"</span> % n_clusters)</div><div class=\"line\">print(<span class=\"string\">\"propotion of noise data for dbscan_diy: %.4f\"</span> % (np.sum(dbscan_diy.pred_label == <span class=\"number\">-1</span>) / n_samples))</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">plot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points,</div><div class=\"line\">                title=<span class=\"string\">\"DBSCAN(DIY) Results\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类</span></div><div class=\"line\">dbscan_sklearn = DBSCAN_SKLEARN(min_samples=<span class=\"number\">20</span>, eps=<span class=\"number\">0.5</span>)</div><div class=\"line\">dbscan_sklearn.fit(X)</div><div class=\"line\">print(<span class=\"string\">\"propotion of noise data for dbscan_sklearn: %.4f\"</span> % (np.sum(dbscan_sklearn.labels_ == <span class=\"number\">-1</span>) / n_samples))</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">plot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_,</div><div class=\"line\">                title=<span class=\"string\">\"DBSCAN(SKLEARN) Results\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行得到的输出和可视化结果如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">time used: 4.2602 seconds</div><div class=\"line\">count of clusters generated: 3</div><div class=\"line\">propotion of noise data for dbscan_diy: 0.1220</div><div class=\"line\">propotion of noise data for dbscan_sklearn: 0.1220</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_results.png\" width=\"1000\" height=\"500\" alt=\"DBSCAN 的运行结果\" align=\"center\"><br></div>\n\n<p>上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。</p>\n<p>我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_circles, make_moons</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">noisy_circles, _ = make_circles(n_samples=n_samples, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.05</span>)</div><div class=\"line\">noisy_circles = StandardScaler().fit_transform(noisy_circles)</div><div class=\"line\">noisy_moons, _ = make_moons(n_samples=n_samples, noise=<span class=\"number\">.05</span>)</div><div class=\"line\">noisy_moons = StandardScaler().fit_transform(noisy_moons)</div><div class=\"line\">dbscan = DBSCAN(min_pts=<span class=\"number\">5</span>, eps=<span class=\"number\">0.22</span>)</div><div class=\"line\">dbscan.predict(noisy_circles)</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">plot_clustering(noisy_circles, dbscan.pred_label, title=<span class=\"string\">\"Concentric Circles Dataset\"</span>)</div><div class=\"line\"></div><div class=\"line\">dbscan.predict(noisy_moons)</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">plot_clustering(noisy_moons, dbscan.pred_label, title=<span class=\"string\">\"Interleaved Moons DataSet\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行的结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_result_for_nonconvex_datasets.png\" width=\"1000\" height=\"500\" alt=\"DBSCAN 在非凸数据集下的运行结果\" align=\"center\"><br></div>\n\n<p>可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"DBSCAN-算法\"><a href=\"#DBSCAN-算法\" class=\"headerlink\" title=\"DBSCAN 算法\"></a>DBSCAN 算法</h1><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的<strong>密度聚类算法</strong>。密度聚类算法认为各个 <code>cluster</code> 是样本点密度高的区域，而 <code>cluster</code> 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。</p>\n<p>要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。</p>\n<h2 id=\"几个定义\"><a href=\"#几个定义\" class=\"headerlink\" title=\"几个定义\"></a>几个定义</h2><p>在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 <code>cluster</code>，而噪声点则存在于各 <code>cluster</code> 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。</p>\n<blockquote>\n<p><strong>定义1：</strong> （样本点的 \\( \\text {Eps} \\)-邻域）假设数据集为 \\( \\bf X \\)，则样本点 \\( \\bf p \\) 的 \\( \\text {Eps} \\)-邻域定义为 \\( N_{\\text {Eps}} ({\\bf p}) = \\lbrace {\\bf q} \\in {\\bf X} | d({\\bf p}, {\\bf q}) \\le {\\text {Eps}} \\rbrace \\). </p>\n</blockquote>\n<p>我们再给定一个参数 \\( \\text {MinPts} \\)，并定义核心点须满足的条件为：其 \\( \\text {Eps} \\)-邻域内包含的样本点的数目不小于 \\( \\text {MinPts} \\) ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 <code>cluster</code> 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 <code>cluster</code> 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 <code>cluster</code> 所具有的形式。</p>\n<blockquote>\n<p><strong>定义2： </strong>（密度直达）我们称样本点 \\( \\bf p \\) 是由样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 密度直达的，如果它们满足 \\( {\\bf p} \\in N_{\\text {Eps}} ({\\bf q}) \\) 且 \\( |N_{\\text {Eps}}({\\bf q})| \\ge \\text {MinPts} \\) （即样本点 \\( \\bf q \\) 是核心点）. </p>\n</blockquote>\n<p>很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 \\( \\text {Eps} = 5 \\)，可以看到，样本点 \\( \\bf q \\) 为核心点，样本点 \\( \\bf p \\) 不是核心点，且 \\( \\bf p \\) 在 \\( \\bf q \\) 的 \\( \\text {Eps} \\)-邻域内，因而 \\( \\bf p \\) 可由 \\( \\bf q \\) 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_definition_in_dbscan.png\" width=\"900\" height=\"600\" alt=\"DBSCAN 中的相关概念的图示\" align=\"center\"><br></div>\n\n<blockquote>\n<p><strong>定义3：</strong>（密度可达）我们称样本点 \\( \\bf p \\) 是由样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 密度可达的，如果存在一系列的样本点 \\( {\\bf p}_{1}, …, {\\bf p}_n \\)（其中 \\( {\\bf p}_1 = {\\bf q}, {\\bf p}_n = {\\bf p} \\)）使得对于 \\( i = 1, …, n-1 \\)，样本点 \\( {\\bf p}_{i + 1} \\) 可由样本点 \\( {\\bf p}_{i} \\) 密度可达.</p>\n</blockquote>\n<p>我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 <code>cluster</code> 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。</p>\n<blockquote>\n<p><strong>定义4：</strong>（密度相连）我们称样本点 \\( \\bf p \\) 与样本点 \\( \\bf q \\) 对于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 是密度相连的，如果存在一个样本点 \\( {\\bf o} \\)，使得 \\( \\bf p \\) 和 \\( \\bf q \\) 均由样本点 \\( \\bf o \\) 密度可达。</p>\n</blockquote>\n<p>密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 <code>cluster</code> ，使得每一个 <code>cluster</code> 中的任意两个样本点都是密度相连的，且每一个 <code>cluster</code> 在密度可达的意义上都是最大的。<code>cluster</code> 的定义如下：</p>\n<blockquote>\n<p><strong>定义5：</strong>（<code>cluster</code>）假设数据集为 \\( \\bf X \\)，给定参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\)，则某个 <code>cluster</code> \\( C \\) 是数据集  \\( \\bf X \\) 的一个非空子集，且满足如下条件：<br>1）对于任意的样本点 \\( \\bf p \\) 和 \\( \\bf q \\)，如果 \\( {\\bf p} \\in C \\) 且 \\( \\bf q \\) 可由 \\( \\bf p \\) 密度可达，则 \\( {\\bf q} \\in C \\) .（最大性）<br>2）对于 \\( C \\) 中的任意样本点 \\( \\bf p \\) 和 \\( \\bf q \\)， \\( \\bf p \\) 和 \\( \\bf q \\) 关于参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 是密度相连的.（连接性）</p>\n</blockquote>\n<p>这样我们就定义出了 DBSCAN 算法最终产生的 <code>cluster</code> 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 <code>cluster</code> 的样本点。根据以上定义，由于一个 <code>cluster</code> 中的任意两个样本点都是密度相连的，每一个 <code>cluster</code> 至少包含 \\( \\text {MinPts} \\) 个样本点。 </p>\n<h2 id=\"算法描述\"><a href=\"#算法描述\" class=\"headerlink\" title=\"算法描述\"></a>算法描述</h2><p>DBSCAN 算法就是为了寻找以上定义 5 中定义的 <code>cluster</code>，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 <code>cluster</code>，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 <code>cluster</code>，…，依此过程，直至没有未被聚类的核心点为止。依照 <code>cluster</code> 的“最大性”和“连接性”，在给定参数 \\( \\lbrace \\text {Eps}, \\text {MinPts} \\rbrace \\) 的情况下，最终产生的 <code>cluster</code> 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 <code>cluster</code> 的边缘的情况除外，这种情况下，这些临界位置的样本点的 <code>cluster</code> 归属与种子的选取顺序有关）。</p>\n<h2 id=\"合理地选取参数\"><a href=\"#合理地选取参数\" class=\"headerlink\" title=\"合理地选取参数\"></a>合理地选取参数</h2><p>DBSCAN 的聚类结果和效果取决于参数 \\( \\text {Eps} \\) 和 \\( \\text {MinPts} \\) 以及距离衡量方法的选取。</p>\n<p>由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 <a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/\"><em>聚类分析（一）：层次聚类算法</em></a>。</p>\n<p>然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 <code>cluster</code> 所对应的 \\( \\text {Eps} \\) 和 \\( \\text {MinPts} \\)。这里我们给出一种启发式的参数选取方法。假设 \\( d \\) 是某个样本点 \\( \\bf p \\) 距离它的第 \\( k \\) 近邻的距离，则一般情况下 \\( \\bf p \\) 的 \\( d \\)-邻域内正好包含 \\( k + 1 \\) 个样本点。我们可以推断，在一个合理的 <code>cluster</code> 内，改变 \\( k \\) 的值不应该导致 \\( d \\) 值有较大的变化，除非 \\( \\bf p \\) 的第 \\( k \\) 近邻们（\\( k = 1, 2, 3,… \\) ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 <code>cluster</code>。</p>\n<p>因而我们一般将 \\( k \\) 的值固定下来，一个合理的选择是令 \\( k = 3 \\) 或 \\( k = 4 \\)，那么 \\( \\text {MinPts} \\) 的值也确定了（为 \\( k + 1 \\)）；然后再来看每个样本点的 \\( \\text {k-dist} \\) 距离（即该样本点距离它的第 \\( k \\) 近邻的距离）的分布情况，我们把每个样本点的 \\( \\text {k-dist} \\) 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 \\( \\text {Eps} \\)，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 \\( \\text {k-dist} \\) 距离作为 \\( \\text {Eps} \\)，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 \\( \\text {k-dist} \\) 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 \\( \\text {k-dist} \\) 距离作为 \\( \\text {k-dist} \\) ，如下图中的图（b）所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_parameter_selection_for_dbscan.png\" width=\"900\" height=\"500\" alt=\"DBSCAN 中数据集的 k-dist 图\" align=\"center\"><br></div>\n\n<h2 id=\"算法的复杂度及其优缺点\"><a href=\"#算法的复杂度及其优缺点\" class=\"headerlink\" title=\"算法的复杂度及其优缺点\"></a>算法的复杂度及其优缺点</h2><h3 id=\"算法复杂度\"><a href=\"#算法复杂度\" class=\"headerlink\" title=\"算法复杂度\"></a>算法复杂度</h3><p>DBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 \\( \\text {Eps} \\)-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 \\( O(N) \\)，其中 \\( N \\) 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 <a href=\"https://en.wikipedia.org/wiki/R*_tree?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">R* tree</a> 建立索引时，查询邻域的平均时间复杂度为 \\( O(\\log N) \\)。因而，DBSCAN 算法的平均时间复杂度为 \\( O(N\\log N) \\)；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 \\( O(N) \\)。</p>\n<h3 id=\"优缺点\"><a href=\"#优缺点\" class=\"headerlink\" title=\"优缺点\"></a>优缺点</h3><p>DBSCAN 算法有很多优点，总结如下：</p>\n<ul>\n<li>DBSCAN 不需要事先指定最终需要生成的 <code>cluster</code> 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 <code>cluster</code> 的数目，实际上我们对数据集是有相当多的了解的。</li>\n<li>DBSCAN 可以找到具有任意形状的 <code>cluster</code>，如非凸的 <code>cluster</code>，这基于其对 <code>cluster</code> 的定义（<code>cluster</code> 是由密度低的区域所隔开的密度高的区域）。</li>\n<li>DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。</li>\n<li>DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 <code>cluster</code> 的边缘的情况，这种情况下，这些临界位置的样本点的 <code>cluster</code> 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。</li>\n<li>DBSCAN 的运行速度快，当采用索引时，其复杂度仅为  \\( O(N\\log N) \\)。</li>\n</ul>\n<p>当然，它也有一个主要缺点，即对于具有密度相差较大的 <code>cluster</code> 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 \\( \\text {MinPts} \\) 和 \\( \\text {Eps} \\) 是参照数据集中最稀疏的 <code>cluster</code> 所选取的，那么很有可能最终所有的样本最终都被归为一个 <code>cluster</code>，因为可能数据集中的 <code>cluster</code> 之间的区域的密度和最稀疏的 <code>cluster</code> 的密度相当；如果选取的参数 \\( \\text {MinPts} \\) 和 \\( \\text {Eps} \\) 倾向于聚出密度比较大的 <code>cluster</code>，那么极有可能，比较稀疏的这些 <code>cluster</code> 都被归为噪声。<a href=\"http://suraj.lums.edu.pk/~cs536a04/handouts/OPTICS.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> OPTICS</a> 算法一般被用来解决这一问题。</p>\n<hr>\n<h1 id=\"实现-DBSCAN-聚类\"><a href=\"#实现-DBSCAN-聚类\" class=\"headerlink\" title=\"实现 DBSCAN 聚类\"></a>实现 DBSCAN 聚类</h1><p>现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 <code>DBSCAN</code>，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 <code>min_pts</code> 和 <code>eps</code> 以及距离衡量方法（默认为 <code>euclidean</code>），对数据集进行聚类时，我们对构造出来的实例调用方法 <code>predict</code>。<code>predict</code> 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 <code>rtree</code> 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">from</span> shapely.geometry <span class=\"keyword\">import</span> Point</div><div class=\"line\"><span class=\"keyword\">import</span> rtree</div><div class=\"line\"></div><div class=\"line\">UNCLASSIFIED = <span class=\"number\">-2</span></div><div class=\"line\">NOISE = <span class=\"number\">-1</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DBSCAN</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, min_pts, eps, metric=<span class=\"string\">'euclidean'</span>, index_flag=True)</span>:</span></div><div class=\"line\">        self.min_pts = min_pts</div><div class=\"line\">        self.eps = eps</div><div class=\"line\">        self.metric = metric</div><div class=\"line\">        self.index_flag = index_flag</div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.core_points = set()</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        self.n_samples, self.n_features = self.data_set.shape</div><div class=\"line\"></div><div class=\"line\">        self.data_index = <span class=\"keyword\">None</span></div><div class=\"line\">        self.dist_matrix = <span class=\"keyword\">None</span></div><div class=\"line\"></div><div class=\"line\">        start_time = time.time()</div><div class=\"line\">        <span class=\"keyword\">if</span> self.n_features == <span class=\"number\">2</span> <span class=\"keyword\">and</span> self.metric == <span class=\"string\">'euclidean'</span> \\</div><div class=\"line\">            <span class=\"keyword\">and</span> self.index_flag:</div><div class=\"line\">            <span class=\"comment\"># 此种情形下对数据集建立空间索引</span></div><div class=\"line\">            self.construct_index()</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"comment\"># 其它情形下对数据集计算距离矩阵</span></div><div class=\"line\">            self.cal_dist_matrix()</div><div class=\"line\"></div><div class=\"line\">        self.pred_label = np.array([UNCLASSIFIED] * self.n_samples)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始 DBSCAN 聚类</span></div><div class=\"line\">        crt_cluster_label = <span class=\"number\">-1</span></div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            <span class=\"keyword\">if</span> self.pred_label[i] == UNCLASSIFIED:</div><div class=\"line\">                query_result = self.query_eps_region_data(i)</div><div class=\"line\">                <span class=\"keyword\">if</span> len(query_result) &lt; self.min_pts:</div><div class=\"line\">                    self.pred_label[i] = NOISE</div><div class=\"line\">                <span class=\"keyword\">else</span>:</div><div class=\"line\">                    crt_cluster_label += <span class=\"number\">1</span></div><div class=\"line\">                    self.core_points.add(i)</div><div class=\"line\">                    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> query_result:</div><div class=\"line\">                        self.pred_label[j] = crt_cluster_label</div><div class=\"line\">                    query_result.discard(i)</div><div class=\"line\">                    self.generate_cluster_by_seed(query_result, crt_cluster_label)</div><div class=\"line\">        print(<span class=\"string\">\"time used: %.4f seconds\"</span> % (time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">construct_index</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.data_index = rtree.index.Index()</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            data = self.data_set[i]</div><div class=\"line\">            self.data_index.insert(i, (data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>], data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>]))</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">    @staticmethod</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distance</span><span class=\"params\">(data1, data2, metric=<span class=\"string\">'euclidean'</span>)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> metric == <span class=\"string\">'euclidean'</span>:</div><div class=\"line\">            dist = np.sqrt(np.dot(data1 - data2, data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">elif</span> metric == <span class=\"string\">'manhattan'</span>:</div><div class=\"line\">            dist = np.sum(np.abs(data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">elif</span> metric == <span class=\"string\">'chebyshev'</span>:</div><div class=\"line\">            dist = np.max(np.abs(data1 - data2))</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            <span class=\"keyword\">raise</span> Exception(<span class=\"string\">\"invalid or unsupported distance metric!\"</span>)</div><div class=\"line\">        <span class=\"keyword\">return</span> dist</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_dist_matrix</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">        self.dist_matrix = np.zeros((self.n_samples, self.n_samples))</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(i + <span class=\"number\">1</span>, self.n_samples):</div><div class=\"line\">                dist = self.distance(self.data_set[i], self.data_set[j], self.metric)</div><div class=\"line\">                self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">query_eps_region_data</span><span class=\"params\">(self, i)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> self.data_index:</div><div class=\"line\">            data = self.data_set[i]</div><div class=\"line\">            query_result = set()</div><div class=\"line\">            buff_polygon = Point(data[<span class=\"number\">0</span>], data[<span class=\"number\">1</span>]).buffer(self.eps)</div><div class=\"line\">            xmin, ymin, xmax, ymax = buff_polygon.bounds</div><div class=\"line\">            <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> self.data_index.intersection((xmin, ymin, xmax, ymax)):</div><div class=\"line\">                <span class=\"keyword\">if</span> Point(self.data_set[idx][<span class=\"number\">0</span>], self.data_set[idx][<span class=\"number\">1</span>]).intersects(buff_polygon):</div><div class=\"line\">                    query_result.add(idx)</div><div class=\"line\">        <span class=\"keyword\">else</span>:</div><div class=\"line\">            query_result = set(item[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> np.argwhere(self.dist_matrix[i] &lt;= self.eps))</div><div class=\"line\">        <span class=\"keyword\">return</span> query_result</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_cluster_by_seed</span><span class=\"params\">(self, seed_set, cluster_label)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">while</span> seed_set:</div><div class=\"line\">            crt_data_index = seed_set.pop()</div><div class=\"line\">            crt_query_result = self.query_eps_region_data(crt_data_index)</div><div class=\"line\">            <span class=\"keyword\">if</span> len(crt_query_result) &gt;= self.min_pts:</div><div class=\"line\">                self.core_points.add(crt_data_index)</div><div class=\"line\">                <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> crt_query_result:</div><div class=\"line\">                    <span class=\"keyword\">if</span> self.pred_label[i] == UNCLASSIFIED:</div><div class=\"line\">                        seed_set.add(i)</div><div class=\"line\">                    self.pred_label[i] = cluster_label</div></pre></td></tr></table></figure></p>\n<p>我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 <code>sklearn</code> 中实现的 <code>DBSCAN</code> 类对同一份数据集进行了 DBSCAN 聚类，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> DBSCAN <span class=\"keyword\">as</span> DBSCAN_SKLEARN</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(X, y, core_pts_idx=None, title=None)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">if</span> core_pts_idx <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        core_pts_idx = np.array(list(core_pts_idx), dtype=int)</div><div class=\"line\">        core_sample_mask = np.zeros_like(y, dtype=bool)</div><div class=\"line\">        core_sample_mask[core_pts_idx] = <span class=\"keyword\">True</span></div><div class=\"line\"></div><div class=\"line\">        unique_labels = set(y)</div><div class=\"line\">        colors = [plt.cm.Spectral(item) <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> np.linspace(<span class=\"number\">0</span>, <span class=\"number\">1</span>, len(unique_labels))]</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> k, col <span class=\"keyword\">in</span> zip(unique_labels, colors):</div><div class=\"line\">            <span class=\"keyword\">if</span> k == <span class=\"number\">-1</span>:</div><div class=\"line\">                col = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</div><div class=\"line\">            class_member_mask = (y == k)</div><div class=\"line\">            xy = X[class_member_mask &amp; core_sample_mask]</div><div class=\"line\">            plt.plot(xy[:, <span class=\"number\">0</span>], xy[:, <span class=\"number\">1</span>], <span class=\"string\">'o'</span>, markerfacecolor=tuple(col),</div><div class=\"line\">                    markeredgecolor=<span class=\"string\">'k'</span>, markersize=<span class=\"number\">12</span>, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">            xy = X[class_member_mask &amp; ~core_sample_mask]</div><div class=\"line\">            plt.plot(xy[:, <span class=\"number\">0</span>], xy[:, <span class=\"number\">1</span>], <span class=\"string\">'o'</span>, markerfacecolor=tuple(col),</div><div class=\"line\">                    markeredgecolor=<span class=\"string\">'k'</span>, markersize=<span class=\"number\">6</span>, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">    <span class=\"keyword\">else</span>:</div><div class=\"line\">        plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y, alpha=<span class=\"number\">0.6</span>)</div><div class=\"line\">    <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">    plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">    plt.tight_layout()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 构造数据集</span></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 利用我自己手写的 DBSCAN 算法对数据集进行聚类</span></div><div class=\"line\">dbscan_diy = DBSCAN(min_pts=<span class=\"number\">20</span>, eps=<span class=\"number\">0.5</span>)</div><div class=\"line\">dbscan_diy.predict(X)</div><div class=\"line\">n_clusters = len(set(dbscan_diy.pred_label)) - (<span class=\"number\">1</span> <span class=\"keyword\">if</span> <span class=\"number\">-1</span> <span class=\"keyword\">in</span> dbscan_diy.pred_label <span class=\"keyword\">else</span> <span class=\"number\">0</span>)</div><div class=\"line\">print(<span class=\"string\">\"count of clusters generated: %s\"</span> % n_clusters)</div><div class=\"line\">print(<span class=\"string\">\"propotion of noise data for dbscan_diy: %.4f\"</span> % (np.sum(dbscan_diy.pred_label == <span class=\"number\">-1</span>) / n_samples))</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">plot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points,</div><div class=\"line\">                title=<span class=\"string\">\"DBSCAN(DIY) Results\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类</span></div><div class=\"line\">dbscan_sklearn = DBSCAN_SKLEARN(min_samples=<span class=\"number\">20</span>, eps=<span class=\"number\">0.5</span>)</div><div class=\"line\">dbscan_sklearn.fit(X)</div><div class=\"line\">print(<span class=\"string\">\"propotion of noise data for dbscan_sklearn: %.4f\"</span> % (np.sum(dbscan_sklearn.labels_ == <span class=\"number\">-1</span>) / n_samples))</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">plot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_,</div><div class=\"line\">                title=<span class=\"string\">\"DBSCAN(SKLEARN) Results\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行得到的输出和可视化结果如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">time used: 4.2602 seconds</div><div class=\"line\">count of clusters generated: 3</div><div class=\"line\">propotion of noise data for dbscan_diy: 0.1220</div><div class=\"line\">propotion of noise data for dbscan_sklearn: 0.1220</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_results.png\" width=\"1000\" height=\"500\" alt=\"DBSCAN 的运行结果\" align=\"center\"><br></div>\n\n<p>上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。</p>\n<p>我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_circles, make_moons</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">noisy_circles, _ = make_circles(n_samples=n_samples, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.05</span>)</div><div class=\"line\">noisy_circles = StandardScaler().fit_transform(noisy_circles)</div><div class=\"line\">noisy_moons, _ = make_moons(n_samples=n_samples, noise=<span class=\"number\">.05</span>)</div><div class=\"line\">noisy_moons = StandardScaler().fit_transform(noisy_moons)</div><div class=\"line\">dbscan = DBSCAN(min_pts=<span class=\"number\">5</span>, eps=<span class=\"number\">0.22</span>)</div><div class=\"line\">dbscan.predict(noisy_circles)</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">plot_clustering(noisy_circles, dbscan.pred_label, title=<span class=\"string\">\"Concentric Circles Dataset\"</span>)</div><div class=\"line\"></div><div class=\"line\">dbscan.predict(noisy_moons)</div><div class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">plot_clustering(noisy_moons, dbscan.pred_label, title=<span class=\"string\">\"Interleaved Moons DataSet\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行的结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dbscan_clustering_result_for_nonconvex_datasets.png\" width=\"1000\" height=\"500\" alt=\"DBSCAN 在非凸数据集下的运行结果\" align=\"center\"><br></div>\n\n<p>可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。</p>\n"},{"title":"聚类分析（一）：层次聚类算法","layout":"post","date":"2017-12-01T12:59:23.000Z","keywords":"聚类,层次聚类,非监督学习,clustering,machine learning","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# 聚类算法综述\n聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 `cluster`，使得同一 `cluster` 内的对象在某种意义上比不同的 `cluster` 之间的对象更为相似。\n\n由于 “`cluster`” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类：\n- 基于**连通模型**（connectivity-based）的聚类算法： 即本文将要讲述的**层次聚类**算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 `cluster`。\n- 基于**中心点模型**（centroid-based）的聚类算法： 在此类算法中，每个 `cluster` 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 `k` 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 `cluster` 的中心点的距离的平方和，优化变量为每个 `cluster` 的中心点以及每个对象属于哪个 `cluster`；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，[ k-means 算法 ][2]即是其中的一种。\n- 基于**分布模型**（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 `cluster` 即可，最常被使用的此类算法为[ 高斯混合模型（GMM）聚类][3]。\n- 基于**密度**（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 `cluster`，`cluster` 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。\n\n---- \n# 层次聚类综述\n层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 `clusters`，后面一层生成的 `clusters` 基于前面一层的结果。层次聚类算法一般分为两类：\n- Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 `cluster`，每次按一定的准则将最相近的两个 `cluster` 合并生成一个新的 `cluster`，如此往复，直至最终所有的对象都属于一个 `cluster`。本文主要关注此类算法。\n- Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 `cluster`，每次按一定的准则将某个 `cluster` 划分为多个 `cluster`，如此往复，直至每个对象均是一个 `cluster`。\n\n下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_for_hierarchical_clustering.PNG\" width = \"560\" height = \"400\"  = \"层次聚类图示\" align = center />\n</div>\n\n另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。\n\n## 树形图\n[ 树形图 ][4]（dendrogram）可以用来直观地表示层次聚类的成果。一个有 `5` 个点的树形图如下图所示，其中纵坐标高度表示不同的 `cluster` 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 的距离最近（为 `1`），因此将 \\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 合并为一个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)，所以在树形图中首先将节点 \\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 连接，使其成为一个新的节点  \\\\( (x\\_1, x\\_2) \\\\) 的子节点，并将这个新的节点的高度置为 `1`；之后再在剩下的 `4` 个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)， \\\\( x\\_3 \\\\)， \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 中选取距离最近的两个 `cluster` 合并，\\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 的距离最近（为 `2`），因此将 \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 合并为一个 cluster \\\\( (x\\_4, x\\_5) \\\\)，体现在树形图上，是将节点 \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 连接，使其成为一个新的节点 \\\\( (x\\_4, x\\_5) \\\\) 的子节点，并将此新节点的高度置为 `2`；....依此模式进行树形图的生成，直至最终只剩下一个 `cluster` \\\\( ((x\\_1, x\\_2), x\\_3), (x\\_4, x\\_5)) \\\\)。\n\n可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 `cluster` 之间的距离都不大于 \\\\( h \\\\)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \\\\( h \\\\)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \\\\( h=2.5 \\\\)，即可得到 `3` 个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)， \\\\( x\\_3 \\\\) 和 \\\\( (x\\_4, x\\_5) \\\\)。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/a_dendrogram.png\" width = \"550\" height = \"440\" alt = \"树形图示例\" align = center />\n</div>\n\n## 对象之间的距离衡量\n衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \\\\( d \\\\)  维空间的两个对象 \\\\({\\\\bf x} =[ x\\_1, x\\_2, …, x\\_d]^{T} \\\\) 和 \\\\({\\\\bf y} = [y\\_1, y\\_2, …, y\\_d]^{T}\\\\)，其在不同距离准则下的距离计算方法如下表所示:\n\n| 距离准则  | 距离计算方法 |\n| :-: | :-: |\n| Euclidean 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = [\\\\sum\\_{j=1}^{d} (x\\_j-y\\_j)^{2} ]^{\\\\frac{1}{2}} = [({\\\\bf x} - {\\\\bf y})^{T} ({\\\\bf x} - {\\\\bf y})]^{\\\\frac{1}{2}} \\\\) |\n| Manhattan 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = \\\\sum\\_{j=1}^{d} \\\\mid{x\\_j-y\\_j}\\\\mid \\\\) |\n| Chebyshev 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = \\\\max\\_{1\\\\leq{j}\\\\leq{d}} \\\\mid{x\\_j-y\\_j}\\\\mid \\\\) |\n| Minkowski 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = [\\\\sum\\_{j=1}^{d} (x\\_j-y\\_j)^{p} ]^{\\\\frac{1}{p}}, p\\\\geq{1} \\\\) |\n\nMinkowski 距离就是 \\\\( \\\\rm{L}\\\\it{p}\\\\) 范数（\\\\( p\\\\geq{1} \\\\))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \\\\( p = 1, 2, \\\\infty \\\\) 时的情形。\n\n另一种常用的距离是 Maholanobis 距离，其定义如下：\n$$ d\\_{mah}({\\\\bf x}, {\\\\bf y}) = \\\\sqrt{({\\\\bf x} - {\\\\bf y})^{T}\\\\Sigma^{-1} ({\\\\bf x} - {\\\\bf y})} $$\n其中 \\\\( \\\\Sigma \\\\) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \\\\( {\\\\bf{X}} = ({\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_n) \\\\in \\\\Bbb{R}^{d \\\\times n}\\\\)，\\\\( {\\\\bf x}\\_i \\\\in \\\\Bbb{R}^{d} \\\\) 为第 \\\\( i \\\\) 个样本点，每个样本点的维度为 \\\\( d \\\\)，样本点的总数为 \\\\( n \\\\) 个；再假设样本点的平均值 \\\\( m\\_{\\\\bf x} = \\\\frac{1}{n}\\\\sum\\_{i=1}^{n} {\\\\bf x}\\_i \\\\) 为 \\\\( {\\\\bf 0} \\\\) 向量（若不为 \\\\( {\\\\bf 0} \\\\)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \\\\( \\\\Sigma \\\\in \\\\Bbb{R}^{d \\\\times d} \\\\) 可被定义为\n$$ \\\\Sigma = \\\\frac{1}{n} {\\\\bf X}{\\\\bf X}^{T} $$\nMaholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。\n\n## Cluster 之间的距离衡量\n除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 `cluster` 之间的距离，常见的 `cluster` 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，`cluster` 是由样本点或一些子 `cluster` （这些样本点或子 `cluster` 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 `cluster` 。假设 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 为两个 cluster，则前四种方法定义的  \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 之间的距离如下表所示：\n\n| 方法 | 定义 |\n| :-: | :-: |\n| Single-link | \\\\( D(C\\_i, C\\_j) = \\\\min\\_{ {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j } d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| Complete-link | \\\\( D(C\\_i, C\\_j) = \\\\max\\_{ {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j} d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| UPGMA | \\\\( D(C\\_i, C\\_j) = \\\\frac{1}{\\\\mid C\\_i \\\\mid \\\\mid C\\_j \\\\mid} \\\\sum\\_ { {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j}  d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| WPGMA | omitting |\n\n其中 Single-link 定义两个 `cluster` 之间的距离为两个 `cluster` 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 `cluster`；而 Complete-link 则定义两个 `cluster` 之间的距离为两个 `cluster` 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 `cluster` 之间的距离为两个 `cluster` 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 `cluster` 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 `cluster` 对距离的计算的影响在同一层次上，而不受 `cluster` 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 `cluster` 之间的距离，而是通过已有的 `cluster` 之间的距离来计算合并后的新的 `cluster` 和剩余 `cluster` 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。\n\nCentroid/UPGMC 方法给每一个 `cluster` 计算一个质心，两个 `cluster` 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：\n$$ D\\_{\\\\rm{UPGMC}}(C\\_i, C\\_j) =  \\\\frac{1}{\\\\mid C\\_i \\\\mid \\\\mid C\\_j \\\\mid} \\\\sum\\_ { {\\\\bf x} \\\\in C\\_i, {\\\\bf y}\\\\in C\\_j}  d({\\\\bf x}, {\\\\bf y}) - \\\\frac{1}{2{\\\\mid C\\_i \\\\mid \\}^{2}} \\\\sum\\_ { {\\\\bf x}, {\\\\bf y}\\\\in C\\_i}  d( {\\\\bf x}, {\\\\bf y}) - \\\\frac{1}{2{\\\\mid \\\\it{C\\_j} \\\\mid \\}^{2}} \\\\sum\\_ { {\\\\bf x}, {\\\\bf y} \\\\in C\\_j}  d( {\\\\bf x}, {\\\\bf y}) $$\n当上式中的 \\\\( d(.,.) \\\\) 为平方 Euclidean 距离时，\\\\( D\\_{\\\\rm{UPGMC}}(C\\_i, C\\_j) \\\\) 为 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 的中心点（每个 `cluster` 内所有样本点之间的平均值）之间的平方 Euclidean 距离。\n\nMedian/WPGMC 方法为每个 `cluster` 计算质心时，引入了权重。\n\nWard 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 `cluster` 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，`cluster` \\\\( C \\\\) 的 \\\\( {\\\\rm ESS} \\\\) 定义如下：\n$$  {\\\\rm ESS} ( C ) = \\\\sum\\_{ {\\\\bf x} \\\\in C} ({\\\\bf x} - m\\_{\\\\bf x})^{T} ({\\\\bf x} - m\\_{\\\\bf x}) $$\n其中 \\\\( m\\_{\\\\bf x} \\\\) 为 \\\\( C \\\\) 中样本点的均值。可以看到 \\\\( {\\\\rm ESS} \\\\) 衡量的是一个 `cluster` 内的样本点的聚合程度，样本点越聚合，\\\\( {\\\\rm ESS} \\\\) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 `cluster` 的 \\\\( {\\\\rm ESS} \\\\) 之和相对于合并前的 `cluster` 的 \\\\( {\\\\rm ESS} \\\\) 之和的增长最小。\n\n---- \n# Agglomerative 层次聚类算法\n这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 `cluster` 之间的距离的 Lance-Williams 方法。\n\n## Lance-Williams 方法\n在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 `cluster` 合并为一个新的 `cluster`，然后再计算这个新的 `cluster` 与其他当前未被合并的 `cluster` 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 `cluster` 距离衡量方法都适用。具体地，对于三个 `cluster` \\\\( C\\_k \\\\)，\\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\)， Lance-Williams 给出的 \\\\( C\\_k \\\\) 与 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 合并后的新 `cluster`  之间的距离的计算方法如下式所示：\n$$  D(C\\_k, C\\_i \\\\cup C\\_j) = \\\\alpha\\_i D(C\\_k, C\\_i) + \\\\alpha\\_j D(C\\_k, C\\_j) + \\\\beta D(C\\_i, C\\_j) +  \\\\gamma \\\\mid D(C\\_k, C\\_i) - D(C\\_k, C\\_j) \\\\mid $$\n其中，\\\\( \\\\alpha\\_i \\\\)，\\\\( \\\\alpha\\_j \\\\)，\\\\( \\\\beta \\\\)，\\\\( \\\\gamma \\\\) 均为参数，随 `cluster` 之间的距离计算方法的不同而不同，具体总结为下表（注：\\\\( n\\_i \\\\) 为 `cluster` \\\\( C\\_i \\\\) 中的样本点的个数)：\n\n| 方法 |  参数 \\\\( \\\\alpha\\_i \\\\) | 参数 \\\\( \\\\alpha\\_j \\\\) | 参数 \\\\( \\\\beta \\\\) | 参数 \\\\( \\\\gamma \\\\) |\n| :-: | :-: | :-: | :-: | :-: |\n| Single-link | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( -1/2 \\\\) |\n| Complete-link | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( 1/2 \\\\) |\n| UPGMA | \\\\( n\\_i/(n\\_i + n\\_j) \\\\) | \\\\( n\\_j/(n\\_i + n\\_j) \\\\) | \\\\( 0 \\\\) | \\\\( 0 \\\\) |\n| WPGMA | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( 0 \\\\) |\n| UPGMC | \\\\( n\\_i/(n\\_i + n\\_j) \\\\) | \\\\( n\\_j/(n\\_i + n\\_j) \\\\) | \\\\( n\\_{i}n\\_{j}/(n\\_i + n\\_j)^{2} \\\\) | \\\\( 0 \\\\) |\n| WPGMC | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 1/4 \\\\) | \\\\( 0 \\\\) |\n| Ward | \\\\( (n\\_k + n\\_i)/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( (n\\_k + n\\_j)/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( n\\_k/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( 0 \\\\) |\n\n其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。\n\n## Naive 算法\n给定数据集 \\\\( {\\\\bf{X}} = ({\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_n) \\\\)，Agglomerative 层次聚类最简单的实现方法分为以下几步：\n\n1.  初始时每个样本为一个 `cluster`，计算距离矩阵 \\\\( \\\\bf D \\\\)，其中元素 \\\\( D\\_{ij} \\\\) 为样本点 \\\\( {\\\\bf x}\\_i \\\\)  和 \\\\( {\\\\bf x}\\_j \\\\) 之间的距离；\n2.  遍历距离矩阵 \\\\( \\\\bf D \\\\)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 `cluster` 的编号，将这两个 `cluster` 合并为一个新的 `cluster` 并依据 Lance-Williams 方法更新距离矩阵 \\\\( \\\\bf D \\\\) （删除这两个 `cluster` 对应的行和列，并把由新  `cluster` 所算出来的距离向量插入 \\\\( \\\\bf D \\\\) 中），存储本次合并的相关信息；\n3.  重复 2 的过程，直至最终只剩下一个 `cluster` 。\n\n当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。\n\n可以看到，该 Naive 算法的时间复杂度为 \\\\( O(n^\\{3}) \\\\) （由于每次合并两个 `cluster` 时都要遍历大小为  \\\\( O(n^\\{2}) \\\\) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \\\\( n - 1 \\\\) 次），空间复杂度为 \\\\( O(n^\\{2}) \\\\) （由于要存储距离矩阵）。\n\n当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \\\\( O(n^\\{2} ) \\\\) 或更低，例如 [SLINK][5] 算法（Single-link 方法），[CLINK][6] 算法（Complete-link 方法），[BIRCH][7] 算法（适用于 Euclidean 距离准则）等等。\n\n---- \n# 利用 Scipy 实现层次聚类\n在这里我们将利用 [SciPy][8]（python 中的一个用于数值分析和科学计算的第三方包，功能强大，[NumPy][9]+SciPy+[matplotlib][10] 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。\n## 生成实验样本集\n首先，我们需要导入相关的模块，代码如下所示：\n```python\n# python 3.6\n>>> import numpy as np\n>>> from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n>>> from sklearn.datasets.samples_generator import make_blobs\n>>> import matplotlib.pyplot as plt\n```\n\n其中 [`make_blobs`][11] 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 `cluster` 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 `X` 的维度为 `n*d`。\n```python\n>>> centers = [[1, 1], [-1, -1], [1, -1]]    # 定义 3 个中心点\n# 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别\n>>> X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0）  \n>>> plt.figure(figsize=(10, 8))\n>>> plt.scatter(X[:, 0], X[:, 1], c='b')\n>>> plt.title('The dataset')\n>>> plt.show()\n```\n\n样本的分布如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/the_dataset_one_for_hierarchy_clustering.png\" width = \"780\" height = \"650\" alt = \"数据集分布图\" align = center />\n</div>\n\n## 进行 Agglomerative 层次聚类\nSciPy 里面进行层次聚类非常简单，直接调用 [`linkage`][12] 函数，一行代码即可搞定。\n```python\n>>> Z = linkage(X,  method='ward', metric='euclidean')\n```\n\n以上即进行了一次 `cluster` 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 `method` 参数可以为 `'single'`、 `'complete'` 、`'average'`、 `'weighted'`、 `'centroid'`、 `'median'`、 `'ward'` 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 `metric` 参数调整。\n\n`linkage` 函数的返回值 `Z` 为一个维度 `(n-1)*4` 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 `4` 个值分别对应合并的两个 `cluster` 的序号、两个 `cluster` 之间的距离以及本次合并后产生的新的 `cluster` 所包含的样本点的个数；具体地，对于第 `i` 次迭代（对应 `Z` 的第 `i` 行），序号为 `Z[i, 0]` 和序号为 `Z[i, 1]` 的 `cluster` 合并产生新的 `cluster` `n + i`, `Z[i, 2]` 为序号为 `Z[i, 0]` 和序号为 `Z[i, 1]` 的 `cluster` 之间的距离，合并后的 `cluster` 包含 `Z[i, 3]` 个样本点。\n\n例如本次实验中 `Z` 记录到的前 `25` 次合并的信息如下所示：\n```python\n>>> print(Z.shape)\n(749, 4)\n>>> print(Z[: 25])\n[[253.      491.        0.00185   2.     ]\n [452.      696.        0.00283   2.     ]\n [ 70.      334.        0.00374   2.     ]\n [237.      709.        0.00378   2.     ]\n [244.      589.        0.00423   2.     ]\n [141.      550.        0.00424   2.     ]\n [195.      672.        0.00431   2.     ]\n [ 71.      102.        0.00496   2.     ]\n [307.      476.        0.00536   2.     ]\n [351.      552.        0.00571   2.     ]\n [ 62.      715.        0.00607   2.     ]\n [ 98.      433.        0.0065    2.     ]\n [255.      572.        0.00671   2.     ]\n [437.      699.        0.00685   2.     ]\n [ 55.      498.        0.00765   2.     ]\n [143.      734.        0.00823   2.     ]\n [182.      646.        0.00843   2.     ]\n [ 45.      250.        0.0087    2.     ]\n [298.      728.        0.00954   2.     ]\n [580.      619.        0.01033   2.     ]\n [179.      183.        0.01062   2.     ]\n [101.      668.        0.01079   2.     ]\n [131.      544.        0.01125   2.     ]\n [655.      726.        0.01141   2.     ]\n [503.      756.        0.01265   3.     ]]\n```\n\n从上面的信息可以看到，在第 `6` 次合并中，样本点 `141` 与样本点 `550` 进行了合并，生成新 `cluster` `756`；在第 25 次合并中，样本点 `503` 与 `cluster` `756` 进行了合并，生成新的 `cluster` `770`。我们可以将样本点 `141`、`550` 和 `503` 的特征信息打印出来，来看看它们是否确实很接近。\n```python\n>>> print(X[[141, 550, 503]])\n[[ 1.27098 -0.97927]\n [ 1.27515 -0.98006]\n [ 1.37274  1.13599]]\n```\n\n看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 `cluster` 中相隔最近的。\n\n## 画出树形图\nSciPy 中给出了根据层次聚类的结果 `Z` 绘制树形图的函数 [`dendrogram`][13]，我们由此画出本次实验中的最后 `20` 次的合并过程。\n```python\n>>> plt.figure(figsize=(10, 8))\n>>> dendrogram(Z, truncate_mode='lastp', p=20, show_leaf_counts=False, leaf_rotation=90, leaf_font_size=15, show_contracted=True)\n>>> plt.title('Dendrogram for the Agglomerative Clustering')\n>>> plt.xlabel('sample index')\n>>> plt.ylabel('distance')\n>>> plt.show()\n```\n\n得到的树形图如下所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dendrogram_for_the_agglomerative_clustering.png\" width = \"780\" height = \"650\" alt = \"程序绘制出的树形图\" align = center />\n</div>\n\n可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 `3` 个 `cluster`（和实际相符），分别在上图中由三种颜色所表示。\n\n## 获取聚类结果\n在得到了层次聚类的过程信息 `Z` 后，我们可以使用 [`fcluster`][14] 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 `d`，得到在该距离以下的未合并的所有 `cluster` 作为聚类结果；另一个是指定 `cluster` 的数量 `k`，函数会返回最后的 `k` 个 `cluster` 作为聚类结果。使用哪个维度由参数 `criterion` 决定，对应的临界距离或聚类的数量则由参数 `t` 所记录。`fcluster` 函数的结果为一个一维数组，记录每个样本的类别信息。\n\n对应的代码与返回结果如下所示。\n```python\n# 根据临界距离返回聚类结果\n>>> d = 15\n>>> labels_1 = fcluster(Z, t=d, criterion='distance')\n>>> print(labels_1[: 100])    # 打印聚类结果\n[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3\n 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2\n 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]\n>>> print(len(set(labels_1)))   # 看看在该临界距离下有几个 cluster\n3   \n# 根据聚类数目返回聚类结果\n>>> k = 3\n>>> labels_2 = fcluster(Z, t=k, criterion='maxclust')\n>>> print(labels_2[: 100])  \n[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3\n 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2\n 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]\n>>> list(labels_1) == list(labels_2)      # 看看两种不同维度下得到的聚类结果是否一致\nTrue\n```\n\n下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。\n```python\n>>> plt.figure(figsize=(10, 8))\n>>> plt.title('The Result of the Agglomerative Clustering')\n>>> plt.scatter(X[:, 0], X[:, 1], c=labels_2, cmap='prism')\n>>> plt.show()\n```\n\n可视化结果如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/result_of_agglomerative_clustering.png\" width = \"780\" height = \"650\" alt = \"层次聚类结果\" align = center />\n</div>\n\n上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。\n\n## 比较不同方法下的聚类结果\n最后，我们对同一份样本集进行了 `cluster` 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 `3`，程序如下：\n```python\nfrom time import time\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\nimport matplotlib.pyplot as plt\n\n# 生成样本点\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels = make_blobs(n_samples=750, centers=centers,\n                        cluster_std=0.4, random_state=0)\n\n# 可视化聚类结果\ndef plot_clustering(X, labels, title=None):\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism')\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis('off')\n    plt.tight_layout()\n\n# 进行 Agglomerative 层次聚类\nlinkage_method_list = ['single', 'complete', 'average', 'ward']\nplt.figure(figsize=(10, 8))\nncols, nrows = 2, int(np.ceil(len(linkage_method_list) / 2))\nplt.subplots(nrows=nrows, ncols=ncols)\n\nfor i, linkage_method in enumerate(linkage_method_list):\n    print('method %s:' % linkage_method)\n    start_time = time()\n    Z = linkage(X, method=linkage_method)\n    labels_pred = fcluster(Z, t=3, criterion='maxclust')\n    print('Adjust mutual information: %.3f' %\n            adjusted_mutual_info_score(labels, labels_pred))\n    print('time used: %.3f seconds' % (time() - start_time))\n    plt.subplot(nrows, ncols, i + 1)\n    plot_clustering(X, labels_pred, '%s linkage' % linkage_method)\n\nplt.show()\n ```\n\n可以得到 4 种方法下的聚类结果如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/aggm_clust_linkage_comparing.png\" width = \"780\" height = \"650\" alt = \"不同方法下的聚类结果\" align = center />\n</div>\n\n在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 [AMI]()()（Adjust Mutual Information）量，该量越接近于 `1` 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下：\n\n```\nmethod single:\nAdjust mutual information: 0.001\ntime used: 0.008 seconds\nmethod complete:\nAdjust mutual information: 0.838\ntime used: 0.013 seconds\nmethod average:\nAdjust mutual information: 0.945\ntime used: 0.019 seconds\nmethod ward:\nAdjust mutual information: 0.956\ntime used: 0.015 seconds\n```\n\n从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 `cluster`，而其他两个 `cluster` 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\n[3]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[4]:\thttps://en.wikipedia.org/wiki/Dendrogram\n[5]:\thttps://www.cs.ucsb.edu/~veronika/MAE/SLINK_sibson.pdf\n[6]:\thttps://watermark.silverchair.com/200364.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAaswggGnBgkqhkiG9w0BBwagggGYMIIBlAIBADCCAY0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMf_lVS5Dqd4euptUDAgEQgIIBXoHBDW9Uw-YT4wPpKAKR0gkqiihYSy4CLlK_Pm1lZz5SR-2MJyucOW9m523Gx4wPgLpuVr-mcSnq5BOMQRb3RP4QmxEWnImOOXip-Bxz0TGys7iiYn9Ie0Of_9xoR3xmZP8UJY4uVinVSFzPLVlwVpusPrEFjcLABmjjbz5dmGR10kHHrlvtTiS0imCnPKNiZ3zdA8wMNWVM0mOABq4cxCnicAo5iI7zmgIEoYfi23sR6lQ0Y-77ohbWBcm61XMJJh-OTl-xmY6fedf9LdtgcKBiReB1HGH6U6SEDx3cW7I3J9c33YM_cRkFwz1WIkeZI5_fDc-z3Mc_f7c9y0L_zHXYnN1pgQvGTEKeRy1h_w9KjG3D2UKl9JRrE3Kflw_zS8TVGFi8laoU2ZyujHJLLr7F_gsfpyIEDXkelxdVAF2Srnkz_Bk4FhRcsKzx7AspBEFrGbJtDBBZz0uHbvGO\n[7]:\thttp://www.cs.uvm.edu/~xwu/kdd/BIRCH.pdf\n[8]:\thttps://www.scipy.org/\n[9]:\thttp://www.numpy.org/ \"NumPy\"\n[10]:\thttps://matplotlib.org/index.html\n[11]:\thttp://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\n[12]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n[13]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram\n[14]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster\n","source":"_posts/聚类分析（一）：层次聚类算法.md","raw":"---\ntitle: 聚类分析（一）：层次聚类算法\nlayout: post\ndate: 2017-12-01 20:59:23\ntags:\n- 聚类\n- 非监督学习\n- 层次聚类\ncategories:\n- 机器学习算法\nkeywords: 聚类,层次聚类,非监督学习,clustering,machine learning\n\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# 聚类算法综述\n聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 `cluster`，使得同一 `cluster` 内的对象在某种意义上比不同的 `cluster` 之间的对象更为相似。\n\n由于 “`cluster`” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类：\n- 基于**连通模型**（connectivity-based）的聚类算法： 即本文将要讲述的**层次聚类**算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 `cluster`。\n- 基于**中心点模型**（centroid-based）的聚类算法： 在此类算法中，每个 `cluster` 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 `k` 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 `cluster` 的中心点的距离的平方和，优化变量为每个 `cluster` 的中心点以及每个对象属于哪个 `cluster`；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，[ k-means 算法 ][2]即是其中的一种。\n- 基于**分布模型**（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 `cluster` 即可，最常被使用的此类算法为[ 高斯混合模型（GMM）聚类][3]。\n- 基于**密度**（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 `cluster`，`cluster` 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。\n\n---- \n# 层次聚类综述\n层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 `clusters`，后面一层生成的 `clusters` 基于前面一层的结果。层次聚类算法一般分为两类：\n- Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 `cluster`，每次按一定的准则将最相近的两个 `cluster` 合并生成一个新的 `cluster`，如此往复，直至最终所有的对象都属于一个 `cluster`。本文主要关注此类算法。\n- Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 `cluster`，每次按一定的准则将某个 `cluster` 划分为多个 `cluster`，如此往复，直至每个对象均是一个 `cluster`。\n\n下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_for_hierarchical_clustering.PNG\" width = \"560\" height = \"400\"  = \"层次聚类图示\" align = center />\n</div>\n\n另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。\n\n## 树形图\n[ 树形图 ][4]（dendrogram）可以用来直观地表示层次聚类的成果。一个有 `5` 个点的树形图如下图所示，其中纵坐标高度表示不同的 `cluster` 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 的距离最近（为 `1`），因此将 \\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 合并为一个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)，所以在树形图中首先将节点 \\\\( x\\_1 \\\\) 和 \\\\( x\\_2 \\\\) 连接，使其成为一个新的节点  \\\\( (x\\_1, x\\_2) \\\\) 的子节点，并将这个新的节点的高度置为 `1`；之后再在剩下的 `4` 个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)， \\\\( x\\_3 \\\\)， \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 中选取距离最近的两个 `cluster` 合并，\\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 的距离最近（为 `2`），因此将 \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 合并为一个 cluster \\\\( (x\\_4, x\\_5) \\\\)，体现在树形图上，是将节点 \\\\( x\\_4 \\\\) 和 \\\\( x\\_5 \\\\) 连接，使其成为一个新的节点 \\\\( (x\\_4, x\\_5) \\\\) 的子节点，并将此新节点的高度置为 `2`；....依此模式进行树形图的生成，直至最终只剩下一个 `cluster` \\\\( ((x\\_1, x\\_2), x\\_3), (x\\_4, x\\_5)) \\\\)。\n\n可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 `cluster` 之间的距离都不大于 \\\\( h \\\\)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \\\\( h \\\\)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \\\\( h=2.5 \\\\)，即可得到 `3` 个 `cluster` \\\\( (x\\_1, x\\_2) \\\\)， \\\\( x\\_3 \\\\) 和 \\\\( (x\\_4, x\\_5) \\\\)。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/a_dendrogram.png\" width = \"550\" height = \"440\" alt = \"树形图示例\" align = center />\n</div>\n\n## 对象之间的距离衡量\n衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \\\\( d \\\\)  维空间的两个对象 \\\\({\\\\bf x} =[ x\\_1, x\\_2, …, x\\_d]^{T} \\\\) 和 \\\\({\\\\bf y} = [y\\_1, y\\_2, …, y\\_d]^{T}\\\\)，其在不同距离准则下的距离计算方法如下表所示:\n\n| 距离准则  | 距离计算方法 |\n| :-: | :-: |\n| Euclidean 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = [\\\\sum\\_{j=1}^{d} (x\\_j-y\\_j)^{2} ]^{\\\\frac{1}{2}} = [({\\\\bf x} - {\\\\bf y})^{T} ({\\\\bf x} - {\\\\bf y})]^{\\\\frac{1}{2}} \\\\) |\n| Manhattan 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = \\\\sum\\_{j=1}^{d} \\\\mid{x\\_j-y\\_j}\\\\mid \\\\) |\n| Chebyshev 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = \\\\max\\_{1\\\\leq{j}\\\\leq{d}} \\\\mid{x\\_j-y\\_j}\\\\mid \\\\) |\n| Minkowski 距离 | \\\\( d({\\\\bf x},{\\\\bf y}) = [\\\\sum\\_{j=1}^{d} (x\\_j-y\\_j)^{p} ]^{\\\\frac{1}{p}}, p\\\\geq{1} \\\\) |\n\nMinkowski 距离就是 \\\\( \\\\rm{L}\\\\it{p}\\\\) 范数（\\\\( p\\\\geq{1} \\\\))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \\\\( p = 1, 2, \\\\infty \\\\) 时的情形。\n\n另一种常用的距离是 Maholanobis 距离，其定义如下：\n$$ d\\_{mah}({\\\\bf x}, {\\\\bf y}) = \\\\sqrt{({\\\\bf x} - {\\\\bf y})^{T}\\\\Sigma^{-1} ({\\\\bf x} - {\\\\bf y})} $$\n其中 \\\\( \\\\Sigma \\\\) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \\\\( {\\\\bf{X}} = ({\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_n) \\\\in \\\\Bbb{R}^{d \\\\times n}\\\\)，\\\\( {\\\\bf x}\\_i \\\\in \\\\Bbb{R}^{d} \\\\) 为第 \\\\( i \\\\) 个样本点，每个样本点的维度为 \\\\( d \\\\)，样本点的总数为 \\\\( n \\\\) 个；再假设样本点的平均值 \\\\( m\\_{\\\\bf x} = \\\\frac{1}{n}\\\\sum\\_{i=1}^{n} {\\\\bf x}\\_i \\\\) 为 \\\\( {\\\\bf 0} \\\\) 向量（若不为 \\\\( {\\\\bf 0} \\\\)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \\\\( \\\\Sigma \\\\in \\\\Bbb{R}^{d \\\\times d} \\\\) 可被定义为\n$$ \\\\Sigma = \\\\frac{1}{n} {\\\\bf X}{\\\\bf X}^{T} $$\nMaholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。\n\n## Cluster 之间的距离衡量\n除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 `cluster` 之间的距离，常见的 `cluster` 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，`cluster` 是由样本点或一些子 `cluster` （这些样本点或子 `cluster` 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 `cluster` 。假设 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 为两个 cluster，则前四种方法定义的  \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 之间的距离如下表所示：\n\n| 方法 | 定义 |\n| :-: | :-: |\n| Single-link | \\\\( D(C\\_i, C\\_j) = \\\\min\\_{ {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j } d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| Complete-link | \\\\( D(C\\_i, C\\_j) = \\\\max\\_{ {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j} d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| UPGMA | \\\\( D(C\\_i, C\\_j) = \\\\frac{1}{\\\\mid C\\_i \\\\mid \\\\mid C\\_j \\\\mid} \\\\sum\\_ { {\\\\bf x} \\\\in C\\_i, {\\\\bf y} \\\\in C\\_j}  d({\\\\bf x}, {\\\\bf y}) \\\\) |\n| WPGMA | omitting |\n\n其中 Single-link 定义两个 `cluster` 之间的距离为两个 `cluster` 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 `cluster`；而 Complete-link 则定义两个 `cluster` 之间的距离为两个 `cluster` 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 `cluster` 之间的距离为两个 `cluster` 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 `cluster` 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 `cluster` 对距离的计算的影响在同一层次上，而不受 `cluster` 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 `cluster` 之间的距离，而是通过已有的 `cluster` 之间的距离来计算合并后的新的 `cluster` 和剩余 `cluster` 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。\n\nCentroid/UPGMC 方法给每一个 `cluster` 计算一个质心，两个 `cluster` 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：\n$$ D\\_{\\\\rm{UPGMC}}(C\\_i, C\\_j) =  \\\\frac{1}{\\\\mid C\\_i \\\\mid \\\\mid C\\_j \\\\mid} \\\\sum\\_ { {\\\\bf x} \\\\in C\\_i, {\\\\bf y}\\\\in C\\_j}  d({\\\\bf x}, {\\\\bf y}) - \\\\frac{1}{2{\\\\mid C\\_i \\\\mid \\}^{2}} \\\\sum\\_ { {\\\\bf x}, {\\\\bf y}\\\\in C\\_i}  d( {\\\\bf x}, {\\\\bf y}) - \\\\frac{1}{2{\\\\mid \\\\it{C\\_j} \\\\mid \\}^{2}} \\\\sum\\_ { {\\\\bf x}, {\\\\bf y} \\\\in C\\_j}  d( {\\\\bf x}, {\\\\bf y}) $$\n当上式中的 \\\\( d(.,.) \\\\) 为平方 Euclidean 距离时，\\\\( D\\_{\\\\rm{UPGMC}}(C\\_i, C\\_j) \\\\) 为 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 的中心点（每个 `cluster` 内所有样本点之间的平均值）之间的平方 Euclidean 距离。\n\nMedian/WPGMC 方法为每个 `cluster` 计算质心时，引入了权重。\n\nWard 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 `cluster` 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，`cluster` \\\\( C \\\\) 的 \\\\( {\\\\rm ESS} \\\\) 定义如下：\n$$  {\\\\rm ESS} ( C ) = \\\\sum\\_{ {\\\\bf x} \\\\in C} ({\\\\bf x} - m\\_{\\\\bf x})^{T} ({\\\\bf x} - m\\_{\\\\bf x}) $$\n其中 \\\\( m\\_{\\\\bf x} \\\\) 为 \\\\( C \\\\) 中样本点的均值。可以看到 \\\\( {\\\\rm ESS} \\\\) 衡量的是一个 `cluster` 内的样本点的聚合程度，样本点越聚合，\\\\( {\\\\rm ESS} \\\\) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 `cluster` 的 \\\\( {\\\\rm ESS} \\\\) 之和相对于合并前的 `cluster` 的 \\\\( {\\\\rm ESS} \\\\) 之和的增长最小。\n\n---- \n# Agglomerative 层次聚类算法\n这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 `cluster` 之间的距离的 Lance-Williams 方法。\n\n## Lance-Williams 方法\n在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 `cluster` 合并为一个新的 `cluster`，然后再计算这个新的 `cluster` 与其他当前未被合并的 `cluster` 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 `cluster` 距离衡量方法都适用。具体地，对于三个 `cluster` \\\\( C\\_k \\\\)，\\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\)， Lance-Williams 给出的 \\\\( C\\_k \\\\) 与 \\\\( C\\_i \\\\) 和 \\\\( C\\_j \\\\) 合并后的新 `cluster`  之间的距离的计算方法如下式所示：\n$$  D(C\\_k, C\\_i \\\\cup C\\_j) = \\\\alpha\\_i D(C\\_k, C\\_i) + \\\\alpha\\_j D(C\\_k, C\\_j) + \\\\beta D(C\\_i, C\\_j) +  \\\\gamma \\\\mid D(C\\_k, C\\_i) - D(C\\_k, C\\_j) \\\\mid $$\n其中，\\\\( \\\\alpha\\_i \\\\)，\\\\( \\\\alpha\\_j \\\\)，\\\\( \\\\beta \\\\)，\\\\( \\\\gamma \\\\) 均为参数，随 `cluster` 之间的距离计算方法的不同而不同，具体总结为下表（注：\\\\( n\\_i \\\\) 为 `cluster` \\\\( C\\_i \\\\) 中的样本点的个数)：\n\n| 方法 |  参数 \\\\( \\\\alpha\\_i \\\\) | 参数 \\\\( \\\\alpha\\_j \\\\) | 参数 \\\\( \\\\beta \\\\) | 参数 \\\\( \\\\gamma \\\\) |\n| :-: | :-: | :-: | :-: | :-: |\n| Single-link | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( -1/2 \\\\) |\n| Complete-link | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( 1/2 \\\\) |\n| UPGMA | \\\\( n\\_i/(n\\_i + n\\_j) \\\\) | \\\\( n\\_j/(n\\_i + n\\_j) \\\\) | \\\\( 0 \\\\) | \\\\( 0 \\\\) |\n| WPGMA | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 0 \\\\) | \\\\( 0 \\\\) |\n| UPGMC | \\\\( n\\_i/(n\\_i + n\\_j) \\\\) | \\\\( n\\_j/(n\\_i + n\\_j) \\\\) | \\\\( n\\_{i}n\\_{j}/(n\\_i + n\\_j)^{2} \\\\) | \\\\( 0 \\\\) |\n| WPGMC | \\\\( 1/2 \\\\) | \\\\( 1/2 \\\\) | \\\\( 1/4 \\\\) | \\\\( 0 \\\\) |\n| Ward | \\\\( (n\\_k + n\\_i)/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( (n\\_k + n\\_j)/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( n\\_k/(n\\_i + n\\_j + n\\_k) \\\\) | \\\\( 0 \\\\) |\n\n其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。\n\n## Naive 算法\n给定数据集 \\\\( {\\\\bf{X}} = ({\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_n) \\\\)，Agglomerative 层次聚类最简单的实现方法分为以下几步：\n\n1.  初始时每个样本为一个 `cluster`，计算距离矩阵 \\\\( \\\\bf D \\\\)，其中元素 \\\\( D\\_{ij} \\\\) 为样本点 \\\\( {\\\\bf x}\\_i \\\\)  和 \\\\( {\\\\bf x}\\_j \\\\) 之间的距离；\n2.  遍历距离矩阵 \\\\( \\\\bf D \\\\)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 `cluster` 的编号，将这两个 `cluster` 合并为一个新的 `cluster` 并依据 Lance-Williams 方法更新距离矩阵 \\\\( \\\\bf D \\\\) （删除这两个 `cluster` 对应的行和列，并把由新  `cluster` 所算出来的距离向量插入 \\\\( \\\\bf D \\\\) 中），存储本次合并的相关信息；\n3.  重复 2 的过程，直至最终只剩下一个 `cluster` 。\n\n当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。\n\n可以看到，该 Naive 算法的时间复杂度为 \\\\( O(n^\\{3}) \\\\) （由于每次合并两个 `cluster` 时都要遍历大小为  \\\\( O(n^\\{2}) \\\\) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \\\\( n - 1 \\\\) 次），空间复杂度为 \\\\( O(n^\\{2}) \\\\) （由于要存储距离矩阵）。\n\n当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \\\\( O(n^\\{2} ) \\\\) 或更低，例如 [SLINK][5] 算法（Single-link 方法），[CLINK][6] 算法（Complete-link 方法），[BIRCH][7] 算法（适用于 Euclidean 距离准则）等等。\n\n---- \n# 利用 Scipy 实现层次聚类\n在这里我们将利用 [SciPy][8]（python 中的一个用于数值分析和科学计算的第三方包，功能强大，[NumPy][9]+SciPy+[matplotlib][10] 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。\n## 生成实验样本集\n首先，我们需要导入相关的模块，代码如下所示：\n```python\n# python 3.6\n>>> import numpy as np\n>>> from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n>>> from sklearn.datasets.samples_generator import make_blobs\n>>> import matplotlib.pyplot as plt\n```\n\n其中 [`make_blobs`][11] 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 `cluster` 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 `X` 的维度为 `n*d`。\n```python\n>>> centers = [[1, 1], [-1, -1], [1, -1]]    # 定义 3 个中心点\n# 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别\n>>> X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0）  \n>>> plt.figure(figsize=(10, 8))\n>>> plt.scatter(X[:, 0], X[:, 1], c='b')\n>>> plt.title('The dataset')\n>>> plt.show()\n```\n\n样本的分布如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/the_dataset_one_for_hierarchy_clustering.png\" width = \"780\" height = \"650\" alt = \"数据集分布图\" align = center />\n</div>\n\n## 进行 Agglomerative 层次聚类\nSciPy 里面进行层次聚类非常简单，直接调用 [`linkage`][12] 函数，一行代码即可搞定。\n```python\n>>> Z = linkage(X,  method='ward', metric='euclidean')\n```\n\n以上即进行了一次 `cluster` 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 `method` 参数可以为 `'single'`、 `'complete'` 、`'average'`、 `'weighted'`、 `'centroid'`、 `'median'`、 `'ward'` 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 `metric` 参数调整。\n\n`linkage` 函数的返回值 `Z` 为一个维度 `(n-1)*4` 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 `4` 个值分别对应合并的两个 `cluster` 的序号、两个 `cluster` 之间的距离以及本次合并后产生的新的 `cluster` 所包含的样本点的个数；具体地，对于第 `i` 次迭代（对应 `Z` 的第 `i` 行），序号为 `Z[i, 0]` 和序号为 `Z[i, 1]` 的 `cluster` 合并产生新的 `cluster` `n + i`, `Z[i, 2]` 为序号为 `Z[i, 0]` 和序号为 `Z[i, 1]` 的 `cluster` 之间的距离，合并后的 `cluster` 包含 `Z[i, 3]` 个样本点。\n\n例如本次实验中 `Z` 记录到的前 `25` 次合并的信息如下所示：\n```python\n>>> print(Z.shape)\n(749, 4)\n>>> print(Z[: 25])\n[[253.      491.        0.00185   2.     ]\n [452.      696.        0.00283   2.     ]\n [ 70.      334.        0.00374   2.     ]\n [237.      709.        0.00378   2.     ]\n [244.      589.        0.00423   2.     ]\n [141.      550.        0.00424   2.     ]\n [195.      672.        0.00431   2.     ]\n [ 71.      102.        0.00496   2.     ]\n [307.      476.        0.00536   2.     ]\n [351.      552.        0.00571   2.     ]\n [ 62.      715.        0.00607   2.     ]\n [ 98.      433.        0.0065    2.     ]\n [255.      572.        0.00671   2.     ]\n [437.      699.        0.00685   2.     ]\n [ 55.      498.        0.00765   2.     ]\n [143.      734.        0.00823   2.     ]\n [182.      646.        0.00843   2.     ]\n [ 45.      250.        0.0087    2.     ]\n [298.      728.        0.00954   2.     ]\n [580.      619.        0.01033   2.     ]\n [179.      183.        0.01062   2.     ]\n [101.      668.        0.01079   2.     ]\n [131.      544.        0.01125   2.     ]\n [655.      726.        0.01141   2.     ]\n [503.      756.        0.01265   3.     ]]\n```\n\n从上面的信息可以看到，在第 `6` 次合并中，样本点 `141` 与样本点 `550` 进行了合并，生成新 `cluster` `756`；在第 25 次合并中，样本点 `503` 与 `cluster` `756` 进行了合并，生成新的 `cluster` `770`。我们可以将样本点 `141`、`550` 和 `503` 的特征信息打印出来，来看看它们是否确实很接近。\n```python\n>>> print(X[[141, 550, 503]])\n[[ 1.27098 -0.97927]\n [ 1.27515 -0.98006]\n [ 1.37274  1.13599]]\n```\n\n看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 `cluster` 中相隔最近的。\n\n## 画出树形图\nSciPy 中给出了根据层次聚类的结果 `Z` 绘制树形图的函数 [`dendrogram`][13]，我们由此画出本次实验中的最后 `20` 次的合并过程。\n```python\n>>> plt.figure(figsize=(10, 8))\n>>> dendrogram(Z, truncate_mode='lastp', p=20, show_leaf_counts=False, leaf_rotation=90, leaf_font_size=15, show_contracted=True)\n>>> plt.title('Dendrogram for the Agglomerative Clustering')\n>>> plt.xlabel('sample index')\n>>> plt.ylabel('distance')\n>>> plt.show()\n```\n\n得到的树形图如下所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dendrogram_for_the_agglomerative_clustering.png\" width = \"780\" height = \"650\" alt = \"程序绘制出的树形图\" align = center />\n</div>\n\n可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 `3` 个 `cluster`（和实际相符），分别在上图中由三种颜色所表示。\n\n## 获取聚类结果\n在得到了层次聚类的过程信息 `Z` 后，我们可以使用 [`fcluster`][14] 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 `d`，得到在该距离以下的未合并的所有 `cluster` 作为聚类结果；另一个是指定 `cluster` 的数量 `k`，函数会返回最后的 `k` 个 `cluster` 作为聚类结果。使用哪个维度由参数 `criterion` 决定，对应的临界距离或聚类的数量则由参数 `t` 所记录。`fcluster` 函数的结果为一个一维数组，记录每个样本的类别信息。\n\n对应的代码与返回结果如下所示。\n```python\n# 根据临界距离返回聚类结果\n>>> d = 15\n>>> labels_1 = fcluster(Z, t=d, criterion='distance')\n>>> print(labels_1[: 100])    # 打印聚类结果\n[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3\n 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2\n 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]\n>>> print(len(set(labels_1)))   # 看看在该临界距离下有几个 cluster\n3   \n# 根据聚类数目返回聚类结果\n>>> k = 3\n>>> labels_2 = fcluster(Z, t=k, criterion='maxclust')\n>>> print(labels_2[: 100])  \n[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3\n 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2\n 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]\n>>> list(labels_1) == list(labels_2)      # 看看两种不同维度下得到的聚类结果是否一致\nTrue\n```\n\n下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。\n```python\n>>> plt.figure(figsize=(10, 8))\n>>> plt.title('The Result of the Agglomerative Clustering')\n>>> plt.scatter(X[:, 0], X[:, 1], c=labels_2, cmap='prism')\n>>> plt.show()\n```\n\n可视化结果如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/result_of_agglomerative_clustering.png\" width = \"780\" height = \"650\" alt = \"层次聚类结果\" align = center />\n</div>\n\n上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。\n\n## 比较不同方法下的聚类结果\n最后，我们对同一份样本集进行了 `cluster` 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 `3`，程序如下：\n```python\nfrom time import time\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\nimport matplotlib.pyplot as plt\n\n# 生成样本点\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels = make_blobs(n_samples=750, centers=centers,\n                        cluster_std=0.4, random_state=0)\n\n# 可视化聚类结果\ndef plot_clustering(X, labels, title=None):\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism')\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis('off')\n    plt.tight_layout()\n\n# 进行 Agglomerative 层次聚类\nlinkage_method_list = ['single', 'complete', 'average', 'ward']\nplt.figure(figsize=(10, 8))\nncols, nrows = 2, int(np.ceil(len(linkage_method_list) / 2))\nplt.subplots(nrows=nrows, ncols=ncols)\n\nfor i, linkage_method in enumerate(linkage_method_list):\n    print('method %s:' % linkage_method)\n    start_time = time()\n    Z = linkage(X, method=linkage_method)\n    labels_pred = fcluster(Z, t=3, criterion='maxclust')\n    print('Adjust mutual information: %.3f' %\n            adjusted_mutual_info_score(labels, labels_pred))\n    print('time used: %.3f seconds' % (time() - start_time))\n    plt.subplot(nrows, ncols, i + 1)\n    plot_clustering(X, labels_pred, '%s linkage' % linkage_method)\n\nplt.show()\n ```\n\n可以得到 4 种方法下的聚类结果如下图所示。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/aggm_clust_linkage_comparing.png\" width = \"780\" height = \"650\" alt = \"不同方法下的聚类结果\" align = center />\n</div>\n\n在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 [AMI]()()（Adjust Mutual Information）量，该量越接近于 `1` 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下：\n\n```\nmethod single:\nAdjust mutual information: 0.001\ntime used: 0.008 seconds\nmethod complete:\nAdjust mutual information: 0.838\ntime used: 0.013 seconds\nmethod average:\nAdjust mutual information: 0.945\ntime used: 0.019 seconds\nmethod ward:\nAdjust mutual information: 0.956\ntime used: 0.015 seconds\n```\n\n从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 `cluster`，而其他两个 `cluster` 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\n[3]:\t../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\n[4]:\thttps://en.wikipedia.org/wiki/Dendrogram\n[5]:\thttps://www.cs.ucsb.edu/~veronika/MAE/SLINK_sibson.pdf\n[6]:\thttps://watermark.silverchair.com/200364.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAaswggGnBgkqhkiG9w0BBwagggGYMIIBlAIBADCCAY0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMf_lVS5Dqd4euptUDAgEQgIIBXoHBDW9Uw-YT4wPpKAKR0gkqiihYSy4CLlK_Pm1lZz5SR-2MJyucOW9m523Gx4wPgLpuVr-mcSnq5BOMQRb3RP4QmxEWnImOOXip-Bxz0TGys7iiYn9Ie0Of_9xoR3xmZP8UJY4uVinVSFzPLVlwVpusPrEFjcLABmjjbz5dmGR10kHHrlvtTiS0imCnPKNiZ3zdA8wMNWVM0mOABq4cxCnicAo5iI7zmgIEoYfi23sR6lQ0Y-77ohbWBcm61XMJJh-OTl-xmY6fedf9LdtgcKBiReB1HGH6U6SEDx3cW7I3J9c33YM_cRkFwz1WIkeZI5_fDc-z3Mc_f7c9y0L_zHXYnN1pgQvGTEKeRy1h_w9KjG3D2UKl9JRrE3Kflw_zS8TVGFi8laoU2ZyujHJLLr7F_gsfpyIEDXkelxdVAF2Srnkz_Bk4FhRcsKzx7AspBEFrGbJtDBBZz0uHbvGO\n[7]:\thttp://www.cs.uvm.edu/~xwu/kdd/BIRCH.pdf\n[8]:\thttps://www.scipy.org/\n[9]:\thttp://www.numpy.org/ \"NumPy\"\n[10]:\thttps://matplotlib.org/index.html\n[11]:\thttp://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\n[12]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n[13]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram\n[14]:\thttp://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster\n","slug":"聚类分析（一）：层次聚类算法","published":1,"updated":"2018-03-22T12:14:49.825Z","comments":1,"photos":[],"link":"","_id":"cjfclp3ty0007hws659wnpb53","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"聚类算法综述\"><a href=\"#聚类算法综述\" class=\"headerlink\" title=\"聚类算法综述\"></a>聚类算法综述</h1><p>聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 <code>cluster</code>，使得同一 <code>cluster</code> 内的对象在某种意义上比不同的 <code>cluster</code> 之间的对象更为相似。</p>\n<p>由于 “<code>cluster</code>” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类：</p>\n<ul>\n<li>基于<strong>连通模型</strong>（connectivity-based）的聚类算法： 即本文将要讲述的<strong>层次聚类</strong>算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 <code>cluster</code>。</li>\n<li>基于<strong>中心点模型</strong>（centroid-based）的聚类算法： 在此类算法中，每个 <code>cluster</code> 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 <code>k</code> 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 <code>cluster</code> 的中心点的距离的平方和，优化变量为每个 <code>cluster</code> 的中心点以及每个对象属于哪个 <code>cluster</code>；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，<a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\"> k-means 算法 </a>即是其中的一种。</li>\n<li>基于<strong>分布模型</strong>（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 <code>cluster</code> 即可，最常被使用的此类算法为<a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\"> 高斯混合模型（GMM）聚类</a>。</li>\n<li>基于<strong>密度</strong>（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 <code>cluster</code>，<code>cluster</code> 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。</li>\n</ul>\n<hr>\n<h1 id=\"层次聚类综述\"><a href=\"#层次聚类综述\" class=\"headerlink\" title=\"层次聚类综述\"></a>层次聚类综述</h1><p>层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 <code>clusters</code>，后面一层生成的 <code>clusters</code> 基于前面一层的结果。层次聚类算法一般分为两类：</p>\n<ul>\n<li>Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 <code>cluster</code>，每次按一定的准则将最相近的两个 <code>cluster</code> 合并生成一个新的 <code>cluster</code>，如此往复，直至最终所有的对象都属于一个 <code>cluster</code>。本文主要关注此类算法。</li>\n<li>Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 <code>cluster</code>，每次按一定的准则将某个 <code>cluster</code> 划分为多个 <code>cluster</code>，如此往复，直至每个对象均是一个 <code>cluster</code>。</li>\n</ul>\n<p>下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_for_hierarchical_clustering.PNG\" width=\"560\" height=\"400\" ==\"\" \"层次聚类图示\"=\"\" align=\"center\"><br></div>\n\n<p>另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。</p>\n<h2 id=\"树形图\"><a href=\"#树形图\" class=\"headerlink\" title=\"树形图\"></a>树形图</h2><p><a href=\"https://en.wikipedia.org/wiki/Dendrogram\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> 树形图 </a>（dendrogram）可以用来直观地表示层次聚类的成果。一个有 <code>5</code> 个点的树形图如下图所示，其中纵坐标高度表示不同的 <code>cluster</code> 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\\( x_1 \\) 和 \\( x_2 \\) 的距离最近（为 <code>1</code>），因此将 \\( x_1 \\) 和 \\( x_2 \\) 合并为一个 <code>cluster</code> \\( (x_1, x_2) \\)，所以在树形图中首先将节点 \\( x_1 \\) 和 \\( x_2 \\) 连接，使其成为一个新的节点  \\( (x_1, x_2) \\) 的子节点，并将这个新的节点的高度置为 <code>1</code>；之后再在剩下的 <code>4</code> 个 <code>cluster</code> \\( (x_1, x_2) \\)， \\( x_3 \\)， \\( x_4 \\) 和 \\( x_5 \\) 中选取距离最近的两个 <code>cluster</code> 合并，\\( x_4 \\) 和 \\( x_5 \\) 的距离最近（为 <code>2</code>），因此将 \\( x_4 \\) 和 \\( x_5 \\) 合并为一个 cluster \\( (x_4, x_5) \\)，体现在树形图上，是将节点 \\( x_4 \\) 和 \\( x_5 \\) 连接，使其成为一个新的节点 \\( (x_4, x_5) \\) 的子节点，并将此新节点的高度置为 <code>2</code>；….依此模式进行树形图的生成，直至最终只剩下一个 <code>cluster</code> \\( ((x_1, x_2), x_3), (x_4, x_5)) \\)。</p>\n<p>可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 <code>cluster</code> 之间的距离都不大于 \\( h \\)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \\( h \\)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \\( h=2.5 \\)，即可得到 <code>3</code> 个 <code>cluster</code> \\( (x_1, x_2) \\)， \\( x_3 \\) 和 \\( (x_4, x_5) \\)。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/a_dendrogram.png\" width=\"550\" height=\"440\" alt=\"树形图示例\" align=\"center\"><br></div>\n\n<h2 id=\"对象之间的距离衡量\"><a href=\"#对象之间的距离衡量\" class=\"headerlink\" title=\"对象之间的距离衡量\"></a>对象之间的距离衡量</h2><p>衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \\( d \\)  维空间的两个对象 \\({\\bf x} =[ x_1, x_2, …, x_d]^{T} \\) 和 \\({\\bf y} = [y_1, y_2, …, y_d]^{T}\\)，其在不同距离准则下的距离计算方法如下表所示:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">距离准则</th>\n<th style=\"text-align:center\">距离计算方法</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Euclidean 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = [\\sum_{j=1}^{d} (x_j-y_j)^{2} ]^{\\frac{1}{2}} = [({\\bf x} - {\\bf y})^{T} ({\\bf x} - {\\bf y})]^{\\frac{1}{2}} \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Manhattan 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = \\sum_{j=1}^{d} \\mid{x_j-y_j}\\mid \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Chebyshev 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = \\max_{1\\leq{j}\\leq{d}} \\mid{x_j-y_j}\\mid \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Minkowski 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = [\\sum_{j=1}^{d} (x_j-y_j)^{p} ]^{\\frac{1}{p}}, p\\geq{1} \\)</td>\n</tr>\n</tbody>\n</table>\n<p>Minkowski 距离就是 \\( \\rm{L}\\it{p}\\) 范数（\\( p\\geq{1} \\))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \\( p = 1, 2, \\infty \\) 时的情形。</p>\n<p>另一种常用的距离是 Maholanobis 距离，其定义如下：<br>$$ d_{mah}({\\bf x}, {\\bf y}) = \\sqrt{({\\bf x} - {\\bf y})^{T}\\Sigma^{-1} ({\\bf x} - {\\bf y})} $$<br>其中 \\( \\Sigma \\) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \\( {\\bf{X}} = ({\\bf x}_1, {\\bf x}_2, …, {\\bf x}_n) \\in \\Bbb{R}^{d \\times n}\\)，\\( {\\bf x}_i \\in \\Bbb{R}^{d} \\) 为第 \\( i \\) 个样本点，每个样本点的维度为 \\( d \\)，样本点的总数为 \\( n \\) 个；再假设样本点的平均值 \\( m_{\\bf x} = \\frac{1}{n}\\sum_{i=1}^{n} {\\bf x}_i \\) 为 \\( {\\bf 0} \\) 向量（若不为 \\( {\\bf 0} \\)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \\( \\Sigma \\in \\Bbb{R}^{d \\times d} \\) 可被定义为<br>$$ \\Sigma = \\frac{1}{n} {\\bf X}{\\bf X}^{T} $$<br>Maholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。</p>\n<h2 id=\"Cluster-之间的距离衡量\"><a href=\"#Cluster-之间的距离衡量\" class=\"headerlink\" title=\"Cluster 之间的距离衡量\"></a>Cluster 之间的距离衡量</h2><p>除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 <code>cluster</code> 之间的距离，常见的 <code>cluster</code> 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，<code>cluster</code> 是由样本点或一些子 <code>cluster</code> （这些样本点或子 <code>cluster</code> 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 <code>cluster</code> 。假设 \\( C_i \\) 和 \\( C_j \\) 为两个 cluster，则前四种方法定义的  \\( C_i \\) 和 \\( C_j \\) 之间的距离如下表所示：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">定义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Single-link</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\min_{ {\\bf x} \\in C_i, {\\bf y} \\in C_j } d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Complete-link</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\max_{ {\\bf x} \\in C_i, {\\bf y} \\in C_j} d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMA</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\frac{1}{\\mid C_i \\mid \\mid C_j \\mid} \\sum_ { {\\bf x} \\in C_i, {\\bf y} \\in C_j}  d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMA</td>\n<td style=\"text-align:center\">omitting</td>\n</tr>\n</tbody>\n</table>\n<p>其中 Single-link 定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 <code>cluster</code>；而 Complete-link 则定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 <code>cluster</code> 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 <code>cluster</code> 对距离的计算的影响在同一层次上，而不受 <code>cluster</code> 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 <code>cluster</code> 之间的距离，而是通过已有的 <code>cluster</code> 之间的距离来计算合并后的新的 <code>cluster</code> 和剩余 <code>cluster</code> 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。</p>\n<p>Centroid/UPGMC 方法给每一个 <code>cluster</code> 计算一个质心，两个 <code>cluster</code> 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：<br>$$ D_{\\rm{UPGMC}}(C_i, C_j) =  \\frac{1}{\\mid C_i \\mid \\mid C_j \\mid} \\sum_ { {\\bf x} \\in C_i, {\\bf y}\\in C_j}  d({\\bf x}, {\\bf y}) - \\frac{1}{2{\\mid C_i \\mid }^{2}} \\sum_ { {\\bf x}, {\\bf y}\\in C_i}  d( {\\bf x}, {\\bf y}) - \\frac{1}{2{\\mid \\it{C_j} \\mid }^{2}} \\sum_ { {\\bf x}, {\\bf y} \\in C_j}  d( {\\bf x}, {\\bf y}) $$<br>当上式中的 \\( d(.,.) \\) 为平方 Euclidean 距离时，\\( D_{\\rm{UPGMC}}(C_i, C_j) \\) 为 \\( C_i \\) 和 \\( C_j \\) 的中心点（每个 <code>cluster</code> 内所有样本点之间的平均值）之间的平方 Euclidean 距离。</p>\n<p>Median/WPGMC 方法为每个 <code>cluster</code> 计算质心时，引入了权重。</p>\n<p>Ward 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 <code>cluster</code> 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，<code>cluster</code> \\( C \\) 的 \\( {\\rm ESS} \\) 定义如下：<br>$$  {\\rm ESS} ( C ) = \\sum_{ {\\bf x} \\in C} ({\\bf x} - m_{\\bf x})^{T} ({\\bf x} - m_{\\bf x}) $$<br>其中 \\( m_{\\bf x} \\) 为 \\( C \\) 中样本点的均值。可以看到 \\( {\\rm ESS} \\) 衡量的是一个 <code>cluster</code> 内的样本点的聚合程度，样本点越聚合，\\( {\\rm ESS} \\) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 <code>cluster</code> 的 \\( {\\rm ESS} \\) 之和相对于合并前的 <code>cluster</code> 的 \\( {\\rm ESS} \\) 之和的增长最小。</p>\n<hr>\n<h1 id=\"Agglomerative-层次聚类算法\"><a href=\"#Agglomerative-层次聚类算法\" class=\"headerlink\" title=\"Agglomerative 层次聚类算法\"></a>Agglomerative 层次聚类算法</h1><p>这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 <code>cluster</code> 之间的距离的 Lance-Williams 方法。</p>\n<h2 id=\"Lance-Williams-方法\"><a href=\"#Lance-Williams-方法\" class=\"headerlink\" title=\"Lance-Williams 方法\"></a>Lance-Williams 方法</h2><p>在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 <code>cluster</code> 合并为一个新的 <code>cluster</code>，然后再计算这个新的 <code>cluster</code> 与其他当前未被合并的 <code>cluster</code> 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 <code>cluster</code> 距离衡量方法都适用。具体地，对于三个 <code>cluster</code> \\( C_k \\)，\\( C_i \\) 和 \\( C_j \\)， Lance-Williams 给出的 \\( C_k \\) 与 \\( C_i \\) 和 \\( C_j \\) 合并后的新 <code>cluster</code>  之间的距离的计算方法如下式所示：<br>$$  D(C_k, C_i \\cup C_j) = \\alpha_i D(C_k, C_i) + \\alpha_j D(C_k, C_j) + \\beta D(C_i, C_j) +  \\gamma \\mid D(C_k, C_i) - D(C_k, C_j) \\mid $$<br>其中，\\( \\alpha_i \\)，\\( \\alpha_j \\)，\\( \\beta \\)，\\( \\gamma \\) 均为参数，随 <code>cluster</code> 之间的距离计算方法的不同而不同，具体总结为下表（注：\\( n_i \\) 为 <code>cluster</code> \\( C_i \\) 中的样本点的个数)：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">参数 \\( \\alpha_i \\)</th>\n<th style=\"text-align:center\">参数 \\( \\alpha_j \\)</th>\n<th style=\"text-align:center\">参数 \\( \\beta \\)</th>\n<th style=\"text-align:center\">参数 \\( \\gamma \\)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Single-link</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( -1/2 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Complete-link</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMA</td>\n<td style=\"text-align:center\">\\( n_i/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_j/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMA</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMC</td>\n<td style=\"text-align:center\">\\( n_i/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_j/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_{i}n_{j}/(n_i + n_j)^{2} \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMC</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/4 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Ward</td>\n<td style=\"text-align:center\">\\( (n_k + n_i)/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( (n_k + n_j)/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( n_k/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n</tbody>\n</table>\n<p>其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。</p>\n<h2 id=\"Naive-算法\"><a href=\"#Naive-算法\" class=\"headerlink\" title=\"Naive 算法\"></a>Naive 算法</h2><p>给定数据集 \\( {\\bf{X}} = ({\\bf x}_1, {\\bf x}_2, …, {\\bf x}_n) \\)，Agglomerative 层次聚类最简单的实现方法分为以下几步：</p>\n<ol>\n<li>初始时每个样本为一个 <code>cluster</code>，计算距离矩阵 \\( \\bf D \\)，其中元素 \\( D_{ij} \\) 为样本点 \\( {\\bf x}_i \\)  和 \\( {\\bf x}_j \\) 之间的距离；</li>\n<li>遍历距离矩阵 \\( \\bf D \\)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 <code>cluster</code> 的编号，将这两个 <code>cluster</code> 合并为一个新的 <code>cluster</code> 并依据 Lance-Williams 方法更新距离矩阵 \\( \\bf D \\) （删除这两个 <code>cluster</code> 对应的行和列，并把由新  <code>cluster</code> 所算出来的距离向量插入 \\( \\bf D \\) 中），存储本次合并的相关信息；</li>\n<li>重复 2 的过程，直至最终只剩下一个 <code>cluster</code> 。</li>\n</ol>\n<p>当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。</p>\n<p>可以看到，该 Naive 算法的时间复杂度为 \\( O(n^{3}) \\) （由于每次合并两个 <code>cluster</code> 时都要遍历大小为  \\( O(n^{2}) \\) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \\( n - 1 \\) 次），空间复杂度为 \\( O(n^{2}) \\) （由于要存储距离矩阵）。</p>\n<p>当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \\( O(n^{2} ) \\) 或更低，例如 <a href=\"https://www.cs.ucsb.edu/~veronika/MAE/SLINK_sibson.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">SLINK</a> 算法（Single-link 方法），<a href=\"https://watermark.silverchair.com/200364.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAaswggGnBgkqhkiG9w0BBwagggGYMIIBlAIBADCCAY0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMf_lVS5Dqd4euptUDAgEQgIIBXoHBDW9Uw-YT4wPpKAKR0gkqiihYSy4CLlK_Pm1lZz5SR-2MJyucOW9m523Gx4wPgLpuVr-mcSnq5BOMQRb3RP4QmxEWnImOOXip-Bxz0TGys7iiYn9Ie0Of_9xoR3xmZP8UJY4uVinVSFzPLVlwVpusPrEFjcLABmjjbz5dmGR10kHHrlvtTiS0imCnPKNiZ3zdA8wMNWVM0mOABq4cxCnicAo5iI7zmgIEoYfi23sR6lQ0Y-77ohbWBcm61XMJJh-OTl-xmY6fedf9LdtgcKBiReB1HGH6U6SEDx3cW7I3J9c33YM_cRkFwz1WIkeZI5_fDc-z3Mc_f7c9y0L_zHXYnN1pgQvGTEKeRy1h_w9KjG3D2UKl9JRrE3Kflw_zS8TVGFi8laoU2ZyujHJLLr7F_gsfpyIEDXkelxdVAF2Srnkz_Bk4FhRcsKzx7AspBEFrGbJtDBBZz0uHbvGO\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">CLINK</a> 算法（Complete-link 方法），<a href=\"http://www.cs.uvm.edu/~xwu/kdd/BIRCH.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">BIRCH</a> 算法（适用于 Euclidean 距离准则）等等。</p>\n<hr>\n<h1 id=\"利用-Scipy-实现层次聚类\"><a href=\"#利用-Scipy-实现层次聚类\" class=\"headerlink\" title=\"利用 Scipy 实现层次聚类\"></a>利用 Scipy 实现层次聚类</h1><p>在这里我们将利用 <a href=\"https://www.scipy.org/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">SciPy</a>（python 中的一个用于数值分析和科学计算的第三方包，功能强大，<a href=\"http://www.numpy.org/\" title=\"NumPy\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">NumPy</a>+SciPy+<a href=\"https://matplotlib.org/index.html\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">matplotlib</a> 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。</p>\n<h2 id=\"生成实验样本集\"><a href=\"#生成实验样本集\" class=\"headerlink\" title=\"生成实验样本集\"></a>生成实验样本集</h2><p>首先，我们需要导入相关的模块，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> scipy.cluster.hierarchy <span class=\"keyword\">import</span> linkage, dendrogram, fcluster</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div></pre></td></tr></table></figure></p>\n<p>其中 <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>make_blobs</code></a> 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 <code>cluster</code> 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 <code>X</code> 的维度为 <code>n*d</code>。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>centers = [[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">-1</span>, <span class=\"number\">-1</span>], [<span class=\"number\">1</span>, <span class=\"number\">-1</span>]]    <span class=\"comment\"># 定义 3 个中心点</span></div><div class=\"line\"><span class=\"comment\"># 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>X, labels_true = make_blobs(n_samples=<span class=\"number\">750</span>, centers=centers, cluster_std=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>）  </div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=<span class=\"string\">'b'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'The dataset'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>样本的分布如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/the_dataset_one_for_hierarchy_clustering.png\" width=\"780\" height=\"650\" alt=\"数据集分布图\" align=\"center\"><br></div>\n\n<h2 id=\"进行-Agglomerative-层次聚类\"><a href=\"#进行-Agglomerative-层次聚类\" class=\"headerlink\" title=\"进行 Agglomerative 层次聚类\"></a>进行 Agglomerative 层次聚类</h2><p>SciPy 里面进行层次聚类非常简单，直接调用 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>linkage</code></a> 函数，一行代码即可搞定。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>Z = linkage(X,  method=<span class=\"string\">'ward'</span>, metric=<span class=\"string\">'euclidean'</span>)</div></pre></td></tr></table></figure></p>\n<p>以上即进行了一次 <code>cluster</code> 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 <code>method</code> 参数可以为 <code>&#39;single&#39;</code>、 <code>&#39;complete&#39;</code> 、<code>&#39;average&#39;</code>、 <code>&#39;weighted&#39;</code>、 <code>&#39;centroid&#39;</code>、 <code>&#39;median&#39;</code>、 <code>&#39;ward&#39;</code> 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 <code>metric</code> 参数调整。</p>\n<p><code>linkage</code> 函数的返回值 <code>Z</code> 为一个维度 <code>(n-1)*4</code> 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 <code>4</code> 个值分别对应合并的两个 <code>cluster</code> 的序号、两个 <code>cluster</code> 之间的距离以及本次合并后产生的新的 <code>cluster</code> 所包含的样本点的个数；具体地，对于第 <code>i</code> 次迭代（对应 <code>Z</code> 的第 <code>i</code> 行），序号为 <code>Z[i, 0]</code> 和序号为 <code>Z[i, 1]</code> 的 <code>cluster</code> 合并产生新的 <code>cluster</code> <code>n + i</code>, <code>Z[i, 2]</code> 为序号为 <code>Z[i, 0]</code> 和序号为 <code>Z[i, 1]</code> 的 <code>cluster</code> 之间的距离，合并后的 <code>cluster</code> 包含 <code>Z[i, 3]</code> 个样本点。</p>\n<p>例如本次实验中 <code>Z</code> 记录到的前 <code>25</code> 次合并的信息如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(Z.shape)</div><div class=\"line\">(<span class=\"number\">749</span>, <span class=\"number\">4</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(Z[: <span class=\"number\">25</span>])</div><div class=\"line\">[[<span class=\"number\">253.</span>      <span class=\"number\">491.</span>        <span class=\"number\">0.00185</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">452.</span>      <span class=\"number\">696.</span>        <span class=\"number\">0.00283</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">70.</span>      <span class=\"number\">334.</span>        <span class=\"number\">0.00374</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">237.</span>      <span class=\"number\">709.</span>        <span class=\"number\">0.00378</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">244.</span>      <span class=\"number\">589.</span>        <span class=\"number\">0.00423</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">141.</span>      <span class=\"number\">550.</span>        <span class=\"number\">0.00424</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">195.</span>      <span class=\"number\">672.</span>        <span class=\"number\">0.00431</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">71.</span>      <span class=\"number\">102.</span>        <span class=\"number\">0.00496</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">307.</span>      <span class=\"number\">476.</span>        <span class=\"number\">0.00536</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">351.</span>      <span class=\"number\">552.</span>        <span class=\"number\">0.00571</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">62.</span>      <span class=\"number\">715.</span>        <span class=\"number\">0.00607</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">98.</span>      <span class=\"number\">433.</span>        <span class=\"number\">0.0065</span>    <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">255.</span>      <span class=\"number\">572.</span>        <span class=\"number\">0.00671</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">437.</span>      <span class=\"number\">699.</span>        <span class=\"number\">0.00685</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">55.</span>      <span class=\"number\">498.</span>        <span class=\"number\">0.00765</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">143.</span>      <span class=\"number\">734.</span>        <span class=\"number\">0.00823</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">182.</span>      <span class=\"number\">646.</span>        <span class=\"number\">0.00843</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">45.</span>      <span class=\"number\">250.</span>        <span class=\"number\">0.0087</span>    <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">298.</span>      <span class=\"number\">728.</span>        <span class=\"number\">0.00954</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">580.</span>      <span class=\"number\">619.</span>        <span class=\"number\">0.01033</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">179.</span>      <span class=\"number\">183.</span>        <span class=\"number\">0.01062</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">101.</span>      <span class=\"number\">668.</span>        <span class=\"number\">0.01079</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">131.</span>      <span class=\"number\">544.</span>        <span class=\"number\">0.01125</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">655.</span>      <span class=\"number\">726.</span>        <span class=\"number\">0.01141</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">503.</span>      <span class=\"number\">756.</span>        <span class=\"number\">0.01265</span>   <span class=\"number\">3.</span>     ]]</div></pre></td></tr></table></figure></p>\n<p>从上面的信息可以看到，在第 <code>6</code> 次合并中，样本点 <code>141</code> 与样本点 <code>550</code> 进行了合并，生成新 <code>cluster</code> <code>756</code>；在第 25 次合并中，样本点 <code>503</code> 与 <code>cluster</code> <code>756</code> 进行了合并，生成新的 <code>cluster</code> <code>770</code>。我们可以将样本点 <code>141</code>、<code>550</code> 和 <code>503</code> 的特征信息打印出来，来看看它们是否确实很接近。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(X[[<span class=\"number\">141</span>, <span class=\"number\">550</span>, <span class=\"number\">503</span>]])</div><div class=\"line\">[[ <span class=\"number\">1.27098</span> <span class=\"number\">-0.97927</span>]</div><div class=\"line\"> [ <span class=\"number\">1.27515</span> <span class=\"number\">-0.98006</span>]</div><div class=\"line\"> [ <span class=\"number\">1.37274</span>  <span class=\"number\">1.13599</span>]]</div></pre></td></tr></table></figure></p>\n<p>看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 <code>cluster</code> 中相隔最近的。</p>\n<h2 id=\"画出树形图\"><a href=\"#画出树形图\" class=\"headerlink\" title=\"画出树形图\"></a>画出树形图</h2><p>SciPy 中给出了根据层次聚类的结果 <code>Z</code> 绘制树形图的函数 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>dendrogram</code></a>，我们由此画出本次实验中的最后 <code>20</code> 次的合并过程。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>dendrogram(Z, truncate_mode=<span class=\"string\">'lastp'</span>, p=<span class=\"number\">20</span>, show_leaf_counts=<span class=\"keyword\">False</span>, leaf_rotation=<span class=\"number\">90</span>, leaf_font_size=<span class=\"number\">15</span>, show_contracted=<span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'Dendrogram for the Agglomerative Clustering'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.xlabel(<span class=\"string\">'sample index'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.ylabel(<span class=\"string\">'distance'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>得到的树形图如下所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dendrogram_for_the_agglomerative_clustering.png\" width=\"780\" height=\"650\" alt=\"程序绘制出的树形图\" align=\"center\"><br></div>\n\n<p>可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 <code>3</code> 个 <code>cluster</code>（和实际相符），分别在上图中由三种颜色所表示。</p>\n<h2 id=\"获取聚类结果\"><a href=\"#获取聚类结果\" class=\"headerlink\" title=\"获取聚类结果\"></a>获取聚类结果</h2><p>在得到了层次聚类的过程信息 <code>Z</code> 后，我们可以使用 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>fcluster</code></a> 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 <code>d</code>，得到在该距离以下的未合并的所有 <code>cluster</code> 作为聚类结果；另一个是指定 <code>cluster</code> 的数量 <code>k</code>，函数会返回最后的 <code>k</code> 个 <code>cluster</code> 作为聚类结果。使用哪个维度由参数 <code>criterion</code> 决定，对应的临界距离或聚类的数量则由参数 <code>t</code> 所记录。<code>fcluster</code> 函数的结果为一个一维数组，记录每个样本的类别信息。</p>\n<p>对应的代码与返回结果如下所示。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 根据临界距离返回聚类结果</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = <span class=\"number\">15</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>labels_1 = fcluster(Z, t=d, criterion=<span class=\"string\">'distance'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(labels_1[: <span class=\"number\">100</span>])    <span class=\"comment\"># 打印聚类结果</span></div><div class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span></div><div class=\"line\"> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span></div><div class=\"line\"> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(len(set(labels_1)))   <span class=\"comment\"># 看看在该临界距离下有几个 cluster</span></div><div class=\"line\"><span class=\"number\">3</span>   </div><div class=\"line\"><span class=\"comment\"># 根据聚类数目返回聚类结果</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>k = <span class=\"number\">3</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>labels_2 = fcluster(Z, t=k, criterion=<span class=\"string\">'maxclust'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(labels_2[: <span class=\"number\">100</span>])  </div><div class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span></div><div class=\"line\"> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span></div><div class=\"line\"> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>list(labels_1) == list(labels_2)      <span class=\"comment\"># 看看两种不同维度下得到的聚类结果是否一致</span></div><div class=\"line\"><span class=\"keyword\">True</span></div></pre></td></tr></table></figure></p>\n<p>下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'The Result of the Agglomerative Clustering'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=labels_2, cmap=<span class=\"string\">'prism'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>可视化结果如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/result_of_agglomerative_clustering.png\" width=\"780\" height=\"650\" alt=\"层次聚类结果\" align=\"center\"><br></div>\n\n<p>上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。</p>\n<h2 id=\"比较不同方法下的聚类结果\"><a href=\"#比较不同方法下的聚类结果\" class=\"headerlink\" title=\"比较不同方法下的聚类结果\"></a>比较不同方法下的聚类结果</h2><p>最后，我们对同一份样本集进行了 <code>cluster</code> 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 <code>3</code>，程序如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> time <span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">from</span> scipy.cluster.hierarchy <span class=\"keyword\">import</span> linkage, fcluster</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics.cluster <span class=\"keyword\">import</span> adjusted_mutual_info_score</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 生成样本点</span></div><div class=\"line\">centers = [[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">-1</span>, <span class=\"number\">-1</span>], [<span class=\"number\">1</span>, <span class=\"number\">-1</span>]]</div><div class=\"line\">X, labels = make_blobs(n_samples=<span class=\"number\">750</span>, centers=centers,</div><div class=\"line\">                        cluster_std=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 可视化聚类结果</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(X, labels, title=None)</span>:</span></div><div class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=labels, cmap=<span class=\"string\">'prism'</span>)</div><div class=\"line\">    <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        plt.title(title, size=<span class=\"number\">17</span>)</div><div class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</div><div class=\"line\">    plt.tight_layout()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 进行 Agglomerative 层次聚类</span></div><div class=\"line\">linkage_method_list = [<span class=\"string\">'single'</span>, <span class=\"string\">'complete'</span>, <span class=\"string\">'average'</span>, <span class=\"string\">'ward'</span>]</div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\">ncols, nrows = <span class=\"number\">2</span>, int(np.ceil(len(linkage_method_list) / <span class=\"number\">2</span>))</div><div class=\"line\">plt.subplots(nrows=nrows, ncols=ncols)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i, linkage_method <span class=\"keyword\">in</span> enumerate(linkage_method_list):</div><div class=\"line\">    print(<span class=\"string\">'method %s:'</span> % linkage_method)</div><div class=\"line\">    start_time = time()</div><div class=\"line\">    Z = linkage(X, method=linkage_method)</div><div class=\"line\">    labels_pred = fcluster(Z, t=<span class=\"number\">3</span>, criterion=<span class=\"string\">'maxclust'</span>)</div><div class=\"line\">    print(<span class=\"string\">'Adjust mutual information: %.3f'</span> %</div><div class=\"line\">            adjusted_mutual_info_score(labels, labels_pred))</div><div class=\"line\">    print(<span class=\"string\">'time used: %.3f seconds'</span> % (time() - start_time))</div><div class=\"line\">    plt.subplot(nrows, ncols, i + <span class=\"number\">1</span>)</div><div class=\"line\">    plot_clustering(X, labels_pred, <span class=\"string\">'%s linkage'</span> % linkage_method)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>可以得到 4 种方法下的聚类结果如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/aggm_clust_linkage_comparing.png\" width=\"780\" height=\"650\" alt=\"不同方法下的聚类结果\" align=\"center\"><br></div>\n\n<p>在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 <a href=\"\">AMI</a>()（Adjust Mutual Information）量，该量越接近于 <code>1</code> 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">method single:</div><div class=\"line\">Adjust mutual information: 0.001</div><div class=\"line\">time used: 0.008 seconds</div><div class=\"line\">method complete:</div><div class=\"line\">Adjust mutual information: 0.838</div><div class=\"line\">time used: 0.013 seconds</div><div class=\"line\">method average:</div><div class=\"line\">Adjust mutual information: 0.945</div><div class=\"line\">time used: 0.019 seconds</div><div class=\"line\">method ward:</div><div class=\"line\">Adjust mutual information: 0.956</div><div class=\"line\">time used: 0.015 seconds</div></pre></td></tr></table></figure>\n<p>从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 <code>cluster</code>，而其他两个 <code>cluster</code> 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"聚类算法综述\"><a href=\"#聚类算法综述\" class=\"headerlink\" title=\"聚类算法综述\"></a>聚类算法综述</h1><p>聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 <code>cluster</code>，使得同一 <code>cluster</code> 内的对象在某种意义上比不同的 <code>cluster</code> 之间的对象更为相似。</p>\n<p>由于 “<code>cluster</code>” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类：</p>\n<ul>\n<li>基于<strong>连通模型</strong>（connectivity-based）的聚类算法： 即本文将要讲述的<strong>层次聚类</strong>算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 <code>cluster</code>。</li>\n<li>基于<strong>中心点模型</strong>（centroid-based）的聚类算法： 在此类算法中，每个 <code>cluster</code> 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 <code>k</code> 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 <code>cluster</code> 的中心点的距离的平方和，优化变量为每个 <code>cluster</code> 的中心点以及每个对象属于哪个 <code>cluster</code>；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，<a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95/\"> k-means 算法 </a>即是其中的一种。</li>\n<li>基于<strong>分布模型</strong>（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 <code>cluster</code> 即可，最常被使用的此类算法为<a href=\"../%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95/\"> 高斯混合模型（GMM）聚类</a>。</li>\n<li>基于<strong>密度</strong>（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 <code>cluster</code>，<code>cluster</code> 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。</li>\n</ul>\n<hr>\n<h1 id=\"层次聚类综述\"><a href=\"#层次聚类综述\" class=\"headerlink\" title=\"层次聚类综述\"></a>层次聚类综述</h1><p>层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 <code>clusters</code>，后面一层生成的 <code>clusters</code> 基于前面一层的结果。层次聚类算法一般分为两类：</p>\n<ul>\n<li>Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 <code>cluster</code>，每次按一定的准则将最相近的两个 <code>cluster</code> 合并生成一个新的 <code>cluster</code>，如此往复，直至最终所有的对象都属于一个 <code>cluster</code>。本文主要关注此类算法。</li>\n<li>Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 <code>cluster</code>，每次按一定的准则将某个 <code>cluster</code> 划分为多个 <code>cluster</code>，如此往复，直至每个对象均是一个 <code>cluster</code>。</li>\n</ul>\n<p>下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_for_hierarchical_clustering.PNG\" width=\"560\" height=\"400\" ==\"\" \"层次聚类图示\"=\"\" align=\"center\"><br></div>\n\n<p>另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。</p>\n<h2 id=\"树形图\"><a href=\"#树形图\" class=\"headerlink\" title=\"树形图\"></a>树形图</h2><p><a href=\"https://en.wikipedia.org/wiki/Dendrogram\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> 树形图 </a>（dendrogram）可以用来直观地表示层次聚类的成果。一个有 <code>5</code> 个点的树形图如下图所示，其中纵坐标高度表示不同的 <code>cluster</code> 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\\( x_1 \\) 和 \\( x_2 \\) 的距离最近（为 <code>1</code>），因此将 \\( x_1 \\) 和 \\( x_2 \\) 合并为一个 <code>cluster</code> \\( (x_1, x_2) \\)，所以在树形图中首先将节点 \\( x_1 \\) 和 \\( x_2 \\) 连接，使其成为一个新的节点  \\( (x_1, x_2) \\) 的子节点，并将这个新的节点的高度置为 <code>1</code>；之后再在剩下的 <code>4</code> 个 <code>cluster</code> \\( (x_1, x_2) \\)， \\( x_3 \\)， \\( x_4 \\) 和 \\( x_5 \\) 中选取距离最近的两个 <code>cluster</code> 合并，\\( x_4 \\) 和 \\( x_5 \\) 的距离最近（为 <code>2</code>），因此将 \\( x_4 \\) 和 \\( x_5 \\) 合并为一个 cluster \\( (x_4, x_5) \\)，体现在树形图上，是将节点 \\( x_4 \\) 和 \\( x_5 \\) 连接，使其成为一个新的节点 \\( (x_4, x_5) \\) 的子节点，并将此新节点的高度置为 <code>2</code>；….依此模式进行树形图的生成，直至最终只剩下一个 <code>cluster</code> \\( ((x_1, x_2), x_3), (x_4, x_5)) \\)。</p>\n<p>可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 <code>cluster</code> 之间的距离都不大于 \\( h \\)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \\( h \\)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \\( h=2.5 \\)，即可得到 <code>3</code> 个 <code>cluster</code> \\( (x_1, x_2) \\)， \\( x_3 \\) 和 \\( (x_4, x_5) \\)。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/a_dendrogram.png\" width=\"550\" height=\"440\" alt=\"树形图示例\" align=\"center\"><br></div>\n\n<h2 id=\"对象之间的距离衡量\"><a href=\"#对象之间的距离衡量\" class=\"headerlink\" title=\"对象之间的距离衡量\"></a>对象之间的距离衡量</h2><p>衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \\( d \\)  维空间的两个对象 \\({\\bf x} =[ x_1, x_2, …, x_d]^{T} \\) 和 \\({\\bf y} = [y_1, y_2, …, y_d]^{T}\\)，其在不同距离准则下的距离计算方法如下表所示:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">距离准则</th>\n<th style=\"text-align:center\">距离计算方法</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Euclidean 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = [\\sum_{j=1}^{d} (x_j-y_j)^{2} ]^{\\frac{1}{2}} = [({\\bf x} - {\\bf y})^{T} ({\\bf x} - {\\bf y})]^{\\frac{1}{2}} \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Manhattan 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = \\sum_{j=1}^{d} \\mid{x_j-y_j}\\mid \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Chebyshev 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = \\max_{1\\leq{j}\\leq{d}} \\mid{x_j-y_j}\\mid \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Minkowski 距离</td>\n<td style=\"text-align:center\">\\( d({\\bf x},{\\bf y}) = [\\sum_{j=1}^{d} (x_j-y_j)^{p} ]^{\\frac{1}{p}}, p\\geq{1} \\)</td>\n</tr>\n</tbody>\n</table>\n<p>Minkowski 距离就是 \\( \\rm{L}\\it{p}\\) 范数（\\( p\\geq{1} \\))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \\( p = 1, 2, \\infty \\) 时的情形。</p>\n<p>另一种常用的距离是 Maholanobis 距离，其定义如下：<br>$$ d_{mah}({\\bf x}, {\\bf y}) = \\sqrt{({\\bf x} - {\\bf y})^{T}\\Sigma^{-1} ({\\bf x} - {\\bf y})} $$<br>其中 \\( \\Sigma \\) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \\( {\\bf{X}} = ({\\bf x}_1, {\\bf x}_2, …, {\\bf x}_n) \\in \\Bbb{R}^{d \\times n}\\)，\\( {\\bf x}_i \\in \\Bbb{R}^{d} \\) 为第 \\( i \\) 个样本点，每个样本点的维度为 \\( d \\)，样本点的总数为 \\( n \\) 个；再假设样本点的平均值 \\( m_{\\bf x} = \\frac{1}{n}\\sum_{i=1}^{n} {\\bf x}_i \\) 为 \\( {\\bf 0} \\) 向量（若不为 \\( {\\bf 0} \\)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \\( \\Sigma \\in \\Bbb{R}^{d \\times d} \\) 可被定义为<br>$$ \\Sigma = \\frac{1}{n} {\\bf X}{\\bf X}^{T} $$<br>Maholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。</p>\n<h2 id=\"Cluster-之间的距离衡量\"><a href=\"#Cluster-之间的距离衡量\" class=\"headerlink\" title=\"Cluster 之间的距离衡量\"></a>Cluster 之间的距离衡量</h2><p>除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 <code>cluster</code> 之间的距离，常见的 <code>cluster</code> 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，<code>cluster</code> 是由样本点或一些子 <code>cluster</code> （这些样本点或子 <code>cluster</code> 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 <code>cluster</code> 。假设 \\( C_i \\) 和 \\( C_j \\) 为两个 cluster，则前四种方法定义的  \\( C_i \\) 和 \\( C_j \\) 之间的距离如下表所示：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">定义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Single-link</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\min_{ {\\bf x} \\in C_i, {\\bf y} \\in C_j } d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Complete-link</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\max_{ {\\bf x} \\in C_i, {\\bf y} \\in C_j} d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMA</td>\n<td style=\"text-align:center\">\\( D(C_i, C_j) = \\frac{1}{\\mid C_i \\mid \\mid C_j \\mid} \\sum_ { {\\bf x} \\in C_i, {\\bf y} \\in C_j}  d({\\bf x}, {\\bf y}) \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMA</td>\n<td style=\"text-align:center\">omitting</td>\n</tr>\n</tbody>\n</table>\n<p>其中 Single-link 定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 <code>cluster</code>；而 Complete-link 则定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 <code>cluster</code> 之间的距离为两个 <code>cluster</code> 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 <code>cluster</code> 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 <code>cluster</code> 对距离的计算的影响在同一层次上，而不受 <code>cluster</code> 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 <code>cluster</code> 之间的距离，而是通过已有的 <code>cluster</code> 之间的距离来计算合并后的新的 <code>cluster</code> 和剩余 <code>cluster</code> 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。</p>\n<p>Centroid/UPGMC 方法给每一个 <code>cluster</code> 计算一个质心，两个 <code>cluster</code> 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：<br>$$ D_{\\rm{UPGMC}}(C_i, C_j) =  \\frac{1}{\\mid C_i \\mid \\mid C_j \\mid} \\sum_ { {\\bf x} \\in C_i, {\\bf y}\\in C_j}  d({\\bf x}, {\\bf y}) - \\frac{1}{2{\\mid C_i \\mid }^{2}} \\sum_ { {\\bf x}, {\\bf y}\\in C_i}  d( {\\bf x}, {\\bf y}) - \\frac{1}{2{\\mid \\it{C_j} \\mid }^{2}} \\sum_ { {\\bf x}, {\\bf y} \\in C_j}  d( {\\bf x}, {\\bf y}) $$<br>当上式中的 \\( d(.,.) \\) 为平方 Euclidean 距离时，\\( D_{\\rm{UPGMC}}(C_i, C_j) \\) 为 \\( C_i \\) 和 \\( C_j \\) 的中心点（每个 <code>cluster</code> 内所有样本点之间的平均值）之间的平方 Euclidean 距离。</p>\n<p>Median/WPGMC 方法为每个 <code>cluster</code> 计算质心时，引入了权重。</p>\n<p>Ward 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 <code>cluster</code> 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，<code>cluster</code> \\( C \\) 的 \\( {\\rm ESS} \\) 定义如下：<br>$$  {\\rm ESS} ( C ) = \\sum_{ {\\bf x} \\in C} ({\\bf x} - m_{\\bf x})^{T} ({\\bf x} - m_{\\bf x}) $$<br>其中 \\( m_{\\bf x} \\) 为 \\( C \\) 中样本点的均值。可以看到 \\( {\\rm ESS} \\) 衡量的是一个 <code>cluster</code> 内的样本点的聚合程度，样本点越聚合，\\( {\\rm ESS} \\) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 <code>cluster</code> 的 \\( {\\rm ESS} \\) 之和相对于合并前的 <code>cluster</code> 的 \\( {\\rm ESS} \\) 之和的增长最小。</p>\n<hr>\n<h1 id=\"Agglomerative-层次聚类算法\"><a href=\"#Agglomerative-层次聚类算法\" class=\"headerlink\" title=\"Agglomerative 层次聚类算法\"></a>Agglomerative 层次聚类算法</h1><p>这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 <code>cluster</code> 之间的距离的 Lance-Williams 方法。</p>\n<h2 id=\"Lance-Williams-方法\"><a href=\"#Lance-Williams-方法\" class=\"headerlink\" title=\"Lance-Williams 方法\"></a>Lance-Williams 方法</h2><p>在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 <code>cluster</code> 合并为一个新的 <code>cluster</code>，然后再计算这个新的 <code>cluster</code> 与其他当前未被合并的 <code>cluster</code> 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 <code>cluster</code> 距离衡量方法都适用。具体地，对于三个 <code>cluster</code> \\( C_k \\)，\\( C_i \\) 和 \\( C_j \\)， Lance-Williams 给出的 \\( C_k \\) 与 \\( C_i \\) 和 \\( C_j \\) 合并后的新 <code>cluster</code>  之间的距离的计算方法如下式所示：<br>$$  D(C_k, C_i \\cup C_j) = \\alpha_i D(C_k, C_i) + \\alpha_j D(C_k, C_j) + \\beta D(C_i, C_j) +  \\gamma \\mid D(C_k, C_i) - D(C_k, C_j) \\mid $$<br>其中，\\( \\alpha_i \\)，\\( \\alpha_j \\)，\\( \\beta \\)，\\( \\gamma \\) 均为参数，随 <code>cluster</code> 之间的距离计算方法的不同而不同，具体总结为下表（注：\\( n_i \\) 为 <code>cluster</code> \\( C_i \\) 中的样本点的个数)：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:center\">参数 \\( \\alpha_i \\)</th>\n<th style=\"text-align:center\">参数 \\( \\alpha_j \\)</th>\n<th style=\"text-align:center\">参数 \\( \\beta \\)</th>\n<th style=\"text-align:center\">参数 \\( \\gamma \\)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Single-link</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( -1/2 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Complete-link</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMA</td>\n<td style=\"text-align:center\">\\( n_i/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_j/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMA</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">UPGMC</td>\n<td style=\"text-align:center\">\\( n_i/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_j/(n_i + n_j) \\)</td>\n<td style=\"text-align:center\">\\( n_{i}n_{j}/(n_i + n_j)^{2} \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WPGMC</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/2 \\)</td>\n<td style=\"text-align:center\">\\( 1/4 \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Ward</td>\n<td style=\"text-align:center\">\\( (n_k + n_i)/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( (n_k + n_j)/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( n_k/(n_i + n_j + n_k) \\)</td>\n<td style=\"text-align:center\">\\( 0 \\)</td>\n</tr>\n</tbody>\n</table>\n<p>其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。</p>\n<h2 id=\"Naive-算法\"><a href=\"#Naive-算法\" class=\"headerlink\" title=\"Naive 算法\"></a>Naive 算法</h2><p>给定数据集 \\( {\\bf{X}} = ({\\bf x}_1, {\\bf x}_2, …, {\\bf x}_n) \\)，Agglomerative 层次聚类最简单的实现方法分为以下几步：</p>\n<ol>\n<li>初始时每个样本为一个 <code>cluster</code>，计算距离矩阵 \\( \\bf D \\)，其中元素 \\( D_{ij} \\) 为样本点 \\( {\\bf x}_i \\)  和 \\( {\\bf x}_j \\) 之间的距离；</li>\n<li>遍历距离矩阵 \\( \\bf D \\)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 <code>cluster</code> 的编号，将这两个 <code>cluster</code> 合并为一个新的 <code>cluster</code> 并依据 Lance-Williams 方法更新距离矩阵 \\( \\bf D \\) （删除这两个 <code>cluster</code> 对应的行和列，并把由新  <code>cluster</code> 所算出来的距离向量插入 \\( \\bf D \\) 中），存储本次合并的相关信息；</li>\n<li>重复 2 的过程，直至最终只剩下一个 <code>cluster</code> 。</li>\n</ol>\n<p>当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。</p>\n<p>可以看到，该 Naive 算法的时间复杂度为 \\( O(n^{3}) \\) （由于每次合并两个 <code>cluster</code> 时都要遍历大小为  \\( O(n^{2}) \\) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \\( n - 1 \\) 次），空间复杂度为 \\( O(n^{2}) \\) （由于要存储距离矩阵）。</p>\n<p>当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \\( O(n^{2} ) \\) 或更低，例如 <a href=\"https://www.cs.ucsb.edu/~veronika/MAE/SLINK_sibson.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">SLINK</a> 算法（Single-link 方法），<a href=\"https://watermark.silverchair.com/200364.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAaswggGnBgkqhkiG9w0BBwagggGYMIIBlAIBADCCAY0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMf_lVS5Dqd4euptUDAgEQgIIBXoHBDW9Uw-YT4wPpKAKR0gkqiihYSy4CLlK_Pm1lZz5SR-2MJyucOW9m523Gx4wPgLpuVr-mcSnq5BOMQRb3RP4QmxEWnImOOXip-Bxz0TGys7iiYn9Ie0Of_9xoR3xmZP8UJY4uVinVSFzPLVlwVpusPrEFjcLABmjjbz5dmGR10kHHrlvtTiS0imCnPKNiZ3zdA8wMNWVM0mOABq4cxCnicAo5iI7zmgIEoYfi23sR6lQ0Y-77ohbWBcm61XMJJh-OTl-xmY6fedf9LdtgcKBiReB1HGH6U6SEDx3cW7I3J9c33YM_cRkFwz1WIkeZI5_fDc-z3Mc_f7c9y0L_zHXYnN1pgQvGTEKeRy1h_w9KjG3D2UKl9JRrE3Kflw_zS8TVGFi8laoU2ZyujHJLLr7F_gsfpyIEDXkelxdVAF2Srnkz_Bk4FhRcsKzx7AspBEFrGbJtDBBZz0uHbvGO\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">CLINK</a> 算法（Complete-link 方法），<a href=\"http://www.cs.uvm.edu/~xwu/kdd/BIRCH.pdf\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">BIRCH</a> 算法（适用于 Euclidean 距离准则）等等。</p>\n<hr>\n<h1 id=\"利用-Scipy-实现层次聚类\"><a href=\"#利用-Scipy-实现层次聚类\" class=\"headerlink\" title=\"利用 Scipy 实现层次聚类\"></a>利用 Scipy 实现层次聚类</h1><p>在这里我们将利用 <a href=\"https://www.scipy.org/\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">SciPy</a>（python 中的一个用于数值分析和科学计算的第三方包，功能强大，<a href=\"http://www.numpy.org/\" title=\"NumPy\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">NumPy</a>+SciPy+<a href=\"https://matplotlib.org/index.html\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">matplotlib</a> 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。</p>\n<h2 id=\"生成实验样本集\"><a href=\"#生成实验样本集\" class=\"headerlink\" title=\"生成实验样本集\"></a>生成实验样本集</h2><p>首先，我们需要导入相关的模块，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># python 3.6</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> scipy.cluster.hierarchy <span class=\"keyword\">import</span> linkage, dendrogram, fcluster</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div></pre></td></tr></table></figure></p>\n<p>其中 <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>make_blobs</code></a> 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 <code>cluster</code> 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 <code>X</code> 的维度为 <code>n*d</code>。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>centers = [[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">-1</span>, <span class=\"number\">-1</span>], [<span class=\"number\">1</span>, <span class=\"number\">-1</span>]]    <span class=\"comment\"># 定义 3 个中心点</span></div><div class=\"line\"><span class=\"comment\"># 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>X, labels_true = make_blobs(n_samples=<span class=\"number\">750</span>, centers=centers, cluster_std=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>）  </div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=<span class=\"string\">'b'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'The dataset'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>样本的分布如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/the_dataset_one_for_hierarchy_clustering.png\" width=\"780\" height=\"650\" alt=\"数据集分布图\" align=\"center\"><br></div>\n\n<h2 id=\"进行-Agglomerative-层次聚类\"><a href=\"#进行-Agglomerative-层次聚类\" class=\"headerlink\" title=\"进行 Agglomerative 层次聚类\"></a>进行 Agglomerative 层次聚类</h2><p>SciPy 里面进行层次聚类非常简单，直接调用 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>linkage</code></a> 函数，一行代码即可搞定。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>Z = linkage(X,  method=<span class=\"string\">'ward'</span>, metric=<span class=\"string\">'euclidean'</span>)</div></pre></td></tr></table></figure></p>\n<p>以上即进行了一次 <code>cluster</code> 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 <code>method</code> 参数可以为 <code>&#39;single&#39;</code>、 <code>&#39;complete&#39;</code> 、<code>&#39;average&#39;</code>、 <code>&#39;weighted&#39;</code>、 <code>&#39;centroid&#39;</code>、 <code>&#39;median&#39;</code>、 <code>&#39;ward&#39;</code> 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 <code>metric</code> 参数调整。</p>\n<p><code>linkage</code> 函数的返回值 <code>Z</code> 为一个维度 <code>(n-1)*4</code> 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 <code>4</code> 个值分别对应合并的两个 <code>cluster</code> 的序号、两个 <code>cluster</code> 之间的距离以及本次合并后产生的新的 <code>cluster</code> 所包含的样本点的个数；具体地，对于第 <code>i</code> 次迭代（对应 <code>Z</code> 的第 <code>i</code> 行），序号为 <code>Z[i, 0]</code> 和序号为 <code>Z[i, 1]</code> 的 <code>cluster</code> 合并产生新的 <code>cluster</code> <code>n + i</code>, <code>Z[i, 2]</code> 为序号为 <code>Z[i, 0]</code> 和序号为 <code>Z[i, 1]</code> 的 <code>cluster</code> 之间的距离，合并后的 <code>cluster</code> 包含 <code>Z[i, 3]</code> 个样本点。</p>\n<p>例如本次实验中 <code>Z</code> 记录到的前 <code>25</code> 次合并的信息如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(Z.shape)</div><div class=\"line\">(<span class=\"number\">749</span>, <span class=\"number\">4</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(Z[: <span class=\"number\">25</span>])</div><div class=\"line\">[[<span class=\"number\">253.</span>      <span class=\"number\">491.</span>        <span class=\"number\">0.00185</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">452.</span>      <span class=\"number\">696.</span>        <span class=\"number\">0.00283</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">70.</span>      <span class=\"number\">334.</span>        <span class=\"number\">0.00374</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">237.</span>      <span class=\"number\">709.</span>        <span class=\"number\">0.00378</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">244.</span>      <span class=\"number\">589.</span>        <span class=\"number\">0.00423</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">141.</span>      <span class=\"number\">550.</span>        <span class=\"number\">0.00424</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">195.</span>      <span class=\"number\">672.</span>        <span class=\"number\">0.00431</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">71.</span>      <span class=\"number\">102.</span>        <span class=\"number\">0.00496</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">307.</span>      <span class=\"number\">476.</span>        <span class=\"number\">0.00536</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">351.</span>      <span class=\"number\">552.</span>        <span class=\"number\">0.00571</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">62.</span>      <span class=\"number\">715.</span>        <span class=\"number\">0.00607</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">98.</span>      <span class=\"number\">433.</span>        <span class=\"number\">0.0065</span>    <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">255.</span>      <span class=\"number\">572.</span>        <span class=\"number\">0.00671</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">437.</span>      <span class=\"number\">699.</span>        <span class=\"number\">0.00685</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">55.</span>      <span class=\"number\">498.</span>        <span class=\"number\">0.00765</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">143.</span>      <span class=\"number\">734.</span>        <span class=\"number\">0.00823</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">182.</span>      <span class=\"number\">646.</span>        <span class=\"number\">0.00843</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [ <span class=\"number\">45.</span>      <span class=\"number\">250.</span>        <span class=\"number\">0.0087</span>    <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">298.</span>      <span class=\"number\">728.</span>        <span class=\"number\">0.00954</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">580.</span>      <span class=\"number\">619.</span>        <span class=\"number\">0.01033</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">179.</span>      <span class=\"number\">183.</span>        <span class=\"number\">0.01062</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">101.</span>      <span class=\"number\">668.</span>        <span class=\"number\">0.01079</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">131.</span>      <span class=\"number\">544.</span>        <span class=\"number\">0.01125</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">655.</span>      <span class=\"number\">726.</span>        <span class=\"number\">0.01141</span>   <span class=\"number\">2.</span>     ]</div><div class=\"line\"> [<span class=\"number\">503.</span>      <span class=\"number\">756.</span>        <span class=\"number\">0.01265</span>   <span class=\"number\">3.</span>     ]]</div></pre></td></tr></table></figure></p>\n<p>从上面的信息可以看到，在第 <code>6</code> 次合并中，样本点 <code>141</code> 与样本点 <code>550</code> 进行了合并，生成新 <code>cluster</code> <code>756</code>；在第 25 次合并中，样本点 <code>503</code> 与 <code>cluster</code> <code>756</code> 进行了合并，生成新的 <code>cluster</code> <code>770</code>。我们可以将样本点 <code>141</code>、<code>550</code> 和 <code>503</code> 的特征信息打印出来，来看看它们是否确实很接近。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(X[[<span class=\"number\">141</span>, <span class=\"number\">550</span>, <span class=\"number\">503</span>]])</div><div class=\"line\">[[ <span class=\"number\">1.27098</span> <span class=\"number\">-0.97927</span>]</div><div class=\"line\"> [ <span class=\"number\">1.27515</span> <span class=\"number\">-0.98006</span>]</div><div class=\"line\"> [ <span class=\"number\">1.37274</span>  <span class=\"number\">1.13599</span>]]</div></pre></td></tr></table></figure></p>\n<p>看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 <code>cluster</code> 中相隔最近的。</p>\n<h2 id=\"画出树形图\"><a href=\"#画出树形图\" class=\"headerlink\" title=\"画出树形图\"></a>画出树形图</h2><p>SciPy 中给出了根据层次聚类的结果 <code>Z</code> 绘制树形图的函数 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>dendrogram</code></a>，我们由此画出本次实验中的最后 <code>20</code> 次的合并过程。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>dendrogram(Z, truncate_mode=<span class=\"string\">'lastp'</span>, p=<span class=\"number\">20</span>, show_leaf_counts=<span class=\"keyword\">False</span>, leaf_rotation=<span class=\"number\">90</span>, leaf_font_size=<span class=\"number\">15</span>, show_contracted=<span class=\"keyword\">True</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'Dendrogram for the Agglomerative Clustering'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.xlabel(<span class=\"string\">'sample index'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.ylabel(<span class=\"string\">'distance'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>得到的树形图如下所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/dendrogram_for_the_agglomerative_clustering.png\" width=\"780\" height=\"650\" alt=\"程序绘制出的树形图\" align=\"center\"><br></div>\n\n<p>可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 <code>3</code> 个 <code>cluster</code>（和实际相符），分别在上图中由三种颜色所表示。</p>\n<h2 id=\"获取聚类结果\"><a href=\"#获取聚类结果\" class=\"headerlink\" title=\"获取聚类结果\"></a>获取聚类结果</h2><p>在得到了层次聚类的过程信息 <code>Z</code> 后，我们可以使用 <a href=\"http://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"><code>fcluster</code></a> 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 <code>d</code>，得到在该距离以下的未合并的所有 <code>cluster</code> 作为聚类结果；另一个是指定 <code>cluster</code> 的数量 <code>k</code>，函数会返回最后的 <code>k</code> 个 <code>cluster</code> 作为聚类结果。使用哪个维度由参数 <code>criterion</code> 决定，对应的临界距离或聚类的数量则由参数 <code>t</code> 所记录。<code>fcluster</code> 函数的结果为一个一维数组，记录每个样本的类别信息。</p>\n<p>对应的代码与返回结果如下所示。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 根据临界距离返回聚类结果</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = <span class=\"number\">15</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>labels_1 = fcluster(Z, t=d, criterion=<span class=\"string\">'distance'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(labels_1[: <span class=\"number\">100</span>])    <span class=\"comment\"># 打印聚类结果</span></div><div class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span></div><div class=\"line\"> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span></div><div class=\"line\"> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(len(set(labels_1)))   <span class=\"comment\"># 看看在该临界距离下有几个 cluster</span></div><div class=\"line\"><span class=\"number\">3</span>   </div><div class=\"line\"><span class=\"comment\"># 根据聚类数目返回聚类结果</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>k = <span class=\"number\">3</span></div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>labels_2 = fcluster(Z, t=k, criterion=<span class=\"string\">'maxclust'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>print(labels_2[: <span class=\"number\">100</span>])  </div><div class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span></div><div class=\"line\"> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span></div><div class=\"line\"> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span>]</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>list(labels_1) == list(labels_2)      <span class=\"comment\"># 看看两种不同维度下得到的聚类结果是否一致</span></div><div class=\"line\"><span class=\"keyword\">True</span></div></pre></td></tr></table></figure></p>\n<p>下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.title(<span class=\"string\">'The Result of the Agglomerative Clustering'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=labels_2, cmap=<span class=\"string\">'prism'</span>)</div><div class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>plt.show()</div></pre></td></tr></table></figure></p>\n<p>可视化结果如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/result_of_agglomerative_clustering.png\" width=\"780\" height=\"650\" alt=\"层次聚类结果\" align=\"center\"><br></div>\n\n<p>上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。</p>\n<h2 id=\"比较不同方法下的聚类结果\"><a href=\"#比较不同方法下的聚类结果\" class=\"headerlink\" title=\"比较不同方法下的聚类结果\"></a>比较不同方法下的聚类结果</h2><p>最后，我们对同一份样本集进行了 <code>cluster</code> 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 <code>3</code>，程序如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">from</span> time <span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">from</span> scipy.cluster.hierarchy <span class=\"keyword\">import</span> linkage, fcluster</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics.cluster <span class=\"keyword\">import</span> adjusted_mutual_info_score</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 生成样本点</span></div><div class=\"line\">centers = [[<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">-1</span>, <span class=\"number\">-1</span>], [<span class=\"number\">1</span>, <span class=\"number\">-1</span>]]</div><div class=\"line\">X, labels = make_blobs(n_samples=<span class=\"number\">750</span>, centers=centers,</div><div class=\"line\">                        cluster_std=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 可视化聚类结果</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(X, labels, title=None)</span>:</span></div><div class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=labels, cmap=<span class=\"string\">'prism'</span>)</div><div class=\"line\">    <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">        plt.title(title, size=<span class=\"number\">17</span>)</div><div class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</div><div class=\"line\">    plt.tight_layout()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 进行 Agglomerative 层次聚类</span></div><div class=\"line\">linkage_method_list = [<span class=\"string\">'single'</span>, <span class=\"string\">'complete'</span>, <span class=\"string\">'average'</span>, <span class=\"string\">'ward'</span>]</div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">10</span>, <span class=\"number\">8</span>))</div><div class=\"line\">ncols, nrows = <span class=\"number\">2</span>, int(np.ceil(len(linkage_method_list) / <span class=\"number\">2</span>))</div><div class=\"line\">plt.subplots(nrows=nrows, ncols=ncols)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span> i, linkage_method <span class=\"keyword\">in</span> enumerate(linkage_method_list):</div><div class=\"line\">    print(<span class=\"string\">'method %s:'</span> % linkage_method)</div><div class=\"line\">    start_time = time()</div><div class=\"line\">    Z = linkage(X, method=linkage_method)</div><div class=\"line\">    labels_pred = fcluster(Z, t=<span class=\"number\">3</span>, criterion=<span class=\"string\">'maxclust'</span>)</div><div class=\"line\">    print(<span class=\"string\">'Adjust mutual information: %.3f'</span> %</div><div class=\"line\">            adjusted_mutual_info_score(labels, labels_pred))</div><div class=\"line\">    print(<span class=\"string\">'time used: %.3f seconds'</span> % (time() - start_time))</div><div class=\"line\">    plt.subplot(nrows, ncols, i + <span class=\"number\">1</span>)</div><div class=\"line\">    plot_clustering(X, labels_pred, <span class=\"string\">'%s linkage'</span> % linkage_method)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>可以得到 4 种方法下的聚类结果如下图所示。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/aggm_clust_linkage_comparing.png\" width=\"780\" height=\"650\" alt=\"不同方法下的聚类结果\" align=\"center\"><br></div>\n\n<p>在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 <a href=\"\">AMI</a>()（Adjust Mutual Information）量，该量越接近于 <code>1</code> 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">method single:</div><div class=\"line\">Adjust mutual information: 0.001</div><div class=\"line\">time used: 0.008 seconds</div><div class=\"line\">method complete:</div><div class=\"line\">Adjust mutual information: 0.838</div><div class=\"line\">time used: 0.013 seconds</div><div class=\"line\">method average:</div><div class=\"line\">Adjust mutual information: 0.945</div><div class=\"line\">time used: 0.019 seconds</div><div class=\"line\">method ward:</div><div class=\"line\">Adjust mutual information: 0.956</div><div class=\"line\">time used: 0.015 seconds</div></pre></td></tr></table></figure>\n<p>从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 <code>cluster</code>，而其他两个 <code>cluster</code> 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。</p>\n"},{"title":"聚类分析（三）：高斯混合模型与 EM 算法","date":"2018-03-12T13:26:08.000Z","keywords":"聚类,非监督学习,高斯混合模型,GMM,生成模型,EM 算法,clustering,machine learning,最大似然估计","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# 高斯混合模型\n## 高斯混合模型简介\n高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：\n$$  \np({\\\\bf x}) = \\\\sum\\_{k = 1}^{K} \\\\pi\\_{k} {\\\\cal N}({\\\\bf x}|{\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \n\\\\tag {1}\n$$\n如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 \\\\( K \\\\) 个类别中以 \\\\( \\\\pi\\_{k} \\\\) 的概率随机选择一个类别 \\\\( k \\\\)，然后再依照该类别所对应的高斯分布 \\\\( {\\\\cal N}({\\\\bf x}|{\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\) 随机产生一个数据 \\\\( {\\\\bf x} \\\\)。但最终生成数据集后，我们所观察到的仅仅只是 \\\\( {\\\\bf x} \\\\)，而观察不到用于产生 \\\\( {\\\\bf x} \\\\) 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 \\\\( {\\\\bf z} \\\\in {\\\\lbrace 0, 1 \\\\rbrace}^{K} \\\\) 来表示高斯混合模型中的类别变量，\\\\( {\\\\bf z} \\\\) 中仅有一个元素的值为 \\\\( 1 \\\\)，而其它元素的值为 \\\\( 0 \\\\)，例如当 \\\\( z\\_{k} = 1\\\\) 时，表示当前数据是由高斯混合分布中的第 \\\\( k \\\\) 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 \\\\( {\\\\bf z} \\\\) 的概率质量函数为\n$$ \np(z\\_{k} = 1) = \\\\pi\\_k \n\\\\tag {2}\n$$\n其中 \\\\( \\\\lbrace \\\\pi\\_{k} \\\\rbrace \\\\) 须满足\n$$ \n\\\\sum\\_{k = 1}^{K} \\\\pi\\_k = 1  \\\\text{   ,   }  0 \\\\le \\\\pi\\_k \\\\le 1 \n$$\n给定 \\\\( {\\\\bf z} \\\\) 的值的情况下，\\\\( {\\\\bf x} \\\\) 服从高斯分布\n$$ \np({\\\\bf x} | z\\_{k} = 1) = {\\\\cal N}({\\\\bf x} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \n\\\\tag {3}\n$$\n因而可以得到 \\\\( {\\\\bf x} \\\\) 的边缘概率分布为\n$$\n p({\\\\bf x}) = \\\\sum\\_{\\\\bf z} p({\\\\bf z})p({\\\\bf x} | {\\\\bf z}) = \\\\sum\\_{k =1}^{K} \\\\pi\\_k {\\\\cal N}({\\\\bf x} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \n\\\\tag {4}\n$$\n该分布就是我们前面所看到的高斯混合分布。\n## 最大似然估计问题\n假设有数据集 \\\\( \\\\lbrace {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N \\\\rbrace \\\\)，其中样本的维度为 \\\\( D \\\\)，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 \\\\( {\\\\bf X} = [ {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N ]^{T} \\\\in {\\\\Bbb R}^{N \\\\times D} \\\\) 来表示数据集，以矩阵 \\\\( {\\\\bf Z} = [ {\\\\bf z}\\_1, {\\\\bf z}\\_2, …, {\\\\bf z}\\_N ]^{T} \\\\in {\\\\Bbb R}^{N \\\\times K} \\\\) 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为\n$$\n L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\ln p({\\\\bf X} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\sum\\_{n = 1}^{N} \\\\ln \\\\lbrace  \\\\sum\\_{k = 1}^{K} \\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_{n} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \\\\rbrace \n\\\\tag {5}\n$$\n我们希望求解出使得以上对数似然函数最大的参数集 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\)，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。\n\n 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 \\\\( L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 发散的解。具体地，观察公式 \\\\( (5) \\\\)，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 \\\\( 0 \\\\)）的时候，会导致求和项中的该项的值为无穷大，从而也使得 \\\\( L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 的值为无穷大；这样确实使得式 \\\\( (5) \\\\) 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。\n\n---- \n# EM 算法求解高斯混合模型\nEM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。\n\n我们对对数似然函数 \\\\( (5) \\\\) 关于各参数 \\\\( {\\\\bf \\\\pi} \\\\)， \\\\( {\\\\bf \\\\mu} \\\\)， \\\\( {\\\\bf \\\\Sigma} \\\\) 分别求偏导，并将其置为 \\\\( 0 \\\\) 可以得到一系列的方程，而使得式 \\\\( (5) \\\\) 最大的解也一定满足这些方程。\n\n首先令式 \\\\( (5) \\\\) 关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的偏导为 \\\\( 0 \\\\) 可得以下方程：\n$$\n\\- \\\\sum\\_{n = 1}^{N} \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} {\\\\bf \\\\Sigma}\\_k ({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k) = 0\n\\\\tag {6}\n$$\n注意到，上式中含有项\n$$\n\\\\gamma (z\\_{nk}) =   \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} = p(z\\_{nk} = 1 | {\\\\bf x}\\_n) \n\\\\tag {7}\n$$\n该项具有重要的物理意义，它为给定样本点 \\\\( {\\\\bf x}\\_n \\\\) 后隐变量 \\\\( {\\\\bf z}\\_n \\\\) 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 \\\\( \\\\gamma (z\\_{nk}) \\\\) 的值来做软分配的。\n\n因而，我们可以由式 \\\\( (6) \\\\) 和式 \\\\( (7) \\\\) 写出\n$$\n{\\\\bf \\\\mu}\\_k = \\\\frac{1} {N\\_k} \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) {\\\\bf x}\\_n\n\\\\tag {8}\n$$\n其中 \\\\( N\\_k = \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) \\\\)，我们可以将 \\\\( N\\_{k} \\\\) 解释为被分配给类别 \\\\( k \\\\) 的有效样本数量，而 \\\\( {\\\\bf \\\\mu}\\_{k} \\\\) 即为所有样本点的加权算术平均值，每个样本点的权重等于第 \\\\( k\\\\) 个高斯分量对产生该样本点所负有的“责任”。\n\n我们再将式 \\\\( (5) \\\\) 对 \\\\( {\\\\bf \\\\Sigma}\\_k \\\\) 的偏导数置为 \\\\( 0 \\\\) 可求得\n$$  \n{\\\\bf \\\\Sigma}\\_k = \\\\frac{1} {N\\_k} \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) ({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k)({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k)^{\\\\text T}\n\\\\tag {9}\n$$\n可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 \\\\( \\\\gamma(z\\_{nk}) \\\\)。\n\n最后我们再来推导 \\\\( \\\\pi\\_k \\\\) 的最大似然解须满足的条件。由于 \\\\( \\\\pi\\_k \\\\) 有归一化的约束，我们可以利用 [ Lagrange 乘数法 ][2] 来求解（将 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) + \\\\lambda (\\\\sum\\_{k = 1}^{K} \\\\pi\\_k - 1) \\\\) 关于 \\\\( \\\\pi\\_k \\\\) 的偏导数置 \\\\( 0 \\\\)），最后可求得\n$$\n\\\\pi\\_k = \\\\frac{N\\_k} {N}\n\\\\tag {10}\n$$ \n关于类别 \\\\( k \\\\) 的先验概率的估计值可以理解为所有样本点中被分配给第 \\\\( k \\\\) 个类别的有效样本点的个数占总样本数量的比例。\n\n注意到，式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\) 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 的闭式解，因为这些式子中给出的表达式中都含有 \\\\( \\\\gamma (z\\_{nk}) \\\\)，而 \\\\( \\\\gamma (z\\_{nk}) \\\\) 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 \\\\( (7) \\\\)）。\n\n尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 固定为一个初始值，再按式 \\\\( (7) \\\\) 计算出隐变量的后验概率 \\\\( \\\\gamma (z\\_{nk}) \\\\) （E 步）；然后再固定 \\\\( \\\\gamma (z\\_{nk}) \\\\)，按式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\) 分别更新参数  \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。\n\n实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。\n\n我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率  \\\\( \\\\gamma (z\\_{nk}) \\\\)，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 \\\\( p\\_1 \\\\) ，由红色的高斯分量产生的概率为 \\\\( p\\_2 \\\\)，则我们将其染上 \\\\( p\\_1 \\\\) 比例的蓝色，染上  \\\\( p\\_2 \\\\) 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_for_GMM.png\" width = \"660\" height = \"550\" alt = \"EM 算法求解 GMM 运行过程\" align = center />\n</div>\n\n---- \n# 一般 EM 算法\n前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。\n\n假设观测到的变量集为 \\\\( {\\\\bf X} \\\\)，隐变量集为 \\\\( {\\\\bf Z} \\\\)，模型中所涉及到的参数集为 \\\\( {\\\\bf \\\\theta} \\\\)，我们的目的是最大化关于 \\\\( {\\\\bf X} \\\\) 的似然函数\n$$\np({\\\\bf X} | {\\\\bf \\\\theta}) = \\\\sum\\_{\\\\bf Z} p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})\n\\\\tag {11}\n$$\n一般来讲，直接优化 \\\\( p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 是比较困难的，而优化完全数据集的似然函数 \\\\( p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) \\\\) 的难度则会大大减小，EM 算法就是基于这样的思路。\n\n首先我们引入一个关于隐变量 \\\\( {\\\\bf Z} \\\\) 的分布 \\\\( q({\\\\bf Z}) \\\\)，然后我们可以将对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 分解为如下\n$$\n\\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) = {\\\\cal L}(q, {\\\\bf \\\\theta}) + {\\\\text KL}(q || p) \n\\\\tag {12}\n$$\n其中\n$$\n{\\\\cal L}(q, {\\\\bf \\\\theta}) = \\\\sum\\_{\\\\bf Z} q({\\\\bf Z}) \\\\ln \\\\lbrace \\\\frac{p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})} {q({\\\\bf Z})} \\\\rbrace\n\\\\tag {13}\n$$\n$$\n{\\\\text KL}(q || p) = - \\\\sum\\_{\\\\bf Z} q({\\\\bf Z}) \\\\ln \\\\lbrace \\\\frac{ p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta})} {q({\\\\bf Z})} \\\\rbrace\n\\\\tag {14}\n$$\n其中 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 为关于概率分布 \\\\( q({\\\\bf Z}) \\\\) 的泛函，且为关于参数集 \\\\( {\\\\bf \\\\theta} \\\\) 的函数，另外，\\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 的表达式中包含了关于完全数据集的似然函数 \\\\( p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) \\\\)，这是我们需要好好加以利用的；\\\\( {\\\\text KL}(q || p) \\\\) 为概率分布 \\\\( q({\\\\bf Z}) \\\\) 与隐变量的后验概率分布 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}) \\\\) 间的 [KL 散度][3]，它的值一般大于 \\\\( 0 \\\\)，只有在两个概率分布完全相同的情况下才等于 \\\\( 0 \\\\)，因而其一般被用来衡量两个概率分布之间的差异。\n\n利用 \\\\( {\\\\text KL}(q || p) \\\\ge 0 \\\\) 的性质，我们可以得到 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\le \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\)，即 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 是对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 的一个下界。 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 与 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\)  及 \\\\( {\\\\text KL}(q || p) \\\\) 的关系可用下图中的图（a）来表示。 \n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_derivation.png\" width = \"1000\" height = \"600\" alt = \"EM 算法的推导过程示意图\" align = center />\n</div>\n\n有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。\n\n假设当前迭代步骤的参数的值为 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\)，我们先固定 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\) 的值，来求 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 关于概率分布 \\\\( q({\\\\bf Z}) \\\\) 的最大值。可以看到，\\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta} ^{\\\\text {old}}) \\\\) 现在是一个定值，所以当 \\\\( {\\\\text KL}(q || p) \\\\) 等于 \\\\( 0 \\\\) 时， \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 最大，如上图中的图（b）所示。此时由 \\\\( {\\\\text KL}(q || p) = 0 \\\\) 可以推出，\\\\( q({\\\\bf Z}) = p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)。\n\n现在再固定 \\\\( q({\\\\bf Z}) \\\\)，来求 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 关于 \\\\( {\\\\bf \\\\theta} \\\\) 的最大值，假设求得的最佳 \\\\( {\\\\bf \\\\theta} \\\\) 的值为 \\\\( {\\\\bf \\\\theta} ^{\\\\text {new}} \\\\)，此时 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta} ^{\\\\text {new}}) \\\\) 相比 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 的值增大了，而由于 \\\\( {\\\\bf \\\\theta} \\\\) 值的改变又使得当前 \\\\( {\\\\text KL}(q || p) \\\\) 的值大于或等于 \\\\( 0 \\\\) （当算法收敛时保持 \\\\( 0 \\\\) 的值不变），所以根据式 \\\\( (14) \\\\)，对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。\n\n更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)，我们将 \\\\( q({\\\\bf Z}) = p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 带入  \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\)  中，可得\n$$\n\\\\begin{aligned}\n{\\\\cal L}(q, {\\\\bf \\\\theta}) &= \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) - \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\\\\\\n& = {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) + {\\\\text {const}}\n\\\\end{aligned}\n\\\\tag {15}\n$$\n我们只对上式中的第一项 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。\n\n第二个迭代步骤被称为 M 步（Maximization），是因为要对式 \\\\( (15) \\\\) 中的 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 求解关于  \\\\( {\\\\bf \\\\theta} \\\\) 的最大值，由于 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 的形式相较于对数似然函数  \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。\n\n最后还是来总结一下 EM 算法的运行过程：\n1. 选择一个初始参数集 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\)；\n2. **E  步**，计算隐变量的后验概率函数 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)；\n3. **M 步**，按下式计算 \\\\( {\\\\bf \\\\theta}^{\\\\text {new}} \\\\) 的值\n$$\n{\\\\bf \\\\theta}^{\\\\text {new}} = \\\\rm {arg}  \\\\rm {max}\\_{\\\\bf \\\\theta} {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}})\n\\\\tag {16}\n$$ \n其中 \n$$\n{\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) = \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})\n\\\\tag {17}\n$$\n4. 检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} = {\\\\bf \\\\theta}^{\\\\text {new}} \\\\) ，回到第 2 步继续迭代。\n\n---- \n# 再探高斯混合模型\n在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 \\\\( (7) \\\\) 至 \\\\( (10) \\\\) ）。\n\n## 推导过程\n在高斯混合模型中，参数集 \\\\( {\\\\bf \\\\theta} = \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\)，完整数据集 \\\\( \\\\lbrace {\\\\bf X}, {\\\\bf Z} \\\\rbrace \\\\) 的似然函数为\n$$\np({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\prod\\_{n = 1}^{N} \\\\prod\\_{k = 1}^{K} {\\\\pi\\_k}^{z\\_{nk}} {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)^{z\\_{nk}}\n\\\\tag {18}\n$$ \n对其取对数可得\n$$\n\\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} z\\_{nk} \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {19} \n$$\n按照 EM 算法的迭代步骤，我们先求解隐变量 \\\\( {\\\\bf Z} \\\\) 的后验概率函数，其具有如下形式\n$$\np({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\propto p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\prod\\_{n = 1}^{N} \\\\prod\\_{k = 1}^{K} ({\\\\pi\\_k} {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k))^{z\\_{nk}}\n\\\\tag {20}\n$$\n再来推导完全数据集的对数似然函数在 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 下的期望\n$$\n{\\\\Bbb E}\\_{\\\\bf Z}[ \\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) ] = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} {\\\\Bbb E}[z\\_{nk}] \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {21}\n$$\n而 \\\\( {\\\\Bbb E}[z\\_{nk}] \\\\) 的值可以根据式 \\\\( (20) \\\\) 求出，由于 \\\\( z\\_{nk} \\\\) 只可能取 \\\\( 1 \\\\) 或 \\\\( 0 \\\\)，而取 \\\\( 0 \\\\) 时对期望没有贡献，故有\n$$\n{\\\\Bbb E}[z\\_{nk}] = p(z\\_{nk} = 1 | {\\\\bf x}\\_n, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) = \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} = \\\\gamma (z\\_{nk})\n\\\\tag {22}\n$$\n将上式代入公式 \\\\( (22) \\\\) 中，可得\n$$\n{\\\\Bbb E}\\_{\\\\bf Z}[ \\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) ] = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} \\\\gamma (z\\_{nk}) \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {23}\n$$\n接下来我们就可以对该式关于参数 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 求解最大值了，可以验证，各参数的更新方程就是式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\)。\n\n## 与 k-means 算法的关系\n敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。\n\n首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 \\\\( K \\\\) 个高斯分量，但 k-means 算法做了如下假设：\n1. 假设每个高斯分量的先验概率相等，即 \\\\( \\\\pi\\_k = 1 / K \\\\);\n2. 假设每个高斯分量的协方差矩阵均为 \\\\( \\\\epsilon {\\\\bf I} \\\\)。\n\n所以某一个高斯分量的概率密度函数为\n$$\np({\\\\bf x} | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) = \\\\frac {1} {(2\\\\pi\\\\epsilon)^{D/2}} \\\\exp \\\\lbrace -\\\\frac {\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2}} {2\\\\epsilon} \\\\rbrace\n\\\\tag {24}\n$$\n故根据 EM 算法，可求得隐变量的后验概率函数为\n$$\n\\\\gamma(z\\_{nk}) = \\\\frac{\\\\pi\\_k \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2} /2\\\\epsilon \\\\rbrace } {\\\\sum\\_j \\\\pi\\_j \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_j \\\\|^{2} /2\\\\epsilon \\\\rbrace } = \\\\frac{\\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2} /2\\\\epsilon \\\\rbrace } {\\\\sum\\_j \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_j \\\\|^{2} /2\\\\epsilon \\\\rbrace }\n\\\\tag {25}\n$$\n在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 \\\\( \\\\gamma(z\\_{nk}) \\\\) 的值最大的那个高斯分量，即有\n$$\n r\\_{nk} = \\\\begin{cases} 1, & \\\\text {if \\\\( k = \\\\rm {arg}  \\\\rm {min}\\_{j}  \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_j \\\\|^{2} \\\\) } \\\\\\\\ 0, & \\\\text {otherwise} \\\\end{cases} \n\\\\tag {26}\n$$\n由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求优，将式 \\\\( (8) \\\\) 中的 \\\\( \\\\gamma(z\\_{nk}) \\\\) 替换为 \\\\( r\\_{nk} \\\\)，即可得到 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的更新方法，与 k-means 算法中对中心点的更新方法一致。\n\n现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 `cluster`、期待每个 `cluster` 中的样本数量相近、期待每个 `cluster` 的密度相近。\n\n---- \n# 实现 GMM 聚类\n前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 \\\\( \\\\gamma(z\\_{nk}) \\\\) 的值来对数据集进行聚类。\\\\( \\\\gamma(z\\_{nk}) \\\\) 给出了样本 \\\\( {\\\\bf x}\\_n \\\\) 是由 `cluster` \\\\( k \\\\) 产生的置信程度，最简单的 GMM 聚类即是将样本 \\\\( {\\\\bf x}\\_n \\\\) 分配给 \\\\( \\\\gamma(z\\_{nk}) \\\\) 值最大的 `cluster`。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 `GaussianMixture` 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。\n\n## 利用 python 实现 GMM 聚类\n首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.datasets import make_blobs\n\nclass GMMClust():\n    def __init__(self, n_components=2, max_iter=100, tol=1e-10):\n        self.data_set = None\n        self.n_components = n_components\n        self.pred_label = None\n        self.gamma = None\n        self.component_prob = None\n        self.means = None\n        self.covars = None\n        self.max_iter = max_iter\n        self.tol = tol\n\n    # 计算高斯分布的概率密度函数 \n    @staticmethod\n    def cal_gaussian_prob(x, mean, covar, delta=1e-10):\n        n_dim = x.shape[0]\n        covar = covar + delta * np.eye(n_dim)\n        prob = np.exp(-0.5 * np.dot((x - mean).reshape(1, n_dim),\n                                    np.dot(np.linalg.inv(covar),\n                                           (x - mean).reshape(n_dim, 1))))\n        prob /= np.sqrt(np.linalg.det(covar) * ((2 * np.pi) ** n_dim))\n        return prob\n\n    # 计算每一个样本点的似然函数 \n    def cal_sample_likelihood(self, i):\n        sample_likelihood = sum(self.component_prob[k] *\n                                self.cal_gaussian_prob(self.data_set[i],\n                                                       self.means[k], self.covars[k])\n                                for k in range(self.n_components))\n        return sample_likelihood\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        self.n_samples, self.n_features = self.data_set.shape\n        self.pred_label = np.zeros(self.n_samples, dtype=int)\n        self.gamma = np.zeros((self.n_samples, self.n_components))\n\n        start_time = time.time()\n\n        # 初始化各参数\n        self.component_prob = [1.0 / self.n_components] * self.n_components\n        self.means = np.random.rand(self.n_components, self.n_features)\n        for i in range(self.n_features):\n            dim_min = np.min(self.data_set[:, i])\n            dim_max = np.max(self.data_set[:, i])\n            self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i]\n        self.covars = np.zeros((self.n_components, self.n_features, self.n_features))\n        for i in range(self.n_components):\n            self.covars[i] = np.eye(self.n_features)\n\n        # 开始迭代\n        pre_L = 0\n        iter_cnt = 0\n        while iter_cnt < self.max_iter:\n            iter_cnt += 1\n            crt_L = 0\n            # E 步\n            for i in range(self.n_samples):\n                sample_likelihood = self.cal_sample_likelihood(i)\n                crt_L += np.log(sample_likelihood)\n                for k in range(self.n_components):\n                    self.gamma[i, k] = self.component_prob[k] * \\\n                                       self.cal_gaussian_prob(self.data_set[i],\n                                                              self.means[k],\n                                                              self.covars[k]) / sample_likelihood\n            # M 步\n            effective_num = np.sum(self.gamma, axis=0)\n            for k in range(self.n_components):\n                self.means[k] = sum(self.gamma[i, k] * self.data_set[i] for i in range(self.n_samples))\n                self.means[k] /= effective_num[k]\n                self.covars[k] = sum(self.gamma[i, k] *\n                                     np.outer(self.data_set[i] - self.means[k],\n                                              self.data_set[i] - self.means[k])\n                                     for i in range(self.n_samples))\n                self.covars[k] /= effective_num[k]\n                self.component_prob[k] = effective_num[k] / self.n_samples\n\n            print(\"iteration %s, current value of the log likelihood: %.4f\" % (iter_cnt, crt_L))\n\n            if abs(crt_L - pre_L) < self.tol:\n                break\n            pre_L = crt_L\n\n        self.pred_label = np.argmax(self.gamma, axis=1)\n        print(\"total iteration num: %s, final value of the log likelihood: %.4f, \"\n              \"time used: %.4f seconds\" % (iter_cnt, crt_L, time.time() - start_time))\n\n    # 可视化算法的聚类结果\n    def plot_clustering(self, kind, y=None, title=None):\n        if kind == 1:\n            y = self.pred_label\n        plt.scatter(self.data_set[:, 0], self.data_set[:, 1],\n                    c=y, alpha=0.8)\n        if kind == 1:\n            plt.scatter(self.means[:, 0], self.means[:, 1],\n                        c='r', marker='x')\n        if title is not None:\n            plt.title(title, size=14)\n        plt.axis('on')\n        plt.tight_layout()\n```\n\n创建一个 `GMMClust` 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 `predict` 即可对给定的数据集进行 GMM 聚类；方法 `plot_clustering` 则可以可视化聚类的结果。利用 `GMMClust` 类进行 GMM 聚类的代码如下所示：\n```python\n\t# 生成数据集\n\tn_samples = 1500\n\tcenters = [[0, 0], [5, 6], [8, 3.5]]\n\tcluster_std = [2, 1.0, 0.5]\n\tX, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)\n\n\t# 运行 GMM 聚类算法\n\tgmm_cluster = GMMClust(n_components=3)\n\tgmm_cluster.predict(X)\n    for i in range(3):\n        print(\"cluster %s\" % i)\n        print(\"    mean: %s, covariance: %s\" %(gmm_cluster.means[i], gmm_cluster.covars[i]))\n\n    # 可视化数据集的原始类别情况以及算法的聚类结果\n    plt.subplot(1, 2, 1)\n    gmm_cluster.plot_clustering(kind=0, y=y, title='The Original Dataset')\n    plt.subplot(1, 2, 2)\n    gmm_cluster.plot_clustering(kind=1, title='GMM Clustering Result')\n    plt.show()\n```\n\n以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：\n```\niteration 1, current value of the log likelihood: -15761.9757\niteration 2, current value of the log likelihood: -6435.3937\niteration 3, current value of the log likelihood: -6410.5633\niteration 4, current value of the log likelihood: -6399.4306\niteration 5, current value of the log likelihood: -6389.0317\niteration 6, current value of the log likelihood: -6377.9131\niteration 7, current value of the log likelihood: -6367.5704\niteration 8, current value of the log likelihood: -6359.2076\niteration 9, current value of the log likelihood: -6350.8678\niteration 10, current value of the log likelihood: -6338.6458\n... ...\niteration 35, current value of the log likelihood: -5859.0324\niteration 36, current value of the log likelihood: -5859.0324\niteration 37, current value of the log likelihood: -5859.0324\niteration 38, current value of the log likelihood: -5859.0324\niteration 39, current value of the log likelihood: -5859.0324\niteration 40, current value of the log likelihood: -5859.0324\ntotal iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 seconds\ncluster 0\n    mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269]\n [0.08460269 3.95599185]]\ncluster 1\n    mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461  -0.00345936]\n [-0.00345936  0.9630804 ]]\ncluster 2\n    mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954]\n [0.01120954 0.27281129]]\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_result.png\" width = \"1000\" height = \"500\" alt = \"GMM 聚类结果\" align = center />\n</div>\n\n可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 `cluster` 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 `cluster` 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。\n\n## 利用 sklearn 实现 GMM 聚类\nsklearn 中的  `GaussianMixture` 类可以用来进行 GMM 聚类，其中的 `fit` 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；`predict` 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\n\n# 产生一个理想的数据集\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X)\ny_pred = gmm.predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(\"Normal Blobs\")\n\n# 产生一个非球形分布的数据集\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_aniso)\ny_pred = gmm.predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title(\"Anisotropicly Distributed Blobs\")\n\n# 产生一个各 cluster 的密度不一致的数据集\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_varied)\ny_pred = gmm.predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(\"Unequal Density Blobs\")\n\n# 产生一个各 cluster 的样本数目不一致的数据集\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_filtered)\ny_pred = gmm.predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(\"Unevenly Sized Blobs\")\n\nplt.show()\n```\n\n运行结果如下图所示：\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_different_datasets.png\" width = \"780\" height = \"650\" alt = \"k-means 算法在不同数据集下的表现\" align = center />\n</div>\n\n可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 `cluster`，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 `cluster` 都是超椭球形的。\n\n\n\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\thttps://en.wikipedia.org/wiki/Lagrange_multiplier?oldformat=true\n[3]:\thttps://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence?oldformat=true","source":"_posts/聚类分析（三）：高斯混合模型与EM算法.md","raw":"---\ntitle: 聚类分析（三）：高斯混合模型与 EM 算法\ndate: 2018-03-12 21:26:08\ntags:\n- 聚类\n- 非监督学习\n- 高斯混合模型\n- GMM\n- 生成模型\n- EM 算法\ncategories:\n- 机器学习算法\nkeywords: 聚类,非监督学习,高斯混合模型,GMM,生成模型,EM 算法,clustering,machine learning,最大似然估计\n\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n*本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 [**聚类分析系列文章**][1]*。\n\n# 高斯混合模型\n## 高斯混合模型简介\n高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：\n$$  \np({\\\\bf x}) = \\\\sum\\_{k = 1}^{K} \\\\pi\\_{k} {\\\\cal N}({\\\\bf x}|{\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \n\\\\tag {1}\n$$\n如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 \\\\( K \\\\) 个类别中以 \\\\( \\\\pi\\_{k} \\\\) 的概率随机选择一个类别 \\\\( k \\\\)，然后再依照该类别所对应的高斯分布 \\\\( {\\\\cal N}({\\\\bf x}|{\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\) 随机产生一个数据 \\\\( {\\\\bf x} \\\\)。但最终生成数据集后，我们所观察到的仅仅只是 \\\\( {\\\\bf x} \\\\)，而观察不到用于产生 \\\\( {\\\\bf x} \\\\) 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 \\\\( {\\\\bf z} \\\\in {\\\\lbrace 0, 1 \\\\rbrace}^{K} \\\\) 来表示高斯混合模型中的类别变量，\\\\( {\\\\bf z} \\\\) 中仅有一个元素的值为 \\\\( 1 \\\\)，而其它元素的值为 \\\\( 0 \\\\)，例如当 \\\\( z\\_{k} = 1\\\\) 时，表示当前数据是由高斯混合分布中的第 \\\\( k \\\\) 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 \\\\( {\\\\bf z} \\\\) 的概率质量函数为\n$$ \np(z\\_{k} = 1) = \\\\pi\\_k \n\\\\tag {2}\n$$\n其中 \\\\( \\\\lbrace \\\\pi\\_{k} \\\\rbrace \\\\) 须满足\n$$ \n\\\\sum\\_{k = 1}^{K} \\\\pi\\_k = 1  \\\\text{   ,   }  0 \\\\le \\\\pi\\_k \\\\le 1 \n$$\n给定 \\\\( {\\\\bf z} \\\\) 的值的情况下，\\\\( {\\\\bf x} \\\\) 服从高斯分布\n$$ \np({\\\\bf x} | z\\_{k} = 1) = {\\\\cal N}({\\\\bf x} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \n\\\\tag {3}\n$$\n因而可以得到 \\\\( {\\\\bf x} \\\\) 的边缘概率分布为\n$$\n p({\\\\bf x}) = \\\\sum\\_{\\\\bf z} p({\\\\bf z})p({\\\\bf x} | {\\\\bf z}) = \\\\sum\\_{k =1}^{K} \\\\pi\\_k {\\\\cal N}({\\\\bf x} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \n\\\\tag {4}\n$$\n该分布就是我们前面所看到的高斯混合分布。\n## 最大似然估计问题\n假设有数据集 \\\\( \\\\lbrace {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N \\\\rbrace \\\\)，其中样本的维度为 \\\\( D \\\\)，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 \\\\( {\\\\bf X} = [ {\\\\bf x}\\_1, {\\\\bf x}\\_2, …, {\\\\bf x}\\_N ]^{T} \\\\in {\\\\Bbb R}^{N \\\\times D} \\\\) 来表示数据集，以矩阵 \\\\( {\\\\bf Z} = [ {\\\\bf z}\\_1, {\\\\bf z}\\_2, …, {\\\\bf z}\\_N ]^{T} \\\\in {\\\\Bbb R}^{N \\\\times K} \\\\) 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为\n$$\n L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\ln p({\\\\bf X} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\sum\\_{n = 1}^{N} \\\\ln \\\\lbrace  \\\\sum\\_{k = 1}^{K} \\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_{n} | {\\\\bf \\\\mu}\\_{k}, {\\\\bf \\\\Sigma}\\_{k}) \\\\rbrace \n\\\\tag {5}\n$$\n我们希望求解出使得以上对数似然函数最大的参数集 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\)，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。\n\n 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 \\\\( L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 发散的解。具体地，观察公式 \\\\( (5) \\\\)，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 \\\\( 0 \\\\)）的时候，会导致求和项中的该项的值为无穷大，从而也使得 \\\\( L({\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 的值为无穷大；这样确实使得式 \\\\( (5) \\\\) 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。\n\n---- \n# EM 算法求解高斯混合模型\nEM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。\n\n我们对对数似然函数 \\\\( (5) \\\\) 关于各参数 \\\\( {\\\\bf \\\\pi} \\\\)， \\\\( {\\\\bf \\\\mu} \\\\)， \\\\( {\\\\bf \\\\Sigma} \\\\) 分别求偏导，并将其置为 \\\\( 0 \\\\) 可以得到一系列的方程，而使得式 \\\\( (5) \\\\) 最大的解也一定满足这些方程。\n\n首先令式 \\\\( (5) \\\\) 关于 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的偏导为 \\\\( 0 \\\\) 可得以下方程：\n$$\n\\- \\\\sum\\_{n = 1}^{N} \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} {\\\\bf \\\\Sigma}\\_k ({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k) = 0\n\\\\tag {6}\n$$\n注意到，上式中含有项\n$$\n\\\\gamma (z\\_{nk}) =   \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} = p(z\\_{nk} = 1 | {\\\\bf x}\\_n) \n\\\\tag {7}\n$$\n该项具有重要的物理意义，它为给定样本点 \\\\( {\\\\bf x}\\_n \\\\) 后隐变量 \\\\( {\\\\bf z}\\_n \\\\) 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 \\\\( \\\\gamma (z\\_{nk}) \\\\) 的值来做软分配的。\n\n因而，我们可以由式 \\\\( (6) \\\\) 和式 \\\\( (7) \\\\) 写出\n$$\n{\\\\bf \\\\mu}\\_k = \\\\frac{1} {N\\_k} \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) {\\\\bf x}\\_n\n\\\\tag {8}\n$$\n其中 \\\\( N\\_k = \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) \\\\)，我们可以将 \\\\( N\\_{k} \\\\) 解释为被分配给类别 \\\\( k \\\\) 的有效样本数量，而 \\\\( {\\\\bf \\\\mu}\\_{k} \\\\) 即为所有样本点的加权算术平均值，每个样本点的权重等于第 \\\\( k\\\\) 个高斯分量对产生该样本点所负有的“责任”。\n\n我们再将式 \\\\( (5) \\\\) 对 \\\\( {\\\\bf \\\\Sigma}\\_k \\\\) 的偏导数置为 \\\\( 0 \\\\) 可求得\n$$  \n{\\\\bf \\\\Sigma}\\_k = \\\\frac{1} {N\\_k} \\\\sum\\_{n = 1}^{N} \\\\gamma(z\\_{nk}) ({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k)({\\\\bf x}\\_n - {\\\\bf \\\\mu}\\_k)^{\\\\text T}\n\\\\tag {9}\n$$\n可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 \\\\( \\\\gamma(z\\_{nk}) \\\\)。\n\n最后我们再来推导 \\\\( \\\\pi\\_k \\\\) 的最大似然解须满足的条件。由于 \\\\( \\\\pi\\_k \\\\) 有归一化的约束，我们可以利用 [ Lagrange 乘数法 ][2] 来求解（将 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) + \\\\lambda (\\\\sum\\_{k = 1}^{K} \\\\pi\\_k - 1) \\\\) 关于 \\\\( \\\\pi\\_k \\\\) 的偏导数置 \\\\( 0 \\\\)），最后可求得\n$$\n\\\\pi\\_k = \\\\frac{N\\_k} {N}\n\\\\tag {10}\n$$ \n关于类别 \\\\( k \\\\) 的先验概率的估计值可以理解为所有样本点中被分配给第 \\\\( k \\\\) 个类别的有效样本点的个数占总样本数量的比例。\n\n注意到，式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\) 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 的闭式解，因为这些式子中给出的表达式中都含有 \\\\( \\\\gamma (z\\_{nk}) \\\\)，而 \\\\( \\\\gamma (z\\_{nk}) \\\\) 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 \\\\( (7) \\\\)）。\n\n尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 固定为一个初始值，再按式 \\\\( (7) \\\\) 计算出隐变量的后验概率 \\\\( \\\\gamma (z\\_{nk}) \\\\) （E 步）；然后再固定 \\\\( \\\\gamma (z\\_{nk}) \\\\)，按式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\) 分别更新参数  \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。\n\n实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。\n\n我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率  \\\\( \\\\gamma (z\\_{nk}) \\\\)，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 \\\\( p\\_1 \\\\) ，由红色的高斯分量产生的概率为 \\\\( p\\_2 \\\\)，则我们将其染上 \\\\( p\\_1 \\\\) 比例的蓝色，染上  \\\\( p\\_2 \\\\) 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_for_GMM.png\" width = \"660\" height = \"550\" alt = \"EM 算法求解 GMM 运行过程\" align = center />\n</div>\n\n---- \n# 一般 EM 算法\n前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。\n\n假设观测到的变量集为 \\\\( {\\\\bf X} \\\\)，隐变量集为 \\\\( {\\\\bf Z} \\\\)，模型中所涉及到的参数集为 \\\\( {\\\\bf \\\\theta} \\\\)，我们的目的是最大化关于 \\\\( {\\\\bf X} \\\\) 的似然函数\n$$\np({\\\\bf X} | {\\\\bf \\\\theta}) = \\\\sum\\_{\\\\bf Z} p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})\n\\\\tag {11}\n$$\n一般来讲，直接优化 \\\\( p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 是比较困难的，而优化完全数据集的似然函数 \\\\( p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) \\\\) 的难度则会大大减小，EM 算法就是基于这样的思路。\n\n首先我们引入一个关于隐变量 \\\\( {\\\\bf Z} \\\\) 的分布 \\\\( q({\\\\bf Z}) \\\\)，然后我们可以将对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 分解为如下\n$$\n\\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) = {\\\\cal L}(q, {\\\\bf \\\\theta}) + {\\\\text KL}(q || p) \n\\\\tag {12}\n$$\n其中\n$$\n{\\\\cal L}(q, {\\\\bf \\\\theta}) = \\\\sum\\_{\\\\bf Z} q({\\\\bf Z}) \\\\ln \\\\lbrace \\\\frac{p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})} {q({\\\\bf Z})} \\\\rbrace\n\\\\tag {13}\n$$\n$$\n{\\\\text KL}(q || p) = - \\\\sum\\_{\\\\bf Z} q({\\\\bf Z}) \\\\ln \\\\lbrace \\\\frac{ p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta})} {q({\\\\bf Z})} \\\\rbrace\n\\\\tag {14}\n$$\n其中 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 为关于概率分布 \\\\( q({\\\\bf Z}) \\\\) 的泛函，且为关于参数集 \\\\( {\\\\bf \\\\theta} \\\\) 的函数，另外，\\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 的表达式中包含了关于完全数据集的似然函数 \\\\( p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) \\\\)，这是我们需要好好加以利用的；\\\\( {\\\\text KL}(q || p) \\\\) 为概率分布 \\\\( q({\\\\bf Z}) \\\\) 与隐变量的后验概率分布 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}) \\\\) 间的 [KL 散度][3]，它的值一般大于 \\\\( 0 \\\\)，只有在两个概率分布完全相同的情况下才等于 \\\\( 0 \\\\)，因而其一般被用来衡量两个概率分布之间的差异。\n\n利用 \\\\( {\\\\text KL}(q || p) \\\\ge 0 \\\\) 的性质，我们可以得到 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\le \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\)，即 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 是对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 的一个下界。 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 与 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\)  及 \\\\( {\\\\text KL}(q || p) \\\\) 的关系可用下图中的图（a）来表示。 \n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_derivation.png\" width = \"1000\" height = \"600\" alt = \"EM 算法的推导过程示意图\" align = center />\n</div>\n\n有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。\n\n假设当前迭代步骤的参数的值为 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\)，我们先固定 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\) 的值，来求 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 关于概率分布 \\\\( q({\\\\bf Z}) \\\\) 的最大值。可以看到，\\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta} ^{\\\\text {old}}) \\\\) 现在是一个定值，所以当 \\\\( {\\\\text KL}(q || p) \\\\) 等于 \\\\( 0 \\\\) 时， \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 最大，如上图中的图（b）所示。此时由 \\\\( {\\\\text KL}(q || p) = 0 \\\\) 可以推出，\\\\( q({\\\\bf Z}) = p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)。\n\n现在再固定 \\\\( q({\\\\bf Z}) \\\\)，来求 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\) 关于 \\\\( {\\\\bf \\\\theta} \\\\) 的最大值，假设求得的最佳 \\\\( {\\\\bf \\\\theta} \\\\) 的值为 \\\\( {\\\\bf \\\\theta} ^{\\\\text {new}} \\\\)，此时 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta} ^{\\\\text {new}}) \\\\) 相比 \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 的值增大了，而由于 \\\\( {\\\\bf \\\\theta} \\\\) 值的改变又使得当前 \\\\( {\\\\text KL}(q || p) \\\\) 的值大于或等于 \\\\( 0 \\\\) （当算法收敛时保持 \\\\( 0 \\\\) 的值不变），所以根据式 \\\\( (14) \\\\)，对数似然函数 \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。\n\n更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)，我们将 \\\\( q({\\\\bf Z}) = p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 带入  \\\\( {\\\\cal L}(q, {\\\\bf \\\\theta}) \\\\)  中，可得\n$$\n\\\\begin{aligned}\n{\\\\cal L}(q, {\\\\bf \\\\theta}) &= \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta}) - \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\\\\\\n& = {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) + {\\\\text {const}}\n\\\\end{aligned}\n\\\\tag {15}\n$$\n我们只对上式中的第一项 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。\n\n第二个迭代步骤被称为 M 步（Maximization），是因为要对式 \\\\( (15) \\\\) 中的 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 求解关于  \\\\( {\\\\bf \\\\theta} \\\\) 的最大值，由于 \\\\( {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\) 的形式相较于对数似然函数  \\\\( \\\\ln p({\\\\bf X} | {\\\\bf \\\\theta}) \\\\) 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。\n\n最后还是来总结一下 EM 算法的运行过程：\n1. 选择一个初始参数集 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} \\\\)；\n2. **E  步**，计算隐变量的后验概率函数 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\)；\n3. **M 步**，按下式计算 \\\\( {\\\\bf \\\\theta}^{\\\\text {new}} \\\\) 的值\n$$\n{\\\\bf \\\\theta}^{\\\\text {new}} = \\\\rm {arg}  \\\\rm {max}\\_{\\\\bf \\\\theta} {\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}})\n\\\\tag {16}\n$$ \n其中 \n$$\n{\\\\cal Q} ({\\\\bf \\\\theta}, {\\\\bf \\\\theta}^{\\\\text {old}}) = \\\\sum\\_{\\\\bf Z} p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\theta}^{\\\\text {old}}) \\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\theta})\n\\\\tag {17}\n$$\n4. 检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 \\\\( {\\\\bf \\\\theta}^{\\\\text {old}} = {\\\\bf \\\\theta}^{\\\\text {new}} \\\\) ，回到第 2 步继续迭代。\n\n---- \n# 再探高斯混合模型\n在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 \\\\( (7) \\\\) 至 \\\\( (10) \\\\) ）。\n\n## 推导过程\n在高斯混合模型中，参数集 \\\\( {\\\\bf \\\\theta} = \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\)，完整数据集 \\\\( \\\\lbrace {\\\\bf X}, {\\\\bf Z} \\\\rbrace \\\\) 的似然函数为\n$$\np({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\prod\\_{n = 1}^{N} \\\\prod\\_{k = 1}^{K} {\\\\pi\\_k}^{z\\_{nk}} {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)^{z\\_{nk}}\n\\\\tag {18}\n$$ \n对其取对数可得\n$$\n\\\\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} z\\_{nk} \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {19} \n$$\n按照 EM 算法的迭代步骤，我们先求解隐变量 \\\\( {\\\\bf Z} \\\\) 的后验概率函数，其具有如下形式\n$$\np({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\propto p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) = \\\\prod\\_{n = 1}^{N} \\\\prod\\_{k = 1}^{K} ({\\\\pi\\_k} {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k))^{z\\_{nk}}\n\\\\tag {20}\n$$\n再来推导完全数据集的对数似然函数在 \\\\( p({\\\\bf Z} | {\\\\bf X}, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) \\\\) 下的期望\n$$\n{\\\\Bbb E}\\_{\\\\bf Z}[ \\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) ] = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} {\\\\Bbb E}[z\\_{nk}] \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {21}\n$$\n而 \\\\( {\\\\Bbb E}[z\\_{nk}] \\\\) 的值可以根据式 \\\\( (20) \\\\) 求出，由于 \\\\( z\\_{nk} \\\\) 只可能取 \\\\( 1 \\\\) 或 \\\\( 0 \\\\)，而取 \\\\( 0 \\\\) 时对期望没有贡献，故有\n$$\n{\\\\Bbb E}[z\\_{nk}] = p(z\\_{nk} = 1 | {\\\\bf x}\\_n, {\\\\bf \\\\pi}, {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) = \\\\frac{\\\\pi\\_k {\\\\cal N}({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k)} { \\\\sum\\_{j} \\\\pi\\_j {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_j, {\\\\bf \\\\Sigma}\\_j)} = \\\\gamma (z\\_{nk})\n\\\\tag {22}\n$$\n将上式代入公式 \\\\( (22) \\\\) 中，可得\n$$\n{\\\\Bbb E}\\_{\\\\bf Z}[ \\ln p({\\\\bf X}, {\\\\bf Z} | {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma}) ] = \\\\sum\\_{n = 1}^{N} \\\\sum\\_{k = 1}^{K} \\\\gamma (z\\_{nk}) \\\\lbrace \\\\ln \\\\pi\\_k + \\\\ln {\\\\cal N} ({\\\\bf x}\\_n | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) \\\\rbrace\n\\\\tag {23}\n$$\n接下来我们就可以对该式关于参数 \\\\( \\\\lbrace {\\\\bf \\\\pi}, {\\\\bf \\\\mu}, {\\\\bf \\\\Sigma} \\\\rbrace \\\\) 求解最大值了，可以验证，各参数的更新方程就是式 \\\\( (8) \\\\) 至 \\\\( (10) \\\\)。\n\n## 与 k-means 算法的关系\n敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。\n\n首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 \\\\( K \\\\) 个高斯分量，但 k-means 算法做了如下假设：\n1. 假设每个高斯分量的先验概率相等，即 \\\\( \\\\pi\\_k = 1 / K \\\\);\n2. 假设每个高斯分量的协方差矩阵均为 \\\\( \\\\epsilon {\\\\bf I} \\\\)。\n\n所以某一个高斯分量的概率密度函数为\n$$\np({\\\\bf x} | {\\\\bf \\\\mu}\\_k, {\\\\bf \\\\Sigma}\\_k) = \\\\frac {1} {(2\\\\pi\\\\epsilon)^{D/2}} \\\\exp \\\\lbrace -\\\\frac {\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2}} {2\\\\epsilon} \\\\rbrace\n\\\\tag {24}\n$$\n故根据 EM 算法，可求得隐变量的后验概率函数为\n$$\n\\\\gamma(z\\_{nk}) = \\\\frac{\\\\pi\\_k \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2} /2\\\\epsilon \\\\rbrace } {\\\\sum\\_j \\\\pi\\_j \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_j \\\\|^{2} /2\\\\epsilon \\\\rbrace } = \\\\frac{\\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_k \\\\|^{2} /2\\\\epsilon \\\\rbrace } {\\\\sum\\_j \\\\exp \\\\lbrace -\\\\| {\\\\bf x} - {\\\\bf \\\\mu}\\_j \\\\|^{2} /2\\\\epsilon \\\\rbrace }\n\\\\tag {25}\n$$\n在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 \\\\( \\\\gamma(z\\_{nk}) \\\\) 的值最大的那个高斯分量，即有\n$$\n r\\_{nk} = \\\\begin{cases} 1, & \\\\text {if \\\\( k = \\\\rm {arg}  \\\\rm {min}\\_{j}  \\\\| {\\\\bf x}\\_n -  {\\\\bf \\\\mu}\\_j \\\\|^{2} \\\\) } \\\\\\\\ 0, & \\\\text {otherwise} \\\\end{cases} \n\\\\tag {26}\n$$\n由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 求优，将式 \\\\( (8) \\\\) 中的 \\\\( \\\\gamma(z\\_{nk}) \\\\) 替换为 \\\\( r\\_{nk} \\\\)，即可得到 \\\\( {\\\\bf \\\\mu}\\_k \\\\) 的更新方法，与 k-means 算法中对中心点的更新方法一致。\n\n现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 `cluster`、期待每个 `cluster` 中的样本数量相近、期待每个 `cluster` 的密度相近。\n\n---- \n# 实现 GMM 聚类\n前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 \\\\( \\\\gamma(z\\_{nk}) \\\\) 的值来对数据集进行聚类。\\\\( \\\\gamma(z\\_{nk}) \\\\) 给出了样本 \\\\( {\\\\bf x}\\_n \\\\) 是由 `cluster` \\\\( k \\\\) 产生的置信程度，最简单的 GMM 聚类即是将样本 \\\\( {\\\\bf x}\\_n \\\\) 分配给 \\\\( \\\\gamma(z\\_{nk}) \\\\) 值最大的 `cluster`。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 `GaussianMixture` 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。\n\n## 利用 python 实现 GMM 聚类\n首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.datasets import make_blobs\n\nclass GMMClust():\n    def __init__(self, n_components=2, max_iter=100, tol=1e-10):\n        self.data_set = None\n        self.n_components = n_components\n        self.pred_label = None\n        self.gamma = None\n        self.component_prob = None\n        self.means = None\n        self.covars = None\n        self.max_iter = max_iter\n        self.tol = tol\n\n    # 计算高斯分布的概率密度函数 \n    @staticmethod\n    def cal_gaussian_prob(x, mean, covar, delta=1e-10):\n        n_dim = x.shape[0]\n        covar = covar + delta * np.eye(n_dim)\n        prob = np.exp(-0.5 * np.dot((x - mean).reshape(1, n_dim),\n                                    np.dot(np.linalg.inv(covar),\n                                           (x - mean).reshape(n_dim, 1))))\n        prob /= np.sqrt(np.linalg.det(covar) * ((2 * np.pi) ** n_dim))\n        return prob\n\n    # 计算每一个样本点的似然函数 \n    def cal_sample_likelihood(self, i):\n        sample_likelihood = sum(self.component_prob[k] *\n                                self.cal_gaussian_prob(self.data_set[i],\n                                                       self.means[k], self.covars[k])\n                                for k in range(self.n_components))\n        return sample_likelihood\n\n    def predict(self, data_set):\n        self.data_set = data_set\n        self.n_samples, self.n_features = self.data_set.shape\n        self.pred_label = np.zeros(self.n_samples, dtype=int)\n        self.gamma = np.zeros((self.n_samples, self.n_components))\n\n        start_time = time.time()\n\n        # 初始化各参数\n        self.component_prob = [1.0 / self.n_components] * self.n_components\n        self.means = np.random.rand(self.n_components, self.n_features)\n        for i in range(self.n_features):\n            dim_min = np.min(self.data_set[:, i])\n            dim_max = np.max(self.data_set[:, i])\n            self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i]\n        self.covars = np.zeros((self.n_components, self.n_features, self.n_features))\n        for i in range(self.n_components):\n            self.covars[i] = np.eye(self.n_features)\n\n        # 开始迭代\n        pre_L = 0\n        iter_cnt = 0\n        while iter_cnt < self.max_iter:\n            iter_cnt += 1\n            crt_L = 0\n            # E 步\n            for i in range(self.n_samples):\n                sample_likelihood = self.cal_sample_likelihood(i)\n                crt_L += np.log(sample_likelihood)\n                for k in range(self.n_components):\n                    self.gamma[i, k] = self.component_prob[k] * \\\n                                       self.cal_gaussian_prob(self.data_set[i],\n                                                              self.means[k],\n                                                              self.covars[k]) / sample_likelihood\n            # M 步\n            effective_num = np.sum(self.gamma, axis=0)\n            for k in range(self.n_components):\n                self.means[k] = sum(self.gamma[i, k] * self.data_set[i] for i in range(self.n_samples))\n                self.means[k] /= effective_num[k]\n                self.covars[k] = sum(self.gamma[i, k] *\n                                     np.outer(self.data_set[i] - self.means[k],\n                                              self.data_set[i] - self.means[k])\n                                     for i in range(self.n_samples))\n                self.covars[k] /= effective_num[k]\n                self.component_prob[k] = effective_num[k] / self.n_samples\n\n            print(\"iteration %s, current value of the log likelihood: %.4f\" % (iter_cnt, crt_L))\n\n            if abs(crt_L - pre_L) < self.tol:\n                break\n            pre_L = crt_L\n\n        self.pred_label = np.argmax(self.gamma, axis=1)\n        print(\"total iteration num: %s, final value of the log likelihood: %.4f, \"\n              \"time used: %.4f seconds\" % (iter_cnt, crt_L, time.time() - start_time))\n\n    # 可视化算法的聚类结果\n    def plot_clustering(self, kind, y=None, title=None):\n        if kind == 1:\n            y = self.pred_label\n        plt.scatter(self.data_set[:, 0], self.data_set[:, 1],\n                    c=y, alpha=0.8)\n        if kind == 1:\n            plt.scatter(self.means[:, 0], self.means[:, 1],\n                        c='r', marker='x')\n        if title is not None:\n            plt.title(title, size=14)\n        plt.axis('on')\n        plt.tight_layout()\n```\n\n创建一个 `GMMClust` 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 `predict` 即可对给定的数据集进行 GMM 聚类；方法 `plot_clustering` 则可以可视化聚类的结果。利用 `GMMClust` 类进行 GMM 聚类的代码如下所示：\n```python\n\t# 生成数据集\n\tn_samples = 1500\n\tcenters = [[0, 0], [5, 6], [8, 3.5]]\n\tcluster_std = [2, 1.0, 0.5]\n\tX, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)\n\n\t# 运行 GMM 聚类算法\n\tgmm_cluster = GMMClust(n_components=3)\n\tgmm_cluster.predict(X)\n    for i in range(3):\n        print(\"cluster %s\" % i)\n        print(\"    mean: %s, covariance: %s\" %(gmm_cluster.means[i], gmm_cluster.covars[i]))\n\n    # 可视化数据集的原始类别情况以及算法的聚类结果\n    plt.subplot(1, 2, 1)\n    gmm_cluster.plot_clustering(kind=0, y=y, title='The Original Dataset')\n    plt.subplot(1, 2, 2)\n    gmm_cluster.plot_clustering(kind=1, title='GMM Clustering Result')\n    plt.show()\n```\n\n以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：\n```\niteration 1, current value of the log likelihood: -15761.9757\niteration 2, current value of the log likelihood: -6435.3937\niteration 3, current value of the log likelihood: -6410.5633\niteration 4, current value of the log likelihood: -6399.4306\niteration 5, current value of the log likelihood: -6389.0317\niteration 6, current value of the log likelihood: -6377.9131\niteration 7, current value of the log likelihood: -6367.5704\niteration 8, current value of the log likelihood: -6359.2076\niteration 9, current value of the log likelihood: -6350.8678\niteration 10, current value of the log likelihood: -6338.6458\n... ...\niteration 35, current value of the log likelihood: -5859.0324\niteration 36, current value of the log likelihood: -5859.0324\niteration 37, current value of the log likelihood: -5859.0324\niteration 38, current value of the log likelihood: -5859.0324\niteration 39, current value of the log likelihood: -5859.0324\niteration 40, current value of the log likelihood: -5859.0324\ntotal iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 seconds\ncluster 0\n    mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269]\n [0.08460269 3.95599185]]\ncluster 1\n    mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461  -0.00345936]\n [-0.00345936  0.9630804 ]]\ncluster 2\n    mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954]\n [0.01120954 0.27281129]]\n```\n\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_result.png\" width = \"1000\" height = \"500\" alt = \"GMM 聚类结果\" align = center />\n</div>\n\n可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 `cluster` 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 `cluster` 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。\n\n## 利用 sklearn 实现 GMM 聚类\nsklearn 中的  `GaussianMixture` 类可以用来进行 GMM 聚类，其中的 `fit` 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；`predict` 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\n\n# 产生一个理想的数据集\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X)\ny_pred = gmm.predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(\"Normal Blobs\")\n\n# 产生一个非球形分布的数据集\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_aniso)\ny_pred = gmm.predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title(\"Anisotropicly Distributed Blobs\")\n\n# 产生一个各 cluster 的密度不一致的数据集\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_varied)\ny_pred = gmm.predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(\"Unequal Density Blobs\")\n\n# 产生一个各 cluster 的样本数目不一致的数据集\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))\ngmm = GaussianMixture(n_components=3, random_state=random_state)\ngmm.fit(X_filtered)\ny_pred = gmm.predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(\"Unevenly Sized Blobs\")\n\nplt.show()\n```\n\n运行结果如下图所示：\n<div align = center>\n<img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_different_datasets.png\" width = \"780\" height = \"650\" alt = \"k-means 算法在不同数据集下的表现\" align = center />\n</div>\n\n可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 `cluster`，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 `cluster` 都是超椭球形的。\n\n\n\n\n\n\n\n\n\n[1]:\t../clustering-analysis/index.html\n[2]:\thttps://en.wikipedia.org/wiki/Lagrange_multiplier?oldformat=true\n[3]:\thttps://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence?oldformat=true","slug":"聚类分析（三）：高斯混合模型与EM算法","published":1,"updated":"2018-03-29T14:11:19.653Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfclp3u0000ahws6kbnacbch","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"高斯混合模型\"><a href=\"#高斯混合模型\" class=\"headerlink\" title=\"高斯混合模型\"></a>高斯混合模型</h1><h2 id=\"高斯混合模型简介\"><a href=\"#高斯混合模型简介\" class=\"headerlink\" title=\"高斯混合模型简介\"></a>高斯混合模型简介</h2><p>高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：<br>$$<br>p({\\bf x}) = \\sum_{k = 1}^{K} \\pi_{k} {\\cal N}({\\bf x}|{\\bf \\mu}_k, {\\bf \\Sigma}_k)<br>\\tag {1}<br>$$<br>如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 \\( K \\) 个类别中以 \\( \\pi_{k} \\) 的概率随机选择一个类别 \\( k \\)，然后再依照该类别所对应的高斯分布 \\( {\\cal N}({\\bf x}|{\\bf \\mu}_k, {\\bf \\Sigma}_k) \\) 随机产生一个数据 \\( {\\bf x} \\)。但最终生成数据集后，我们所观察到的仅仅只是 \\( {\\bf x} \\)，而观察不到用于产生 \\( {\\bf x} \\) 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 \\( {\\bf z} \\in {\\lbrace 0, 1 \\rbrace}^{K} \\) 来表示高斯混合模型中的类别变量，\\( {\\bf z} \\) 中仅有一个元素的值为 \\( 1 \\)，而其它元素的值为 \\( 0 \\)，例如当 \\( z_{k} = 1\\) 时，表示当前数据是由高斯混合分布中的第 \\( k \\) 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 \\( {\\bf z} \\) 的概率质量函数为<br>$$<br>p(z_{k} = 1) = \\pi_k<br>\\tag {2}<br>$$<br>其中 \\( \\lbrace \\pi_{k} \\rbrace \\) 须满足<br>$$<br>\\sum_{k = 1}^{K} \\pi_k = 1  \\text{   ,   }  0 \\le \\pi_k \\le 1<br>$$<br>给定 \\( {\\bf z} \\) 的值的情况下，\\( {\\bf x} \\) 服从高斯分布<br>$$<br>p({\\bf x} | z_{k} = 1) = {\\cal N}({\\bf x} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k})<br>\\tag {3}<br>$$<br>因而可以得到 \\( {\\bf x} \\) 的边缘概率分布为<br>$$<br> p({\\bf x}) = \\sum_{\\bf z} p({\\bf z})p({\\bf x} | {\\bf z}) = \\sum_{k =1}^{K} \\pi_k {\\cal N}({\\bf x} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k})<br>\\tag {4}<br>$$<br>该分布就是我们前面所看到的高斯混合分布。</p>\n<h2 id=\"最大似然估计问题\"><a href=\"#最大似然估计问题\" class=\"headerlink\" title=\"最大似然估计问题\"></a>最大似然估计问题</h2><p>假设有数据集 \\( \\lbrace {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N \\rbrace \\)，其中样本的维度为 \\( D \\)，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 \\( {\\bf X} = [ {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N ]^{T} \\in {\\Bbb R}^{N \\times D} \\) 来表示数据集，以矩阵 \\( {\\bf Z} = [ {\\bf z}_1, {\\bf z}_2, …, {\\bf z}_N ]^{T} \\in {\\Bbb R}^{N \\times K} \\) 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为<br>$$<br> L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\ln p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\sum_{n = 1}^{N} \\ln \\lbrace  \\sum_{k = 1}^{K} \\pi_k {\\cal N}({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k}) \\rbrace<br>\\tag {5}<br>$$<br>我们希望求解出使得以上对数似然函数最大的参数集 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\)，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。</p>\n<p> 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 \\( L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 发散的解。具体地，观察公式 \\( (5) \\)，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 \\( 0 \\)）的时候，会导致求和项中的该项的值为无穷大，从而也使得 \\( L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 的值为无穷大；这样确实使得式 \\( (5) \\) 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。</p>\n<hr>\n<h1 id=\"EM-算法求解高斯混合模型\"><a href=\"#EM-算法求解高斯混合模型\" class=\"headerlink\" title=\"EM 算法求解高斯混合模型\"></a>EM 算法求解高斯混合模型</h1><p>EM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。</p>\n<p>我们对对数似然函数 \\( (5) \\) 关于各参数 \\( {\\bf \\pi} \\)， \\( {\\bf \\mu} \\)， \\( {\\bf \\Sigma} \\) 分别求偏导，并将其置为 \\( 0 \\) 可以得到一系列的方程，而使得式 \\( (5) \\) 最大的解也一定满足这些方程。</p>\n<p>首先令式 \\( (5) \\) 关于 \\( {\\bf \\mu}_k \\) 的偏导为 \\( 0 \\) 可得以下方程：<br>$$<br>- \\sum_{n = 1}^{N} \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} {\\bf \\Sigma}_k ({\\bf x}_n - {\\bf \\mu}_k) = 0<br>\\tag {6}<br>$$<br>注意到，上式中含有项<br>$$<br>\\gamma (z_{nk}) =   \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} = p(z_{nk} = 1 | {\\bf x}_n)<br>\\tag {7}<br>$$<br>该项具有重要的物理意义，它为给定样本点 \\( {\\bf x}_n \\) 后隐变量 \\( {\\bf z}_n \\) 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 \\( \\gamma (z_{nk}) \\) 的值来做软分配的。</p>\n<p>因而，我们可以由式 \\( (6) \\) 和式 \\( (7) \\) 写出<br>$$<br>{\\bf \\mu}_k = \\frac{1} {N_k} \\sum_{n = 1}^{N} \\gamma(z_{nk}) {\\bf x}_n<br>\\tag {8}<br>$$<br>其中 \\( N_k = \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\)，我们可以将 \\( N_{k} \\) 解释为被分配给类别 \\( k \\) 的有效样本数量，而 \\( {\\bf \\mu}_{k} \\) 即为所有样本点的加权算术平均值，每个样本点的权重等于第 \\( k\\) 个高斯分量对产生该样本点所负有的“责任”。</p>\n<p>我们再将式 \\( (5) \\) 对 \\( {\\bf \\Sigma}_k \\) 的偏导数置为 \\( 0 \\) 可求得<br>$$<br>{\\bf \\Sigma}_k = \\frac{1} {N_k} \\sum_{n = 1}^{N} \\gamma(z_{nk}) ({\\bf x}_n - {\\bf \\mu}_k)({\\bf x}_n - {\\bf \\mu}_k)^{\\text T}<br>\\tag {9}<br>$$<br>可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 \\( \\gamma(z_{nk}) \\)。</p>\n<p>最后我们再来推导 \\( \\pi_k \\) 的最大似然解须满足的条件。由于 \\( \\pi_k \\) 有归一化的约束，我们可以利用 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Lagrange 乘数法 </a> 来求解（将 \\( \\ln p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) + \\lambda (\\sum_{k = 1}^{K} \\pi_k - 1) \\) 关于 \\( \\pi_k \\) 的偏导数置 \\( 0 \\)），最后可求得<br>$$<br>\\pi_k = \\frac{N_k} {N}<br>\\tag {10}<br>$$<br>关于类别 \\( k \\) 的先验概率的估计值可以理解为所有样本点中被分配给第 \\( k \\) 个类别的有效样本点的个数占总样本数量的比例。</p>\n<p>注意到，式 \\( (8) \\) 至 \\( (10) \\) 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 的闭式解，因为这些式子中给出的表达式中都含有 \\( \\gamma (z_{nk}) \\)，而 \\( \\gamma (z_{nk}) \\) 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 \\( (7) \\)）。</p>\n<p>尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 固定为一个初始值，再按式 \\( (7) \\) 计算出隐变量的后验概率 \\( \\gamma (z_{nk}) \\) （E 步）；然后再固定 \\( \\gamma (z_{nk}) \\)，按式 \\( (8) \\) 至 \\( (10) \\) 分别更新参数  \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。</p>\n<p>实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。</p>\n<p>我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率  \\( \\gamma (z_{nk}) \\)，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 \\( p_1 \\) ，由红色的高斯分量产生的概率为 \\( p_2 \\)，则我们将其染上 \\( p_1 \\) 比例的蓝色，染上  \\( p_2 \\) 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_for_GMM.png\" width=\"660\" height=\"550\" alt=\"EM 算法求解 GMM 运行过程\" align=\"center\"><br></div>\n\n<hr>\n<h1 id=\"一般-EM-算法\"><a href=\"#一般-EM-算法\" class=\"headerlink\" title=\"一般 EM 算法\"></a>一般 EM 算法</h1><p>前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。</p>\n<p>假设观测到的变量集为 \\( {\\bf X} \\)，隐变量集为 \\( {\\bf Z} \\)，模型中所涉及到的参数集为 \\( {\\bf \\theta} \\)，我们的目的是最大化关于 \\( {\\bf X} \\) 的似然函数<br>$$<br>p({\\bf X} | {\\bf \\theta}) = \\sum_{\\bf Z} p({\\bf X}, {\\bf Z} | {\\bf \\theta})<br>\\tag {11}<br>$$<br>一般来讲，直接优化 \\( p({\\bf X} | {\\bf \\theta}) \\) 是比较困难的，而优化完全数据集的似然函数 \\( p({\\bf X}, {\\bf Z} | {\\bf \\theta}) \\) 的难度则会大大减小，EM 算法就是基于这样的思路。</p>\n<p>首先我们引入一个关于隐变量 \\( {\\bf Z} \\) 的分布 \\( q({\\bf Z}) \\)，然后我们可以将对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 分解为如下<br>$$<br>\\ln p({\\bf X} | {\\bf \\theta}) = {\\cal L}(q, {\\bf \\theta}) + {\\text KL}(q || p)<br>\\tag {12}<br>$$<br>其中<br>$$<br>{\\cal L}(q, {\\bf \\theta}) = \\sum_{\\bf Z} q({\\bf Z}) \\ln \\lbrace \\frac{p({\\bf X}, {\\bf Z} | {\\bf \\theta})} {q({\\bf Z})} \\rbrace<br>\\tag {13}<br>$$<br>$$<br>{\\text KL}(q || p) = - \\sum_{\\bf Z} q({\\bf Z}) \\ln \\lbrace \\frac{ p({\\bf Z} | {\\bf X}, {\\bf \\theta})} {q({\\bf Z})} \\rbrace<br>\\tag {14}<br>$$<br>其中 \\( {\\cal L}(q, {\\bf \\theta}) \\) 为关于概率分布 \\( q({\\bf Z}) \\) 的泛函，且为关于参数集 \\( {\\bf \\theta} \\) 的函数，另外，\\( {\\cal L}(q, {\\bf \\theta}) \\) 的表达式中包含了关于完全数据集的似然函数 \\( p({\\bf X}, {\\bf Z} | {\\bf \\theta}) \\)，这是我们需要好好加以利用的；\\( {\\text KL}(q || p) \\) 为概率分布 \\( q({\\bf Z}) \\) 与隐变量的后验概率分布 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}) \\) 间的 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">KL 散度</a>，它的值一般大于 \\( 0 \\)，只有在两个概率分布完全相同的情况下才等于 \\( 0 \\)，因而其一般被用来衡量两个概率分布之间的差异。</p>\n<p>利用 \\( {\\text KL}(q || p) \\ge 0 \\) 的性质，我们可以得到 \\( {\\cal L}(q, {\\bf \\theta}) \\le \\ln p({\\bf X} | {\\bf \\theta}) \\)，即 \\( {\\cal L}(q, {\\bf \\theta}) \\) 是对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 的一个下界。 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 与 \\( {\\cal L}(q, {\\bf \\theta}) \\)  及 \\( {\\text KL}(q || p) \\) 的关系可用下图中的图（a）来表示。 </p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_derivation.png\" width=\"1000\" height=\"600\" alt=\"EM 算法的推导过程示意图\" align=\"center\"><br></div>\n\n<p>有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。</p>\n<p>假设当前迭代步骤的参数的值为 \\( {\\bf \\theta}^{\\text {old}} \\)，我们先固定 \\( {\\bf \\theta}^{\\text {old}} \\) 的值，来求 \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 关于概率分布 \\( q({\\bf Z}) \\) 的最大值。可以看到，\\( \\ln p({\\bf X} | {\\bf \\theta} ^{\\text {old}}) \\) 现在是一个定值，所以当 \\( {\\text KL}(q || p) \\) 等于 \\( 0 \\) 时， \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 最大，如上图中的图（b）所示。此时由 \\( {\\text KL}(q || p) = 0 \\) 可以推出，\\( q({\\bf Z}) = p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)。</p>\n<p>现在再固定 \\( q({\\bf Z}) \\)，来求 \\( {\\cal L}(q, {\\bf \\theta}) \\) 关于 \\( {\\bf \\theta} \\) 的最大值，假设求得的最佳 \\( {\\bf \\theta} \\) 的值为 \\( {\\bf \\theta} ^{\\text {new}} \\)，此时 \\( {\\cal L}(q, {\\bf \\theta} ^{\\text {new}}) \\) 相比 \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 的值增大了，而由于 \\( {\\bf \\theta} \\) 值的改变又使得当前 \\( {\\text KL}(q || p) \\) 的值大于或等于 \\( 0 \\) （当算法收敛时保持 \\( 0 \\) 的值不变），所以根据式 \\( (14) \\)，对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。</p>\n<p>更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)，我们将 \\( q({\\bf Z}) = p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\) 带入  \\( {\\cal L}(q, {\\bf \\theta}) \\)  中，可得<br>$$<br>\\begin{aligned}<br>{\\cal L}(q, {\\bf \\theta}) &amp;= \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf X}, {\\bf Z} | {\\bf \\theta}) - \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\\\<br>&amp; = {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) + {\\text {const}}<br>\\end{aligned}<br>\\tag {15}<br>$$<br>我们只对上式中的第一项 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。</p>\n<p>第二个迭代步骤被称为 M 步（Maximization），是因为要对式 \\( (15) \\) 中的 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 求解关于  \\( {\\bf \\theta} \\) 的最大值，由于 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 的形式相较于对数似然函数  \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。</p>\n<p>最后还是来总结一下 EM 算法的运行过程：</p>\n<ol>\n<li>选择一个初始参数集 \\( {\\bf \\theta}^{\\text {old}} \\)；</li>\n<li><strong>E  步</strong>，计算隐变量的后验概率函数 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)；</li>\n<li><strong>M 步</strong>，按下式计算 \\( {\\bf \\theta}^{\\text {new}} \\) 的值<br>$$<br>{\\bf \\theta}^{\\text {new}} = \\rm {arg}  \\rm {max}_{\\bf \\theta} {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}})<br>\\tag {16}<br>$$<br>其中<br>$$<br>{\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) = \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf X}, {\\bf Z} | {\\bf \\theta})<br>\\tag {17}<br>$$</li>\n<li>检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 \\( {\\bf \\theta}^{\\text {old}} = {\\bf \\theta}^{\\text {new}} \\) ，回到第 2 步继续迭代。</li>\n</ol>\n<hr>\n<h1 id=\"再探高斯混合模型\"><a href=\"#再探高斯混合模型\" class=\"headerlink\" title=\"再探高斯混合模型\"></a>再探高斯混合模型</h1><p>在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 \\( (7) \\) 至 \\( (10) \\) ）。</p>\n<h2 id=\"推导过程\"><a href=\"#推导过程\" class=\"headerlink\" title=\"推导过程\"></a>推导过程</h2><p>在高斯混合模型中，参数集 \\( {\\bf \\theta} = \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\)，完整数据集 \\( \\lbrace {\\bf X}, {\\bf Z} \\rbrace \\) 的似然函数为<br>$$<br>p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\prod_{n = 1}^{N} \\prod_{k = 1}^{K} {\\pi_k}^{z_{nk}} {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)^{z_{nk}}<br>\\tag {18}<br>$$<br>对其取对数可得<br>$$<br>\\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} z_{nk} \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {19}<br>$$<br>按照 EM 算法的迭代步骤，我们先求解隐变量 \\( {\\bf Z} \\) 的后验概率函数，其具有如下形式<br>$$<br>p({\\bf Z} | {\\bf X}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\propto p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\prod_{n = 1}^{N} \\prod_{k = 1}^{K} ({\\pi_k} {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k))^{z_{nk}}<br>\\tag {20}<br>$$<br>再来推导完全数据集的对数似然函数在 \\( p({\\bf Z} | {\\bf X}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 下的期望<br>$$<br>{\\Bbb E}_{\\bf Z}[ \\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) ] = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} {\\Bbb E}[z_{nk}] \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {21}<br>$$<br>而 \\( {\\Bbb E}[z_{nk}] \\) 的值可以根据式 \\( (20) \\) 求出，由于 \\( z_{nk} \\) 只可能取 \\( 1 \\) 或 \\( 0 \\)，而取 \\( 0 \\) 时对期望没有贡献，故有<br>$$<br>{\\Bbb E}[z_{nk}] = p(z_{nk} = 1 | {\\bf x}_n, {\\bf \\pi}, {\\bf \\mu}_k, {\\bf \\Sigma}_k) = \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} = \\gamma (z_{nk})<br>\\tag {22}<br>$$<br>将上式代入公式 \\( (22) \\) 中，可得<br>$$<br>{\\Bbb E}_{\\bf Z}[ \\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) ] = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma (z_{nk}) \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {23}<br>$$<br>接下来我们就可以对该式关于参数 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 求解最大值了，可以验证，各参数的更新方程就是式 \\( (8) \\) 至 \\( (10) \\)。</p>\n<h2 id=\"与-k-means-算法的关系\"><a href=\"#与-k-means-算法的关系\" class=\"headerlink\" title=\"与 k-means 算法的关系\"></a>与 k-means 算法的关系</h2><p>敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。</p>\n<p>首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 \\( K \\) 个高斯分量，但 k-means 算法做了如下假设：</p>\n<ol>\n<li>假设每个高斯分量的先验概率相等，即 \\( \\pi_k = 1 / K \\);</li>\n<li>假设每个高斯分量的协方差矩阵均为 \\( \\epsilon {\\bf I} \\)。</li>\n</ol>\n<p>所以某一个高斯分量的概率密度函数为<br>$$<br>p({\\bf x} | {\\bf \\mu}_k, {\\bf \\Sigma}_k) = \\frac {1} {(2\\pi\\epsilon)^{D/2}} \\exp \\lbrace -\\frac {\\| {\\bf x} - {\\bf \\mu}_k \\|^{2}} {2\\epsilon} \\rbrace<br>\\tag {24}<br>$$<br>故根据 EM 算法，可求得隐变量的后验概率函数为<br>$$<br>\\gamma(z_{nk}) = \\frac{\\pi_k \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_k \\|^{2} /2\\epsilon \\rbrace } {\\sum_j \\pi_j \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_j \\|^{2} /2\\epsilon \\rbrace } = \\frac{\\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_k \\|^{2} /2\\epsilon \\rbrace } {\\sum_j \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_j \\|^{2} /2\\epsilon \\rbrace }<br>\\tag {25}<br>$$<br>在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 \\( \\gamma(z_{nk}) \\) 的值最大的那个高斯分量，即有<br>$$<br> r_{nk} = \\begin{cases} 1, &amp; \\text {if \\( k = \\rm {arg}  \\rm {min}_{j}  \\| {\\bf x}_n -  {\\bf \\mu}_j \\|^{2} \\) } \\\\ 0, &amp; \\text {otherwise} \\end{cases}<br>\\tag {26}<br>$$<br>由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 \\( {\\bf \\mu}_k \\) 求优，将式 \\( (8) \\) 中的 \\( \\gamma(z_{nk}) \\) 替换为 \\( r_{nk} \\)，即可得到 \\( {\\bf \\mu}_k \\) 的更新方法，与 k-means 算法中对中心点的更新方法一致。</p>\n<p>现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 <code>cluster</code>、期待每个 <code>cluster</code> 中的样本数量相近、期待每个 <code>cluster</code> 的密度相近。</p>\n<hr>\n<h1 id=\"实现-GMM-聚类\"><a href=\"#实现-GMM-聚类\" class=\"headerlink\" title=\"实现 GMM 聚类\"></a>实现 GMM 聚类</h1><p>前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 \\( \\gamma(z_{nk}) \\) 的值来对数据集进行聚类。\\( \\gamma(z_{nk}) \\) 给出了样本 \\( {\\bf x}_n \\) 是由 <code>cluster</code> \\( k \\) 产生的置信程度，最简单的 GMM 聚类即是将样本 \\( {\\bf x}_n \\) 分配给 \\( \\gamma(z_{nk}) \\) 值最大的 <code>cluster</code>。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 <code>GaussianMixture</code> 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。</p>\n<h2 id=\"利用-python-实现-GMM-聚类\"><a href=\"#利用-python-实现-GMM-聚类\" class=\"headerlink\" title=\"利用 python 实现 GMM 聚类\"></a>利用 python 实现 GMM 聚类</h2><p>首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GMMClust</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_components=<span class=\"number\">2</span>, max_iter=<span class=\"number\">100</span>, tol=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.n_components = n_components</div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.gamma = <span class=\"keyword\">None</span></div><div class=\"line\">        self.component_prob = <span class=\"keyword\">None</span></div><div class=\"line\">        self.means = <span class=\"keyword\">None</span></div><div class=\"line\">        self.covars = <span class=\"keyword\">None</span></div><div class=\"line\">        self.max_iter = max_iter</div><div class=\"line\">        self.tol = tol</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 计算高斯分布的概率密度函数 </span></div><div class=\"line\"><span class=\"meta\">    @staticmethod</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_gaussian_prob</span><span class=\"params\">(x, mean, covar, delta=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        n_dim = x.shape[<span class=\"number\">0</span>]</div><div class=\"line\">        covar = covar + delta * np.eye(n_dim)</div><div class=\"line\">        prob = np.exp(<span class=\"number\">-0.5</span> * np.dot((x - mean).reshape(<span class=\"number\">1</span>, n_dim),</div><div class=\"line\">                                    np.dot(np.linalg.inv(covar),</div><div class=\"line\">                                           (x - mean).reshape(n_dim, <span class=\"number\">1</span>))))</div><div class=\"line\">        prob /= np.sqrt(np.linalg.det(covar) * ((<span class=\"number\">2</span> * np.pi) ** n_dim))</div><div class=\"line\">        <span class=\"keyword\">return</span> prob</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 计算每一个样本点的似然函数 </span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_sample_likelihood</span><span class=\"params\">(self, i)</span>:</span></div><div class=\"line\">        sample_likelihood = sum(self.component_prob[k] *</div><div class=\"line\">                                self.cal_gaussian_prob(self.data_set[i],</div><div class=\"line\">                                                       self.means[k], self.covars[k])</div><div class=\"line\">                                <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components))</div><div class=\"line\">        <span class=\"keyword\">return</span> sample_likelihood</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        self.n_samples, self.n_features = self.data_set.shape</div><div class=\"line\">        self.pred_label = np.zeros(self.n_samples, dtype=int)</div><div class=\"line\">        self.gamma = np.zeros((self.n_samples, self.n_components))</div><div class=\"line\"></div><div class=\"line\">        start_time = time.time()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 初始化各参数</span></div><div class=\"line\">        self.component_prob = [<span class=\"number\">1.0</span> / self.n_components] * self.n_components</div><div class=\"line\">        self.means = np.random.rand(self.n_components, self.n_features)</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_features):</div><div class=\"line\">            dim_min = np.min(self.data_set[:, i])</div><div class=\"line\">            dim_max = np.max(self.data_set[:, i])</div><div class=\"line\">            self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i]</div><div class=\"line\">        self.covars = np.zeros((self.n_components, self.n_features, self.n_features))</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">            self.covars[i] = np.eye(self.n_features)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始迭代</span></div><div class=\"line\">        pre_L = <span class=\"number\">0</span></div><div class=\"line\">        iter_cnt = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">while</span> iter_cnt &lt; self.max_iter:</div><div class=\"line\">            iter_cnt += <span class=\"number\">1</span></div><div class=\"line\">            crt_L = <span class=\"number\">0</span></div><div class=\"line\">            <span class=\"comment\"># E 步</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">                sample_likelihood = self.cal_sample_likelihood(i)</div><div class=\"line\">                crt_L += np.log(sample_likelihood)</div><div class=\"line\">                <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">                    self.gamma[i, k] = self.component_prob[k] * \\</div><div class=\"line\">                                       self.cal_gaussian_prob(self.data_set[i],</div><div class=\"line\">                                                              self.means[k],</div><div class=\"line\">                                                              self.covars[k]) / sample_likelihood</div><div class=\"line\">            <span class=\"comment\"># M 步</span></div><div class=\"line\">            effective_num = np.sum(self.gamma, axis=<span class=\"number\">0</span>)</div><div class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">                self.means[k] = sum(self.gamma[i, k] * self.data_set[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples))</div><div class=\"line\">                self.means[k] /= effective_num[k]</div><div class=\"line\">                self.covars[k] = sum(self.gamma[i, k] *</div><div class=\"line\">                                     np.outer(self.data_set[i] - self.means[k],</div><div class=\"line\">                                              self.data_set[i] - self.means[k])</div><div class=\"line\">                                     <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples))</div><div class=\"line\">                self.covars[k] /= effective_num[k]</div><div class=\"line\">                self.component_prob[k] = effective_num[k] / self.n_samples</div><div class=\"line\"></div><div class=\"line\">            print(<span class=\"string\">\"iteration %s, current value of the log likelihood: %.4f\"</span> % (iter_cnt, crt_L))</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span> abs(crt_L - pre_L) &lt; self.tol:</div><div class=\"line\">                <span class=\"keyword\">break</span></div><div class=\"line\">            pre_L = crt_L</div><div class=\"line\"></div><div class=\"line\">        self.pred_label = np.argmax(self.gamma, axis=<span class=\"number\">1</span>)</div><div class=\"line\">        print(<span class=\"string\">\"total iteration num: %s, final value of the log likelihood: %.4f, \"</span></div><div class=\"line\">              <span class=\"string\">\"time used: %.4f seconds\"</span> % (iter_cnt, crt_L, time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化算法的聚类结果</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(self, kind, y=None, title=None)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> kind == <span class=\"number\">1</span>:</div><div class=\"line\">            y = self.pred_label</div><div class=\"line\">        plt.scatter(self.data_set[:, <span class=\"number\">0</span>], self.data_set[:, <span class=\"number\">1</span>],</div><div class=\"line\">                    c=y, alpha=<span class=\"number\">0.8</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> kind == <span class=\"number\">1</span>:</div><div class=\"line\">            plt.scatter(self.means[:, <span class=\"number\">0</span>], self.means[:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=<span class=\"string\">'r'</span>, marker=<span class=\"string\">'x'</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">            plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">        plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">        plt.tight_layout()</div></pre></td></tr></table></figure></p>\n<p>创建一个 <code>GMMClust</code> 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 <code>predict</code> 即可对给定的数据集进行 GMM 聚类；方法 <code>plot_clustering</code> 则可以可视化聚类的结果。利用 <code>GMMClust</code> 类进行 GMM 聚类的代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 生成数据集</span></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">centers = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">8</span>, <span class=\"number\">3.5</span>]]</div><div class=\"line\">cluster_std = [<span class=\"number\">2</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>]</div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 运行 GMM 聚类算法</span></div><div class=\"line\">gmm_cluster = GMMClust(n_components=<span class=\"number\">3</span>)</div><div class=\"line\">gmm_cluster.predict(X)</div><div class=\"line\">   <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>):</div><div class=\"line\">       print(<span class=\"string\">\"cluster %s\"</span> % i)</div><div class=\"line\">       print(<span class=\"string\">\"    mean: %s, covariance: %s\"</span> %(gmm_cluster.means[i], gmm_cluster.covars[i]))</div><div class=\"line\"></div><div class=\"line\">   <span class=\"comment\"># 可视化数据集的原始类别情况以及算法的聚类结果</span></div><div class=\"line\">   plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">   gmm_cluster.plot_clustering(kind=<span class=\"number\">0</span>, y=y, title=<span class=\"string\">'The Original Dataset'</span>)</div><div class=\"line\">   plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">   gmm_cluster.plot_clustering(kind=<span class=\"number\">1</span>, title=<span class=\"string\">'GMM Clustering Result'</span>)</div><div class=\"line\">   plt.show()</div></pre></td></tr></table></figure></p>\n<p>以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">iteration 1, current value of the log likelihood: -15761.9757</div><div class=\"line\">iteration 2, current value of the log likelihood: -6435.3937</div><div class=\"line\">iteration 3, current value of the log likelihood: -6410.5633</div><div class=\"line\">iteration 4, current value of the log likelihood: -6399.4306</div><div class=\"line\">iteration 5, current value of the log likelihood: -6389.0317</div><div class=\"line\">iteration 6, current value of the log likelihood: -6377.9131</div><div class=\"line\">iteration 7, current value of the log likelihood: -6367.5704</div><div class=\"line\">iteration 8, current value of the log likelihood: -6359.2076</div><div class=\"line\">iteration 9, current value of the log likelihood: -6350.8678</div><div class=\"line\">iteration 10, current value of the log likelihood: -6338.6458</div><div class=\"line\">... ...</div><div class=\"line\">iteration 35, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 36, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 37, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 38, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 39, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 40, current value of the log likelihood: -5859.0324</div><div class=\"line\">total iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 seconds</div><div class=\"line\">cluster 0</div><div class=\"line\">    mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269]</div><div class=\"line\"> [0.08460269 3.95599185]]</div><div class=\"line\">cluster 1</div><div class=\"line\">    mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461  -0.00345936]</div><div class=\"line\"> [-0.00345936  0.9630804 ]]</div><div class=\"line\">cluster 2</div><div class=\"line\">    mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954]</div><div class=\"line\"> [0.01120954 0.27281129]]</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_result.png\" width=\"1000\" height=\"500\" alt=\"GMM 聚类结果\" align=\"center\"><br></div>\n\n<p>可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 <code>cluster</code> 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 <code>cluster</code> 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。</p>\n<h2 id=\"利用-sklearn-实现-GMM-聚类\"><a href=\"#利用-sklearn-实现-GMM-聚类\" class=\"headerlink\" title=\"利用 sklearn 实现 GMM 聚类\"></a>利用 sklearn 实现 GMM 聚类</h2><p>sklearn 中的  <code>GaussianMixture</code> 类可以用来进行 GMM 聚类，其中的 <code>fit</code> 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；<code>predict</code> 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>, <span class=\"number\">12</span>))</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个理想的数据集</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X)</div><div class=\"line\">y_pred = gmm.predict(X)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">221</span>)</div><div class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Normal Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个非球形分布的数据集</span></div><div class=\"line\">transformation = [[<span class=\"number\">0.60834549</span>, <span class=\"number\">-0.63667341</span>], [<span class=\"number\">-0.40887718</span>, <span class=\"number\">0.85253229</span>]]</div><div class=\"line\">X_aniso = np.dot(X, transformation)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_aniso)</div><div class=\"line\">y_pred = gmm.predict(X_aniso)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">222</span>)</div><div class=\"line\">plt.scatter(X_aniso[:, <span class=\"number\">0</span>], X_aniso[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Anisotropicly Distributed Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的密度不一致的数据集</span></div><div class=\"line\">X_varied, y_varied = make_blobs(n_samples=n_samples,</div><div class=\"line\">                                cluster_std=[<span class=\"number\">1.0</span>, <span class=\"number\">2.5</span>, <span class=\"number\">0.5</span>],</div><div class=\"line\">                                random_state=random_state)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_varied)</div><div class=\"line\">y_pred = gmm.predict(X_varied)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">223</span>)</div><div class=\"line\">plt.scatter(X_varied[:, <span class=\"number\">0</span>], X_varied[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unequal Density Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的样本数目不一致的数据集</span></div><div class=\"line\">X_filtered = np.vstack((X[y == <span class=\"number\">0</span>][:<span class=\"number\">500</span>], X[y == <span class=\"number\">1</span>][:<span class=\"number\">100</span>], X[y == <span class=\"number\">2</span>][:<span class=\"number\">50</span>]))</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_filtered)</div><div class=\"line\">y_pred = gmm.predict(X_filtered)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">224</span>)</div><div class=\"line\">plt.scatter(X_filtered[:, <span class=\"number\">0</span>], X_filtered[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unevenly Sized Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_different_datasets.png\" width=\"780\" height=\"650\" alt=\"k-means 算法在不同数据集下的表现\" align=\"center\"><br></div>\n\n<p>可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 <code>cluster</code>，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 <code>cluster</code> 都是超椭球形的。</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<p><em>本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 <a href=\"../clustering-analysis/index.html\"><strong>聚类分析系列文章</strong></a></em>。</p>\n<h1 id=\"高斯混合模型\"><a href=\"#高斯混合模型\" class=\"headerlink\" title=\"高斯混合模型\"></a>高斯混合模型</h1><h2 id=\"高斯混合模型简介\"><a href=\"#高斯混合模型简介\" class=\"headerlink\" title=\"高斯混合模型简介\"></a>高斯混合模型简介</h2><p>高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：<br>$$<br>p({\\bf x}) = \\sum_{k = 1}^{K} \\pi_{k} {\\cal N}({\\bf x}|{\\bf \\mu}_k, {\\bf \\Sigma}_k)<br>\\tag {1}<br>$$<br>如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 \\( K \\) 个类别中以 \\( \\pi_{k} \\) 的概率随机选择一个类别 \\( k \\)，然后再依照该类别所对应的高斯分布 \\( {\\cal N}({\\bf x}|{\\bf \\mu}_k, {\\bf \\Sigma}_k) \\) 随机产生一个数据 \\( {\\bf x} \\)。但最终生成数据集后，我们所观察到的仅仅只是 \\( {\\bf x} \\)，而观察不到用于产生 \\( {\\bf x} \\) 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 \\( {\\bf z} \\in {\\lbrace 0, 1 \\rbrace}^{K} \\) 来表示高斯混合模型中的类别变量，\\( {\\bf z} \\) 中仅有一个元素的值为 \\( 1 \\)，而其它元素的值为 \\( 0 \\)，例如当 \\( z_{k} = 1\\) 时，表示当前数据是由高斯混合分布中的第 \\( k \\) 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 \\( {\\bf z} \\) 的概率质量函数为<br>$$<br>p(z_{k} = 1) = \\pi_k<br>\\tag {2}<br>$$<br>其中 \\( \\lbrace \\pi_{k} \\rbrace \\) 须满足<br>$$<br>\\sum_{k = 1}^{K} \\pi_k = 1  \\text{   ,   }  0 \\le \\pi_k \\le 1<br>$$<br>给定 \\( {\\bf z} \\) 的值的情况下，\\( {\\bf x} \\) 服从高斯分布<br>$$<br>p({\\bf x} | z_{k} = 1) = {\\cal N}({\\bf x} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k})<br>\\tag {3}<br>$$<br>因而可以得到 \\( {\\bf x} \\) 的边缘概率分布为<br>$$<br> p({\\bf x}) = \\sum_{\\bf z} p({\\bf z})p({\\bf x} | {\\bf z}) = \\sum_{k =1}^{K} \\pi_k {\\cal N}({\\bf x} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k})<br>\\tag {4}<br>$$<br>该分布就是我们前面所看到的高斯混合分布。</p>\n<h2 id=\"最大似然估计问题\"><a href=\"#最大似然估计问题\" class=\"headerlink\" title=\"最大似然估计问题\"></a>最大似然估计问题</h2><p>假设有数据集 \\( \\lbrace {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N \\rbrace \\)，其中样本的维度为 \\( D \\)，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 \\( {\\bf X} = [ {\\bf x}_1, {\\bf x}_2, …, {\\bf x}_N ]^{T} \\in {\\Bbb R}^{N \\times D} \\) 来表示数据集，以矩阵 \\( {\\bf Z} = [ {\\bf z}_1, {\\bf z}_2, …, {\\bf z}_N ]^{T} \\in {\\Bbb R}^{N \\times K} \\) 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为<br>$$<br> L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\ln p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\sum_{n = 1}^{N} \\ln \\lbrace  \\sum_{k = 1}^{K} \\pi_k {\\cal N}({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\bf \\Sigma}_{k}) \\rbrace<br>\\tag {5}<br>$$<br>我们希望求解出使得以上对数似然函数最大的参数集 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\)，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。</p>\n<p> 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 \\( L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 发散的解。具体地，观察公式 \\( (5) \\)，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 \\( 0 \\)）的时候，会导致求和项中的该项的值为无穷大，从而也使得 \\( L({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 的值为无穷大；这样确实使得式 \\( (5) \\) 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。</p>\n<hr>\n<h1 id=\"EM-算法求解高斯混合模型\"><a href=\"#EM-算法求解高斯混合模型\" class=\"headerlink\" title=\"EM 算法求解高斯混合模型\"></a>EM 算法求解高斯混合模型</h1><p>EM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。</p>\n<p>我们对对数似然函数 \\( (5) \\) 关于各参数 \\( {\\bf \\pi} \\)， \\( {\\bf \\mu} \\)， \\( {\\bf \\Sigma} \\) 分别求偏导，并将其置为 \\( 0 \\) 可以得到一系列的方程，而使得式 \\( (5) \\) 最大的解也一定满足这些方程。</p>\n<p>首先令式 \\( (5) \\) 关于 \\( {\\bf \\mu}_k \\) 的偏导为 \\( 0 \\) 可得以下方程：<br>$$<br>- \\sum_{n = 1}^{N} \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} {\\bf \\Sigma}_k ({\\bf x}_n - {\\bf \\mu}_k) = 0<br>\\tag {6}<br>$$<br>注意到，上式中含有项<br>$$<br>\\gamma (z_{nk}) =   \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} = p(z_{nk} = 1 | {\\bf x}_n)<br>\\tag {7}<br>$$<br>该项具有重要的物理意义，它为给定样本点 \\( {\\bf x}_n \\) 后隐变量 \\( {\\bf z}_n \\) 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 \\( \\gamma (z_{nk}) \\) 的值来做软分配的。</p>\n<p>因而，我们可以由式 \\( (6) \\) 和式 \\( (7) \\) 写出<br>$$<br>{\\bf \\mu}_k = \\frac{1} {N_k} \\sum_{n = 1}^{N} \\gamma(z_{nk}) {\\bf x}_n<br>\\tag {8}<br>$$<br>其中 \\( N_k = \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\)，我们可以将 \\( N_{k} \\) 解释为被分配给类别 \\( k \\) 的有效样本数量，而 \\( {\\bf \\mu}_{k} \\) 即为所有样本点的加权算术平均值，每个样本点的权重等于第 \\( k\\) 个高斯分量对产生该样本点所负有的“责任”。</p>\n<p>我们再将式 \\( (5) \\) 对 \\( {\\bf \\Sigma}_k \\) 的偏导数置为 \\( 0 \\) 可求得<br>$$<br>{\\bf \\Sigma}_k = \\frac{1} {N_k} \\sum_{n = 1}^{N} \\gamma(z_{nk}) ({\\bf x}_n - {\\bf \\mu}_k)({\\bf x}_n - {\\bf \\mu}_k)^{\\text T}<br>\\tag {9}<br>$$<br>可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 \\( \\gamma(z_{nk}) \\)。</p>\n<p>最后我们再来推导 \\( \\pi_k \\) 的最大似然解须满足的条件。由于 \\( \\pi_k \\) 有归一化的约束，我们可以利用 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\"> Lagrange 乘数法 </a> 来求解（将 \\( \\ln p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) + \\lambda (\\sum_{k = 1}^{K} \\pi_k - 1) \\) 关于 \\( \\pi_k \\) 的偏导数置 \\( 0 \\)），最后可求得<br>$$<br>\\pi_k = \\frac{N_k} {N}<br>\\tag {10}<br>$$<br>关于类别 \\( k \\) 的先验概率的估计值可以理解为所有样本点中被分配给第 \\( k \\) 个类别的有效样本点的个数占总样本数量的比例。</p>\n<p>注意到，式 \\( (8) \\) 至 \\( (10) \\) 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 的闭式解，因为这些式子中给出的表达式中都含有 \\( \\gamma (z_{nk}) \\)，而 \\( \\gamma (z_{nk}) \\) 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 \\( (7) \\)）。</p>\n<p>尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 固定为一个初始值，再按式 \\( (7) \\) 计算出隐变量的后验概率 \\( \\gamma (z_{nk}) \\) （E 步）；然后再固定 \\( \\gamma (z_{nk}) \\)，按式 \\( (8) \\) 至 \\( (10) \\) 分别更新参数  \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。</p>\n<p>实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。</p>\n<p>我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率  \\( \\gamma (z_{nk}) \\)，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 \\( p_1 \\) ，由红色的高斯分量产生的概率为 \\( p_2 \\)，则我们将其染上 \\( p_1 \\) 比例的蓝色，染上  \\( p_2 \\) 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_for_GMM.png\" width=\"660\" height=\"550\" alt=\"EM 算法求解 GMM 运行过程\" align=\"center\"><br></div>\n\n<hr>\n<h1 id=\"一般-EM-算法\"><a href=\"#一般-EM-算法\" class=\"headerlink\" title=\"一般 EM 算法\"></a>一般 EM 算法</h1><p>前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。</p>\n<p>假设观测到的变量集为 \\( {\\bf X} \\)，隐变量集为 \\( {\\bf Z} \\)，模型中所涉及到的参数集为 \\( {\\bf \\theta} \\)，我们的目的是最大化关于 \\( {\\bf X} \\) 的似然函数<br>$$<br>p({\\bf X} | {\\bf \\theta}) = \\sum_{\\bf Z} p({\\bf X}, {\\bf Z} | {\\bf \\theta})<br>\\tag {11}<br>$$<br>一般来讲，直接优化 \\( p({\\bf X} | {\\bf \\theta}) \\) 是比较困难的，而优化完全数据集的似然函数 \\( p({\\bf X}, {\\bf Z} | {\\bf \\theta}) \\) 的难度则会大大减小，EM 算法就是基于这样的思路。</p>\n<p>首先我们引入一个关于隐变量 \\( {\\bf Z} \\) 的分布 \\( q({\\bf Z}) \\)，然后我们可以将对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 分解为如下<br>$$<br>\\ln p({\\bf X} | {\\bf \\theta}) = {\\cal L}(q, {\\bf \\theta}) + {\\text KL}(q || p)<br>\\tag {12}<br>$$<br>其中<br>$$<br>{\\cal L}(q, {\\bf \\theta}) = \\sum_{\\bf Z} q({\\bf Z}) \\ln \\lbrace \\frac{p({\\bf X}, {\\bf Z} | {\\bf \\theta})} {q({\\bf Z})} \\rbrace<br>\\tag {13}<br>$$<br>$$<br>{\\text KL}(q || p) = - \\sum_{\\bf Z} q({\\bf Z}) \\ln \\lbrace \\frac{ p({\\bf Z} | {\\bf X}, {\\bf \\theta})} {q({\\bf Z})} \\rbrace<br>\\tag {14}<br>$$<br>其中 \\( {\\cal L}(q, {\\bf \\theta}) \\) 为关于概率分布 \\( q({\\bf Z}) \\) 的泛函，且为关于参数集 \\( {\\bf \\theta} \\) 的函数，另外，\\( {\\cal L}(q, {\\bf \\theta}) \\) 的表达式中包含了关于完全数据集的似然函数 \\( p({\\bf X}, {\\bf Z} | {\\bf \\theta}) \\)，这是我们需要好好加以利用的；\\( {\\text KL}(q || p) \\) 为概率分布 \\( q({\\bf Z}) \\) 与隐变量的后验概率分布 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}) \\) 间的 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence?oldformat=true\" rel=\"external nofollow noopener noreferrer\" target=\"_blank\">KL 散度</a>，它的值一般大于 \\( 0 \\)，只有在两个概率分布完全相同的情况下才等于 \\( 0 \\)，因而其一般被用来衡量两个概率分布之间的差异。</p>\n<p>利用 \\( {\\text KL}(q || p) \\ge 0 \\) 的性质，我们可以得到 \\( {\\cal L}(q, {\\bf \\theta}) \\le \\ln p({\\bf X} | {\\bf \\theta}) \\)，即 \\( {\\cal L}(q, {\\bf \\theta}) \\) 是对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 的一个下界。 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 与 \\( {\\cal L}(q, {\\bf \\theta}) \\)  及 \\( {\\text KL}(q || p) \\) 的关系可用下图中的图（a）来表示。 </p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/illustration_of_EM_derivation.png\" width=\"1000\" height=\"600\" alt=\"EM 算法的推导过程示意图\" align=\"center\"><br></div>\n\n<p>有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。</p>\n<p>假设当前迭代步骤的参数的值为 \\( {\\bf \\theta}^{\\text {old}} \\)，我们先固定 \\( {\\bf \\theta}^{\\text {old}} \\) 的值，来求 \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 关于概率分布 \\( q({\\bf Z}) \\) 的最大值。可以看到，\\( \\ln p({\\bf X} | {\\bf \\theta} ^{\\text {old}}) \\) 现在是一个定值，所以当 \\( {\\text KL}(q || p) \\) 等于 \\( 0 \\) 时， \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 最大，如上图中的图（b）所示。此时由 \\( {\\text KL}(q || p) = 0 \\) 可以推出，\\( q({\\bf Z}) = p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)。</p>\n<p>现在再固定 \\( q({\\bf Z}) \\)，来求 \\( {\\cal L}(q, {\\bf \\theta}) \\) 关于 \\( {\\bf \\theta} \\) 的最大值，假设求得的最佳 \\( {\\bf \\theta} \\) 的值为 \\( {\\bf \\theta} ^{\\text {new}} \\)，此时 \\( {\\cal L}(q, {\\bf \\theta} ^{\\text {new}}) \\) 相比 \\( {\\cal L}(q, {\\bf \\theta}^{\\text {old}}) \\) 的值增大了，而由于 \\( {\\bf \\theta} \\) 值的改变又使得当前 \\( {\\text KL}(q || p) \\) 的值大于或等于 \\( 0 \\) （当算法收敛时保持 \\( 0 \\) 的值不变），所以根据式 \\( (14) \\)，对数似然函数 \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。</p>\n<p>更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)，我们将 \\( q({\\bf Z}) = p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\) 带入  \\( {\\cal L}(q, {\\bf \\theta}) \\)  中，可得<br>$$<br>\\begin{aligned}<br>{\\cal L}(q, {\\bf \\theta}) &amp;= \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf X}, {\\bf Z} | {\\bf \\theta}) - \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\\\<br>&amp; = {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) + {\\text {const}}<br>\\end{aligned}<br>\\tag {15}<br>$$<br>我们只对上式中的第一项 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。</p>\n<p>第二个迭代步骤被称为 M 步（Maximization），是因为要对式 \\( (15) \\) 中的 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 求解关于  \\( {\\bf \\theta} \\) 的最大值，由于 \\( {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) \\) 的形式相较于对数似然函数  \\( \\ln p({\\bf X} | {\\bf \\theta}) \\) 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。</p>\n<p>最后还是来总结一下 EM 算法的运行过程：</p>\n<ol>\n<li>选择一个初始参数集 \\( {\\bf \\theta}^{\\text {old}} \\)；</li>\n<li><strong>E  步</strong>，计算隐变量的后验概率函数 \\( p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\)；</li>\n<li><strong>M 步</strong>，按下式计算 \\( {\\bf \\theta}^{\\text {new}} \\) 的值<br>$$<br>{\\bf \\theta}^{\\text {new}} = \\rm {arg}  \\rm {max}_{\\bf \\theta} {\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}})<br>\\tag {16}<br>$$<br>其中<br>$$<br>{\\cal Q} ({\\bf \\theta}, {\\bf \\theta}^{\\text {old}}) = \\sum_{\\bf Z} p({\\bf Z} | {\\bf X}, {\\bf \\theta}^{\\text {old}}) \\ln p({\\bf X}, {\\bf Z} | {\\bf \\theta})<br>\\tag {17}<br>$$</li>\n<li>检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 \\( {\\bf \\theta}^{\\text {old}} = {\\bf \\theta}^{\\text {new}} \\) ，回到第 2 步继续迭代。</li>\n</ol>\n<hr>\n<h1 id=\"再探高斯混合模型\"><a href=\"#再探高斯混合模型\" class=\"headerlink\" title=\"再探高斯混合模型\"></a>再探高斯混合模型</h1><p>在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 \\( (7) \\) 至 \\( (10) \\) ）。</p>\n<h2 id=\"推导过程\"><a href=\"#推导过程\" class=\"headerlink\" title=\"推导过程\"></a>推导过程</h2><p>在高斯混合模型中，参数集 \\( {\\bf \\theta} = \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\)，完整数据集 \\( \\lbrace {\\bf X}, {\\bf Z} \\rbrace \\) 的似然函数为<br>$$<br>p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\prod_{n = 1}^{N} \\prod_{k = 1}^{K} {\\pi_k}^{z_{nk}} {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)^{z_{nk}}<br>\\tag {18}<br>$$<br>对其取对数可得<br>$$<br>\\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} z_{nk} \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {19}<br>$$<br>按照 EM 算法的迭代步骤，我们先求解隐变量 \\( {\\bf Z} \\) 的后验概率函数，其具有如下形式<br>$$<br>p({\\bf Z} | {\\bf X}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\propto p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) = \\prod_{n = 1}^{N} \\prod_{k = 1}^{K} ({\\pi_k} {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k))^{z_{nk}}<br>\\tag {20}<br>$$<br>再来推导完全数据集的对数似然函数在 \\( p({\\bf Z} | {\\bf X}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) \\) 下的期望<br>$$<br>{\\Bbb E}_{\\bf Z}[ \\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) ] = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} {\\Bbb E}[z_{nk}] \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {21}<br>$$<br>而 \\( {\\Bbb E}[z_{nk}] \\) 的值可以根据式 \\( (20) \\) 求出，由于 \\( z_{nk} \\) 只可能取 \\( 1 \\) 或 \\( 0 \\)，而取 \\( 0 \\) 时对期望没有贡献，故有<br>$$<br>{\\Bbb E}[z_{nk}] = p(z_{nk} = 1 | {\\bf x}_n, {\\bf \\pi}, {\\bf \\mu}_k, {\\bf \\Sigma}_k) = \\frac{\\pi_k {\\cal N}({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k)} { \\sum_{j} \\pi_j {\\cal N} ({\\bf x}_n | {\\bf \\mu}_j, {\\bf \\Sigma}_j)} = \\gamma (z_{nk})<br>\\tag {22}<br>$$<br>将上式代入公式 \\( (22) \\) 中，可得<br>$$<br>{\\Bbb E}_{\\bf Z}[ \\ln p({\\bf X}, {\\bf Z} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma}) ] = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma (z_{nk}) \\lbrace \\ln \\pi_k + \\ln {\\cal N} ({\\bf x}_n | {\\bf \\mu}_k, {\\bf \\Sigma}_k) \\rbrace<br>\\tag {23}<br>$$<br>接下来我们就可以对该式关于参数 \\( \\lbrace {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Sigma} \\rbrace \\) 求解最大值了，可以验证，各参数的更新方程就是式 \\( (8) \\) 至 \\( (10) \\)。</p>\n<h2 id=\"与-k-means-算法的关系\"><a href=\"#与-k-means-算法的关系\" class=\"headerlink\" title=\"与 k-means 算法的关系\"></a>与 k-means 算法的关系</h2><p>敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。</p>\n<p>首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 \\( K \\) 个高斯分量，但 k-means 算法做了如下假设：</p>\n<ol>\n<li>假设每个高斯分量的先验概率相等，即 \\( \\pi_k = 1 / K \\);</li>\n<li>假设每个高斯分量的协方差矩阵均为 \\( \\epsilon {\\bf I} \\)。</li>\n</ol>\n<p>所以某一个高斯分量的概率密度函数为<br>$$<br>p({\\bf x} | {\\bf \\mu}_k, {\\bf \\Sigma}_k) = \\frac {1} {(2\\pi\\epsilon)^{D/2}} \\exp \\lbrace -\\frac {\\| {\\bf x} - {\\bf \\mu}_k \\|^{2}} {2\\epsilon} \\rbrace<br>\\tag {24}<br>$$<br>故根据 EM 算法，可求得隐变量的后验概率函数为<br>$$<br>\\gamma(z_{nk}) = \\frac{\\pi_k \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_k \\|^{2} /2\\epsilon \\rbrace } {\\sum_j \\pi_j \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_j \\|^{2} /2\\epsilon \\rbrace } = \\frac{\\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_k \\|^{2} /2\\epsilon \\rbrace } {\\sum_j \\exp \\lbrace -\\| {\\bf x} - {\\bf \\mu}_j \\|^{2} /2\\epsilon \\rbrace }<br>\\tag {25}<br>$$<br>在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 \\( \\gamma(z_{nk}) \\) 的值最大的那个高斯分量，即有<br>$$<br> r_{nk} = \\begin{cases} 1, &amp; \\text {if \\( k = \\rm {arg}  \\rm {min}_{j}  \\| {\\bf x}_n -  {\\bf \\mu}_j \\|^{2} \\) } \\\\ 0, &amp; \\text {otherwise} \\end{cases}<br>\\tag {26}<br>$$<br>由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 \\( {\\bf \\mu}_k \\) 求优，将式 \\( (8) \\) 中的 \\( \\gamma(z_{nk}) \\) 替换为 \\( r_{nk} \\)，即可得到 \\( {\\bf \\mu}_k \\) 的更新方法，与 k-means 算法中对中心点的更新方法一致。</p>\n<p>现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 <code>cluster</code>、期待每个 <code>cluster</code> 中的样本数量相近、期待每个 <code>cluster</code> 的密度相近。</p>\n<hr>\n<h1 id=\"实现-GMM-聚类\"><a href=\"#实现-GMM-聚类\" class=\"headerlink\" title=\"实现 GMM 聚类\"></a>实现 GMM 聚类</h1><p>前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 \\( \\gamma(z_{nk}) \\) 的值来对数据集进行聚类。\\( \\gamma(z_{nk}) \\) 给出了样本 \\( {\\bf x}_n \\) 是由 <code>cluster</code> \\( k \\) 产生的置信程度，最简单的 GMM 聚类即是将样本 \\( {\\bf x}_n \\) 分配给 \\( \\gamma(z_{nk}) \\) 值最大的 <code>cluster</code>。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 <code>GaussianMixture</code> 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。</p>\n<h2 id=\"利用-python-实现-GMM-聚类\"><a href=\"#利用-python-实现-GMM-聚类\" class=\"headerlink\" title=\"利用 python 实现 GMM 聚类\"></a>利用 python 实现 GMM 聚类</h2><p>首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"><span class=\"keyword\">import</span> time</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GMMClust</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_components=<span class=\"number\">2</span>, max_iter=<span class=\"number\">100</span>, tol=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        self.data_set = <span class=\"keyword\">None</span></div><div class=\"line\">        self.n_components = n_components</div><div class=\"line\">        self.pred_label = <span class=\"keyword\">None</span></div><div class=\"line\">        self.gamma = <span class=\"keyword\">None</span></div><div class=\"line\">        self.component_prob = <span class=\"keyword\">None</span></div><div class=\"line\">        self.means = <span class=\"keyword\">None</span></div><div class=\"line\">        self.covars = <span class=\"keyword\">None</span></div><div class=\"line\">        self.max_iter = max_iter</div><div class=\"line\">        self.tol = tol</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 计算高斯分布的概率密度函数 </span></div><div class=\"line\"><span class=\"meta\">    @staticmethod</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_gaussian_prob</span><span class=\"params\">(x, mean, covar, delta=<span class=\"number\">1e-10</span>)</span>:</span></div><div class=\"line\">        n_dim = x.shape[<span class=\"number\">0</span>]</div><div class=\"line\">        covar = covar + delta * np.eye(n_dim)</div><div class=\"line\">        prob = np.exp(<span class=\"number\">-0.5</span> * np.dot((x - mean).reshape(<span class=\"number\">1</span>, n_dim),</div><div class=\"line\">                                    np.dot(np.linalg.inv(covar),</div><div class=\"line\">                                           (x - mean).reshape(n_dim, <span class=\"number\">1</span>))))</div><div class=\"line\">        prob /= np.sqrt(np.linalg.det(covar) * ((<span class=\"number\">2</span> * np.pi) ** n_dim))</div><div class=\"line\">        <span class=\"keyword\">return</span> prob</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 计算每一个样本点的似然函数 </span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cal_sample_likelihood</span><span class=\"params\">(self, i)</span>:</span></div><div class=\"line\">        sample_likelihood = sum(self.component_prob[k] *</div><div class=\"line\">                                self.cal_gaussian_prob(self.data_set[i],</div><div class=\"line\">                                                       self.means[k], self.covars[k])</div><div class=\"line\">                                <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components))</div><div class=\"line\">        <span class=\"keyword\">return</span> sample_likelihood</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, data_set)</span>:</span></div><div class=\"line\">        self.data_set = data_set</div><div class=\"line\">        self.n_samples, self.n_features = self.data_set.shape</div><div class=\"line\">        self.pred_label = np.zeros(self.n_samples, dtype=int)</div><div class=\"line\">        self.gamma = np.zeros((self.n_samples, self.n_components))</div><div class=\"line\"></div><div class=\"line\">        start_time = time.time()</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 初始化各参数</span></div><div class=\"line\">        self.component_prob = [<span class=\"number\">1.0</span> / self.n_components] * self.n_components</div><div class=\"line\">        self.means = np.random.rand(self.n_components, self.n_features)</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_features):</div><div class=\"line\">            dim_min = np.min(self.data_set[:, i])</div><div class=\"line\">            dim_max = np.max(self.data_set[:, i])</div><div class=\"line\">            self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i]</div><div class=\"line\">        self.covars = np.zeros((self.n_components, self.n_features, self.n_features))</div><div class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">            self.covars[i] = np.eye(self.n_features)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\"># 开始迭代</span></div><div class=\"line\">        pre_L = <span class=\"number\">0</span></div><div class=\"line\">        iter_cnt = <span class=\"number\">0</span></div><div class=\"line\">        <span class=\"keyword\">while</span> iter_cnt &lt; self.max_iter:</div><div class=\"line\">            iter_cnt += <span class=\"number\">1</span></div><div class=\"line\">            crt_L = <span class=\"number\">0</span></div><div class=\"line\">            <span class=\"comment\"># E 步</span></div><div class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples):</div><div class=\"line\">                sample_likelihood = self.cal_sample_likelihood(i)</div><div class=\"line\">                crt_L += np.log(sample_likelihood)</div><div class=\"line\">                <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">                    self.gamma[i, k] = self.component_prob[k] * \\</div><div class=\"line\">                                       self.cal_gaussian_prob(self.data_set[i],</div><div class=\"line\">                                                              self.means[k],</div><div class=\"line\">                                                              self.covars[k]) / sample_likelihood</div><div class=\"line\">            <span class=\"comment\"># M 步</span></div><div class=\"line\">            effective_num = np.sum(self.gamma, axis=<span class=\"number\">0</span>)</div><div class=\"line\">            <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(self.n_components):</div><div class=\"line\">                self.means[k] = sum(self.gamma[i, k] * self.data_set[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples))</div><div class=\"line\">                self.means[k] /= effective_num[k]</div><div class=\"line\">                self.covars[k] = sum(self.gamma[i, k] *</div><div class=\"line\">                                     np.outer(self.data_set[i] - self.means[k],</div><div class=\"line\">                                              self.data_set[i] - self.means[k])</div><div class=\"line\">                                     <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(self.n_samples))</div><div class=\"line\">                self.covars[k] /= effective_num[k]</div><div class=\"line\">                self.component_prob[k] = effective_num[k] / self.n_samples</div><div class=\"line\"></div><div class=\"line\">            print(<span class=\"string\">\"iteration %s, current value of the log likelihood: %.4f\"</span> % (iter_cnt, crt_L))</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span> abs(crt_L - pre_L) &lt; self.tol:</div><div class=\"line\">                <span class=\"keyword\">break</span></div><div class=\"line\">            pre_L = crt_L</div><div class=\"line\"></div><div class=\"line\">        self.pred_label = np.argmax(self.gamma, axis=<span class=\"number\">1</span>)</div><div class=\"line\">        print(<span class=\"string\">\"total iteration num: %s, final value of the log likelihood: %.4f, \"</span></div><div class=\"line\">              <span class=\"string\">\"time used: %.4f seconds\"</span> % (iter_cnt, crt_L, time.time() - start_time))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># 可视化算法的聚类结果</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_clustering</span><span class=\"params\">(self, kind, y=None, title=None)</span>:</span></div><div class=\"line\">        <span class=\"keyword\">if</span> kind == <span class=\"number\">1</span>:</div><div class=\"line\">            y = self.pred_label</div><div class=\"line\">        plt.scatter(self.data_set[:, <span class=\"number\">0</span>], self.data_set[:, <span class=\"number\">1</span>],</div><div class=\"line\">                    c=y, alpha=<span class=\"number\">0.8</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> kind == <span class=\"number\">1</span>:</div><div class=\"line\">            plt.scatter(self.means[:, <span class=\"number\">0</span>], self.means[:, <span class=\"number\">1</span>],</div><div class=\"line\">                        c=<span class=\"string\">'r'</span>, marker=<span class=\"string\">'x'</span>)</div><div class=\"line\">        <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</div><div class=\"line\">            plt.title(title, size=<span class=\"number\">14</span>)</div><div class=\"line\">        plt.axis(<span class=\"string\">'on'</span>)</div><div class=\"line\">        plt.tight_layout()</div></pre></td></tr></table></figure></p>\n<p>创建一个 <code>GMMClust</code> 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 <code>predict</code> 即可对给定的数据集进行 GMM 聚类；方法 <code>plot_clustering</code> 则可以可视化聚类的结果。利用 <code>GMMClust</code> 类进行 GMM 聚类的代码如下所示：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 生成数据集</span></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">centers = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">8</span>, <span class=\"number\">3.5</span>]]</div><div class=\"line\">cluster_std = [<span class=\"number\">2</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>]</div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 运行 GMM 聚类算法</span></div><div class=\"line\">gmm_cluster = GMMClust(n_components=<span class=\"number\">3</span>)</div><div class=\"line\">gmm_cluster.predict(X)</div><div class=\"line\">   <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>):</div><div class=\"line\">       print(<span class=\"string\">\"cluster %s\"</span> % i)</div><div class=\"line\">       print(<span class=\"string\">\"    mean: %s, covariance: %s\"</span> %(gmm_cluster.means[i], gmm_cluster.covars[i]))</div><div class=\"line\"></div><div class=\"line\">   <span class=\"comment\"># 可视化数据集的原始类别情况以及算法的聚类结果</span></div><div class=\"line\">   plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</div><div class=\"line\">   gmm_cluster.plot_clustering(kind=<span class=\"number\">0</span>, y=y, title=<span class=\"string\">'The Original Dataset'</span>)</div><div class=\"line\">   plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div><div class=\"line\">   gmm_cluster.plot_clustering(kind=<span class=\"number\">1</span>, title=<span class=\"string\">'GMM Clustering Result'</span>)</div><div class=\"line\">   plt.show()</div></pre></td></tr></table></figure></p>\n<p>以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">iteration 1, current value of the log likelihood: -15761.9757</div><div class=\"line\">iteration 2, current value of the log likelihood: -6435.3937</div><div class=\"line\">iteration 3, current value of the log likelihood: -6410.5633</div><div class=\"line\">iteration 4, current value of the log likelihood: -6399.4306</div><div class=\"line\">iteration 5, current value of the log likelihood: -6389.0317</div><div class=\"line\">iteration 6, current value of the log likelihood: -6377.9131</div><div class=\"line\">iteration 7, current value of the log likelihood: -6367.5704</div><div class=\"line\">iteration 8, current value of the log likelihood: -6359.2076</div><div class=\"line\">iteration 9, current value of the log likelihood: -6350.8678</div><div class=\"line\">iteration 10, current value of the log likelihood: -6338.6458</div><div class=\"line\">... ...</div><div class=\"line\">iteration 35, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 36, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 37, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 38, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 39, current value of the log likelihood: -5859.0324</div><div class=\"line\">iteration 40, current value of the log likelihood: -5859.0324</div><div class=\"line\">total iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 seconds</div><div class=\"line\">cluster 0</div><div class=\"line\">    mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269]</div><div class=\"line\"> [0.08460269 3.95599185]]</div><div class=\"line\">cluster 1</div><div class=\"line\">    mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461  -0.00345936]</div><div class=\"line\"> [-0.00345936  0.9630804 ]]</div><div class=\"line\">cluster 2</div><div class=\"line\">    mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954]</div><div class=\"line\"> [0.01120954 0.27281129]]</div></pre></td></tr></table></figure></p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_result.png\" width=\"1000\" height=\"500\" alt=\"GMM 聚类结果\" align=\"center\"><br></div>\n\n<p>可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 <code>cluster</code> 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 <code>cluster</code> 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。</p>\n<h2 id=\"利用-sklearn-实现-GMM-聚类\"><a href=\"#利用-sklearn-实现-GMM-聚类\" class=\"headerlink\" title=\"利用 sklearn 实现 GMM 聚类\"></a>利用 sklearn 实现 GMM 聚类</h2><p>sklearn 中的  <code>GaussianMixture</code> 类可以用来进行 GMM 聚类，其中的 <code>fit</code> 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；<code>predict</code> 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.mixture <span class=\"keyword\">import</span> GaussianMixture</div><div class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</div><div class=\"line\"></div><div class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>, <span class=\"number\">12</span>))</div><div class=\"line\"></div><div class=\"line\">n_samples = <span class=\"number\">1500</span></div><div class=\"line\">random_state = <span class=\"number\">170</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个理想的数据集</span></div><div class=\"line\">X, y = make_blobs(n_samples=n_samples, random_state=random_state)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X)</div><div class=\"line\">y_pred = gmm.predict(X)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">221</span>)</div><div class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Normal Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个非球形分布的数据集</span></div><div class=\"line\">transformation = [[<span class=\"number\">0.60834549</span>, <span class=\"number\">-0.63667341</span>], [<span class=\"number\">-0.40887718</span>, <span class=\"number\">0.85253229</span>]]</div><div class=\"line\">X_aniso = np.dot(X, transformation)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_aniso)</div><div class=\"line\">y_pred = gmm.predict(X_aniso)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">222</span>)</div><div class=\"line\">plt.scatter(X_aniso[:, <span class=\"number\">0</span>], X_aniso[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Anisotropicly Distributed Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的密度不一致的数据集</span></div><div class=\"line\">X_varied, y_varied = make_blobs(n_samples=n_samples,</div><div class=\"line\">                                cluster_std=[<span class=\"number\">1.0</span>, <span class=\"number\">2.5</span>, <span class=\"number\">0.5</span>],</div><div class=\"line\">                                random_state=random_state)</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_varied)</div><div class=\"line\">y_pred = gmm.predict(X_varied)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">223</span>)</div><div class=\"line\">plt.scatter(X_varied[:, <span class=\"number\">0</span>], X_varied[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unequal Density Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 产生一个各 cluster 的样本数目不一致的数据集</span></div><div class=\"line\">X_filtered = np.vstack((X[y == <span class=\"number\">0</span>][:<span class=\"number\">500</span>], X[y == <span class=\"number\">1</span>][:<span class=\"number\">100</span>], X[y == <span class=\"number\">2</span>][:<span class=\"number\">50</span>]))</div><div class=\"line\">gmm = GaussianMixture(n_components=<span class=\"number\">3</span>, random_state=random_state)</div><div class=\"line\">gmm.fit(X_filtered)</div><div class=\"line\">y_pred = gmm.predict(X_filtered)</div><div class=\"line\"></div><div class=\"line\">plt.subplot(<span class=\"number\">224</span>)</div><div class=\"line\">plt.scatter(X_filtered[:, <span class=\"number\">0</span>], X_filtered[:, <span class=\"number\">1</span>], c=y_pred)</div><div class=\"line\">plt.title(<span class=\"string\">\"Unevenly Sized Blobs\"</span>)</div><div class=\"line\"></div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure></p>\n<p>运行结果如下图所示：</p>\n<div align=\"center\"><br><img src=\"https://raw.githubusercontent.com/ToWeather/MarkdownPhotos/master/gmm_clustering_different_datasets.png\" width=\"780\" height=\"650\" alt=\"k-means 算法在不同数据集下的表现\" align=\"center\"><br></div>\n\n<p>可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 <code>cluster</code>，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 <code>cluster</code> 都是超椭球形的。</p>\n"}],"PostAsset":[{"_id":"source/_posts/聚类分析（一）：层次聚类算法/a_dendrogram.PNG","slug":"a_dendrogram.PNG","post":"cjfclp3ty0007hws659wnpb53","modified":0,"renderable":0},{"_id":"source/_posts/聚类分析（一）：层次聚类算法/illustration_for_hierarchical_clustering.PNG","slug":"illustration_for_hierarchical_clustering.PNG","post":"cjfclp3ty0007hws659wnpb53","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjfclp3tm0001hws6wmpwdcn2","category_id":"cjfclp3tt0003hws6adgqydny","_id":"cjfclp3u1000bhws6d0k3czdm"},{"post_id":"cjfclp3u0000ahws6kbnacbch","category_id":"cjfclp3tz0008hws6natj7trl","_id":"cjfclp3u5000fhws6vrz3bdut"},{"post_id":"cjfclp3tr0002hws6qq92bhcw","category_id":"cjfclp3tz0008hws6natj7trl","_id":"cjfclp3u6000jhws6nfkojvue"},{"post_id":"cjfclp3tv0005hws6lwu23jhe","category_id":"cjfclp3tz0008hws6natj7trl","_id":"cjfclp3u7000lhws6jdrvc24o"},{"post_id":"cjfclp3tw0006hws6orb33bmd","category_id":"cjfclp3tz0008hws6natj7trl","_id":"cjfclp3u8000nhws6kti7itf0"},{"post_id":"cjfclp3ty0007hws659wnpb53","category_id":"cjfclp3tz0008hws6natj7trl","_id":"cjfclp3ua000qhws6nn94zods"}],"PostTag":[{"post_id":"cjfclp3tm0001hws6wmpwdcn2","tag_id":"cjfclp3tu0004hws6hne5l5np","_id":"cjfclp3u4000ehws6ellqx9wb"},{"post_id":"cjfclp3tm0001hws6wmpwdcn2","tag_id":"cjfclp3tz0009hws6jqm1kskz","_id":"cjfclp3u5000ghws6c2yqwl14"},{"post_id":"cjfclp3tr0002hws6qq92bhcw","tag_id":"cjfclp3u2000dhws6ze4875tn","_id":"cjfclp3ua000phws6u66pljqc"},{"post_id":"cjfclp3tr0002hws6qq92bhcw","tag_id":"cjfclp3u6000ihws62p0tcujd","_id":"cjfclp3ua000rhws6qd5qf8wd"},{"post_id":"cjfclp3tr0002hws6qq92bhcw","tag_id":"cjfclp3u7000mhws69b92oux4","_id":"cjfclp3ua000thws65bu68j5a"},{"post_id":"cjfclp3tv0005hws6lwu23jhe","tag_id":"cjfclp3u8000ohws6vlqm6lri","_id":"cjfclp3uc000whws6zsyd61hw"},{"post_id":"cjfclp3tv0005hws6lwu23jhe","tag_id":"cjfclp3ua000shws65lcnnvmt","_id":"cjfclp3uc000xhws602zzig2s"},{"post_id":"cjfclp3tv0005hws6lwu23jhe","tag_id":"cjfclp3ub000uhws64rzkr6xp","_id":"cjfclp3ue000zhws65uldf5d0"},{"post_id":"cjfclp3tw0006hws6orb33bmd","tag_id":"cjfclp3u8000ohws6vlqm6lri","_id":"cjfclp3uf0012hws6w9wmuzji"},{"post_id":"cjfclp3tw0006hws6orb33bmd","tag_id":"cjfclp3ua000shws65lcnnvmt","_id":"cjfclp3ug0013hws6s2sb2v92"},{"post_id":"cjfclp3tw0006hws6orb33bmd","tag_id":"cjfclp3ue0010hws6iswe3ynp","_id":"cjfclp3ug0015hws6fq700fom"},{"post_id":"cjfclp3ty0007hws659wnpb53","tag_id":"cjfclp3u8000ohws6vlqm6lri","_id":"cjfclp3ui0018hws6q48xnv8a"},{"post_id":"cjfclp3ty0007hws659wnpb53","tag_id":"cjfclp3ua000shws65lcnnvmt","_id":"cjfclp3ui0019hws6za9kacgz"},{"post_id":"cjfclp3ty0007hws659wnpb53","tag_id":"cjfclp3uh0016hws6qwb0x4ej","_id":"cjfclp3uj001bhws61pffe6nk"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3u8000ohws6vlqm6lri","_id":"cjfclp3um001ghws67wcpjlbk"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3ua000shws65lcnnvmt","_id":"cjfclp3um001hhws651jgzs5n"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3uj001chws6d5iwnlp2","_id":"cjfclp3um001ihws6j6qawhv9"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3uk001dhws6junjmy35","_id":"cjfclp3um001jhws6xtbpvv1v"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3uk001ehws6un2mlfiy","_id":"cjfclp3um001khws6g8w4podb"},{"post_id":"cjfclp3u0000ahws6kbnacbch","tag_id":"cjfclp3ul001fhws6hzum1el7","_id":"cjfclp3um001lhws69vo7ugdw"}],"Tag":[{"name":"python","_id":"cjfclp3tu0004hws6hne5l5np"},{"name":"unicode","_id":"cjfclp3tz0009hws6jqm1kskz"},{"name":"线性模型","_id":"cjfclp3u2000dhws6ze4875tn"},{"name":"线性回归","_id":"cjfclp3u6000ihws62p0tcujd"},{"name":"监督学习","_id":"cjfclp3u7000mhws69b92oux4"},{"name":"聚类","_id":"cjfclp3u8000ohws6vlqm6lri"},{"name":"非监督学习","_id":"cjfclp3ua000shws65lcnnvmt"},{"name":"k-means 算法","_id":"cjfclp3ub000uhws64rzkr6xp"},{"name":"密度聚类","_id":"cjfclp3ue0010hws6iswe3ynp"},{"name":"层次聚类","_id":"cjfclp3uh0016hws6qwb0x4ej"},{"name":"高斯混合模型","_id":"cjfclp3uj001chws6d5iwnlp2"},{"name":"GMM","_id":"cjfclp3uk001dhws6junjmy35"},{"name":"生成模型","_id":"cjfclp3uk001ehws6un2mlfiy"},{"name":"EM 算法","_id":"cjfclp3ul001fhws6hzum1el7"}]}}