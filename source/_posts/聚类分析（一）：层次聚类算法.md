---
title: 聚类分析（一）：层次聚类算法
layout: post
date: 2017-12-01 20:59:23
tags:
- 聚类
- 非监督学习
- 层次聚类
categories:
- 机器学习算法
keywords: 聚类,层次聚类,非监督学习,clustering,machine learning
---

# 聚类算法综述 #
聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 `cluster`，使得同一 `cluster` 内的对象在某种意义上比不同的 `cluster` 之间的对象更为相似。
由于 "`cluster`" 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类：
- 基于**连通模型**（connectivity-based）的聚类算法： 即本文将要讲述的**层次聚类**算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 `cluster`。
- 基于**中心点模型**（centroid-based）的聚类算法： 在此类算法中，每个 `cluster` 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 `k` 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 `cluster` 的中心点的距离的平方和，优化变量为每个 `cluster` 的中心点以及每个对象属于哪个 `cluster`；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解，k-means 算法即是其中的一种。
- 基于**分布模型**（distribution-based）的聚类算法： 此类算法认为数据集的中数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 `cluster` 即可。
- 基于**密度**（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 `cluster`，`cluster` 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。

------------
# 层次聚类综述 #
层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 `clusters`，后面一层生成的 `clusters` 基于前面一层的结果。层次聚类算法一般分为两类：
- Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 `cluster`，每次按一定的准则将最相近的两个 `cluster` 合并生成一个新的 `cluster`，如此往复，直至最终所有的对象都属于一个 `cluster`。本文主要关注此类算法。
- Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 `cluster`，每次按一定的准则将某个 `cluster` 划分为多个 `cluster`，如此往复，直至每个对象均是一个 `cluster`。

下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。
<div align = center>
<img src="https://i.imgur.com/hiVWi9R.png" width = "500" height = "400" alt = "层次聚类图示" align = center />
</div>

另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。

## 树形图 ##
树形图（dendrogram）可以直观地表示层次聚类的成果。一个有 `5` 个点的树形图如下图所示，其中纵坐标高度表示不同的 `cluster` 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\\( x\_1 \\) 和 \\( x\_2 \\) 的距离最近（为 `1`），因此将 \\( x\_1 \\) 和 \\( x\_2 \\) 合并为一个 cluster \\( (x\_1, x\_2) \\)，所以在树形图中首先将节点 \\( x\_1 \\) 和 \\( x\_2 \\) 连接，使其成为一个新的节点  \\( (x\_1, x\_2) \\) 的子节点，并将这个新的节点的高度置为 `1`；之后再在剩下的 `4` 个 `cluster` \\( (x\_1, x\_2) \\)， \\( x\_3 \\)， \\( x\_4 \\) 和 \\( x\_5 \\) 中选取距离最近的两个 `cluster` 合并，\\( x\_4 \\) 和 \\( x\_5 \\) 的距离最近（为 `2`），因此将 \\( x\_4 \\) 和 \\( x\_5 \\) 合并为一个 cluster \\( (x\_4, x\_5) \\)，体现在树形图上，是将节点 \\( x\_4 \\) 和 \\( x\_5 \\) 连接，使其成为一个新的节点 \\( (x\_4, x\_5) \\) 的子节点，并将此新节点的高度置为 `2`；....依次模式进行树形图的生成，直至最终只剩下一个 `cluster` \\( ((x\_1, x\_2), x\_3), (x\_4, x\_5)) \\)。
可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 `cluster` 之间的距离都不大于 \\( h \\)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \\( h \\)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \\( h=2.5 \\)，即可得到 `3` 个 `cluster` \\( (x\_1, x\_2) \\)， \\( x\_3 \\) 和 \\( (x\_4, x\_5) \\)。
<div align = center>
<img src="https://i.imgur.com/Ag5A0Io.png" width = "500" height = "400" alt = "树形图示例" align = center />
</div>

## 对象之间的距离衡量 ##

## cluster 之间的距离衡量 ##

------------
# Agglomerative 层次聚类算法 #

## Lance-William 方法 ##


## Naive 算法 ##




