<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线性回归模型]]></title>
    <url>%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>线性模型</tag>
        <tag>线性回归</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（四）：DBSCAN 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ADBSCAN-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第四篇文章，其它相关文章请参见 聚类分析系列文章。 DBSCAN 算法DBSCAN（Density-Based Spatial Clustering of Applications with Noise) 算法是一种被广泛使用的密度聚类算法。密度聚类算法认为各个 cluster 是样本点密度高的区域，而 cluster 之间是由样本点密度低的区域所隔开的；这种聚类的方式实际上是符合人们的直觉的，这也是以 DBSCAN 算法为代表的密度聚类算法在实际应用中通常表现的比较好的原因之一。 要想描述清楚 DBSCAN 算法，我们得先定义好一些概念，例如何为密度高的区域、何为密度低的区域等等。 几个定义在密度聚类算法中，我们需要区分密度高的区域以及密度低的区域，并以此作为聚类的依据。DBSCAN 通过将样本点划分为三种类型来达到此目的：核心点（core points）、边缘点（border points）以及噪声点（noise），核心点和边缘点共同构成一个一个的 cluster，而噪声点则存在于各 cluster 之间的区域。为区分清楚这三类样本点，我们首先定义一些基本的概念。 定义1： （样本点的 \( \text {Eps} \)-邻域）假设数据集为 \( \bf X \)，则样本点 \( \bf p \) 的 \( \text {Eps} \)-邻域定义为 \( N_{\text {Eps}} ({\bf p}) = \lbrace {\bf q} \in {\bf X} | d({\bf p}, {\bf q}) \le {\text {Eps}} \rbrace \). 我们再给定一个参数 \( \text {MinPts} \)，并定义核心点须满足的条件为：其 \( \text {Eps} \)-邻域内包含的样本点的数目不小于 \( \text {MinPts} \) ， 因而核心点必然在密度高的区域，我们可能很自然地想到一种简单的聚类方法：聚类产生的每一个 cluster 中的所有点都是核心点，而所有非核心点都被归为噪声。但仔细地想一想，这种方式其实是不太有道理的，因为一个符合密度高的 cluster 的边界处的点并不一定需要是核心点，它可以是非核心点（我们称这类非核心点为边缘点，称其它的非核心点为噪声点），如下图中的图（a）所示。我们通过以下定义来继续刻画 DBSCAN 里面的 cluster 所具有的形式。 定义2： （密度直达）我们称样本点 \( \bf p \) 是由样本点 \( \bf q \) 对于参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \) 密度直达的，如果它们满足 \( {\bf p} \in N_{\text {Eps}} ({\bf q}) \) 且 \( |N_{\text {Eps}}({\bf q})| \ge \text {MinPts} \) （即样本点 \( \bf q \) 是核心点）. 很显然，密度直达并不是一个对称的性质，如下图中的图（b）所示，其中 \( \text {Eps} = 5 \)，可以看到，样本点 \( \bf q \) 为核心点，样本点 \( \bf p \) 不是核心点，且 \( \bf p \) 在 \( \bf q \) 的 \( \text {Eps} \)-邻域内，因而 \( \bf p \) 可由 \( \bf q \) 密度直达，而反过来则不成立。有了密度直达的定义后，我们再来给出密度可达的定义。 定义3：（密度可达）我们称样本点 \( \bf p \) 是由样本点 \( \bf q \) 对于参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \) 密度可达的，如果存在一系列的样本点 \( {\bf p}_{1}, …, {\bf p}_n \)（其中 \( {\bf p}_1 = {\bf q}, {\bf p}_n = {\bf p} \)）使得对于 \( i = 1, …, n-1 \)，样本点 \( {\bf p}_{i + 1} \) 可由样本点 \( {\bf p}_{i} \) 密度可达. 我们可以看到，密度直达是密度可达的特例，同密度直达一样，密度可达同样不是一个对称的性质，如上图中的图（c）所示。为描述清楚我们希望得到的 cluster 中的任意两个样本点所需满足的条件，我们最后再给出一个密度相连的概念。 定义4：（密度相连）我们称样本点 \( \bf p \) 与样本点 \( \bf q \) 对于参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \) 是密度相连的，如果存在一个样本点 \( {\bf o} \)，使得 \( \bf p \) 和 \( \bf q \) 均由样本点 \( \bf o \) 密度可达。 密度相连是一个对称的性质，如上图中的图（d）所示。DBSCAN 算法期望找出一些 cluster ，使得每一个 cluster 中的任意两个样本点都是密度相连的，且每一个 cluster 在密度可达的意义上都是最大的。cluster 的定义如下： 定义5：（cluster）假设数据集为 \( \bf X \)，给定参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \)，则某个 cluster \( C \) 是数据集 \( \bf X \) 的一个非空子集，且满足如下条件：1）对于任意的样本点 \( \bf p \) 和 \( \bf q \)，如果 \( {\bf p} \in C \) 且 \( \bf q \) 可由 \( \bf p \) 密度可达，则 \( {\bf q} \in C \) .（最大性）2）对于 \( C \) 中的任意样本点 \( \bf p \) 和 \( \bf q \)， \( \bf p \) 和 \( \bf q \) 关于参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \) 是密度相连的.（连接性） 这样我们就定义出了 DBSCAN 算法最终产生的 cluster 的形式，它们就是所谓的密度高的区域；那么，噪声点就是不属于任何 cluster 的样本点。根据以上定义，由于一个 cluster 中的任意两个样本点都是密度相连的，每一个 cluster 至少包含 \( \text {MinPts} \) 个样本点。 算法描述DBSCAN 算法就是为了寻找以上定义 5 中定义的 cluster，其主要思路是先指定一个核心点作为种子，寻找所有由该点密度可达的样本点组成一个 cluster，再从未被聚类的核心点中指定一个作为种子，寻找所有由该点密度可达的样本点组成第二个 cluster，…，依此过程，直至没有未被聚类的核心点为止。依照 cluster 的“最大性”和“连接性”，在给定参数 \( \lbrace \text {Eps}, \text {MinPts} \rbrace \) 的情况下，最终产生的 cluster 的结果是一致的，与种子的选取顺序无关（某些样本点位于多个 cluster 的边缘的情况除外，这种情况下，这些临界位置的样本点的 cluster 归属与种子的选取顺序有关）。 合理地选取参数DBSCAN 的聚类结果和效果取决于参数 \( \text {Eps} \) 和 \( \text {MinPts} \) 以及距离衡量方法的选取。 由于算法中涉及到寻找某个样本点的指定邻域内的样本点，因而需要衡量两个样本点之间的距离。这里选取合适的距离衡量函数也变得比较关键，一般而言，我们需要根据所要处理的数据的特点来选取距离函数，例如，当处理的数据是空间地理位置信息时，Euclidean 距离是一个比较合适的选择。各种不同的距离函数可参见 聚类分析（一）：层次聚类算法。 然后我们再来看参数的选取方法，我们一般选取数据集中最稀疏的 cluster 所对应的 \( \text {Eps} \) 和 \( \text {MinPts} \)。这里我们给出一种启发式的参数选取方法。假设 \( d \) 是某个样本点 \( \bf p \) 距离它的第 \( k \) 近邻的距离，则一般情况下 \( \bf p \) 的 \( d \)-邻域内正好包含 \( k + 1 \) 个样本点。我们可以推断，在一个合理的 cluster 内，改变 \( k \) 的值不应该导致 \( d \) 值有较大的变化，除非 \( \bf p \) 的第 \( k \) 近邻们（\( k = 1, 2, 3,… \) ） 都近似在一条直线上，而这种情形的数据不可能构成一个合理的 cluster。 因而我们一般将 \( k \) 的值固定下来，一个合理的选择是令 \( k = 3 \) 或 \( k = 4 \)，那么 \( \text {MinPts} \) 的值也确定了（为 \( k + 1 \)）；然后再来看每个样本点的 \( \text {k-dist} \) 距离（即该样本点距离它的第 \( k \) 近邻的距离）的分布情况，我们把每个样本点的 \( \text {k-dist} \) 距离从大到小排列，并将它绘制出来，如下图所示。我们再来根据这个图像来选择 \( \text {Eps} \)，一种情况是我们对数据集有一定的了解，知道噪声点占总样本数的大致比例，则直接从图像中选取该比例处所对应的样本点的 \( \text {k-dist} \) 距离作为 \( \text {Eps} \)，如下图中的图（a）所示；另一种情况是，我们对数据集不太了解，此时就只能分析 \( \text {k-dist} \) 图了，一般情况下，我们选取第一个斜率变化明显的“折点”所对应的 \( \text {k-dist} \) 距离作为 \( \text {k-dist} \) ，如下图中的图（b）所示。 算法的复杂度及其优缺点算法复杂度DBSCAN 算法的主要计算开销在于对数据集中的每一个样本点都判断一次其是否为核心点，而进行这一步骤的关键是求该样本点的 \( \text {Eps} \)-邻域内的样本点，如果采用穷举的遍历的方法的话，该操作的时间复杂度为 \( O(N) \)，其中 \( N \) 为总样本数；但我们一般会对数据集建立索引，以此来加快查询某邻域内数据的速度，例如，当采用 R* tree 建立索引时，查询邻域的平均时间复杂度为 \( O(\log N) \)。因而，DBSCAN 算法的平均时间复杂度为 \( O(N\log N) \)；由于只需要存储数据集以及索引信息，DBSCAN算法的空间复杂度与总样本数在一个量级，即 \( O(N) \)。 优缺点DBSCAN 算法有很多优点，总结如下： DBSCAN 不需要事先指定最终需要生成的 cluster 的数目，这一点解决了其它聚类算法（如 k-means、GMM 聚类等）在实际应用中的一个悖论：我们由于对数据集的结构不了解才要进行聚类，如果我们事先知道了数据集中的 cluster 的数目，实际上我们对数据集是有相当多的了解的。 DBSCAN 可以找到具有任意形状的 cluster，如非凸的 cluster，这基于其对 cluster 的定义（cluster 是由密度低的区域所隔开的密度高的区域）。 DBSCAN 对异常值鲁棒，因为它定义了噪声点的概念。 DBSCAN 算法在给定参数下对同一数据集的聚类结果是确定的（除非出现某些样本点位于多个 cluster 的边缘的情况，这种情况下，这些临界位置的样本点的 cluster 归属与种子的选取顺序有关，而它们对于聚类结果的影响也是微乎其微的）。 DBSCAN 的运行速度快，当采用索引时，其复杂度仅为 \( O(N\log N) \)。 当然，它也有一个主要缺点，即对于具有密度相差较大的 cluster 的数据集的聚类效果不好，这是因为在这种情况下，如果参数 \( \text {MinPts} \) 和 \( \text {Eps} \) 是参照数据集中最稀疏的 cluster 所选取的，那么很有可能最终所有的样本最终都被归为一个 cluster，因为可能数据集中的 cluster 之间的区域的密度和最稀疏的 cluster 的密度相当；如果选取的参数 \( \text {MinPts} \) 和 \( \text {Eps} \) 倾向于聚出密度比较大的 cluster，那么极有可能，比较稀疏的这些 cluster 都被归为噪声。 OPTICS 算法一般被用来解决这一问题。 实现 DBSCAN 聚类现在我们来实现 DBSCAN 算法。首先我们定义一个用于进行 DBSCAN 聚类的类 DBSCAN，进行聚类时，我们得先构造一个该类的实例，初始化时，我们须指定 DBSCAN 算法的参数 min_pts 和 eps 以及距离衡量方法（默认为 euclidean），对数据集进行聚类时，我们对构造出来的实例调用方法 predict。predict 方法首先为“查询某样本点的邻域”这一经常被用到的操作做了一些准备工作，一般是计算各样本点间的距离并保存，但在当数据维度为 2、距离度量方法为 Euclidean 距离时，我们使用 rtree 模块为数据集建立了空间索引，以加速查询邻域的速度（一般来讲，为提高效率，应该对任意数据维度、任意距离度量方法下的数据集建立索引，但这里为实现方便，仅对此一种类型的数据集建立了索引，其它类型的数据集我们还是通过遍历距离矩阵的形式进行低效的邻域查找）。然后再对数据进行 DBSCAN 聚类，即遍历数据集中的样本点，若该样本点未被聚类且为核心点时，以该样本点为种子按照密度可达的方式扩展至最大为止。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import numpy as npimport timeimport matplotlib.pyplot as pltfrom shapely.geometry import Pointimport rtreeUNCLASSIFIED = -2NOISE = -1class DBSCAN(): def __init__(self, min_pts, eps, metric='euclidean', index_flag=True): self.min_pts = min_pts self.eps = eps self.metric = metric self.index_flag = index_flag self.data_set = None self.pred_label = None self.core_points = set() def predict(self, data_set): self.data_set = data_set self.n_samples, self.n_features = self.data_set.shape self.data_index = None self.dist_matrix = None start_time = time.time() if self.n_features == 2 and self.metric == 'euclidean' \ and self.index_flag: # 此种情形下对数据集建立空间索引 self.construct_index() else: # 其它情形下对数据集计算距离矩阵 self.cal_dist_matrix() self.pred_label = np.array([UNCLASSIFIED] * self.n_samples) # 开始 DBSCAN 聚类 crt_cluster_label = -1 for i in range(self.n_samples): if self.pred_label[i] == UNCLASSIFIED: query_result = self.query_eps_region_data(i) if len(query_result) &lt; self.min_pts: self.pred_label[i] = NOISE else: crt_cluster_label += 1 self.core_points.add(i) for j in query_result: self.pred_label[j] = crt_cluster_label query_result.discard(i) self.generate_cluster_by_seed(query_result, crt_cluster_label) print("time used: %.4f seconds" % (time.time() - start_time)) def construct_index(self): self.data_index = rtree.index.Index() for i in range(self.n_samples): data = self.data_set[i] self.data_index.insert(i, (data[0], data[1], data[0], data[1])) @staticmethod def distance(data1, data2, metric='euclidean'): if metric == 'euclidean': dist = np.sqrt(np.dot(data1 - data2, data1 - data2)) elif metric == 'manhattan': dist = np.sum(np.abs(data1 - data2)) elif metric == 'chebyshev': dist = np.max(np.abs(data1 - data2)) else: raise Exception("invalid or unsupported distance metric!") return dist def cal_dist_matrix(self): self.dist_matrix = np.zeros((self.n_samples, self.n_samples)) for i in range(self.n_samples): for j in range(i + 1, self.n_samples): dist = self.distance(self.data_set[i], self.data_set[j], self.metric) self.dist_matrix[i, j], self.dist_matrix[j, i] = dist, dist def query_eps_region_data(self, i): if self.data_index: data = self.data_set[i] query_result = set() buff_polygon = Point(data[0], data[1]).buffer(self.eps) xmin, ymin, xmax, ymax = buff_polygon.bounds for idx in self.data_index.intersection((xmin, ymin, xmax, ymax)): if Point(self.data_set[idx][0], self.data_set[idx][1]).intersects(buff_polygon): query_result.add(idx) else: query_result = set(item[0] for item in np.argwhere(self.dist_matrix[i] &lt;= self.eps)) return query_result def generate_cluster_by_seed(self, seed_set, cluster_label): while seed_set: crt_data_index = seed_set.pop() crt_query_result = self.query_eps_region_data(crt_data_index) if len(crt_query_result) &gt;= self.min_pts: self.core_points.add(crt_data_index) for i in crt_query_result: if self.pred_label[i] == UNCLASSIFIED: seed_set.add(i) self.pred_label[i] = cluster_label 我们下面构造一个数据集，并对该数据集运行我们手写的算法进行 DBSCAN 聚类，并将 DBSCAN 中定义的核心点、边缘点以及噪声点可视化；同时，我们还想验证一下我们手写的算法的正确性，所以我们利用 sklearn 中实现的 DBSCAN 类对同一份数据集进行了 DBSCAN 聚类，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.datasets import make_blobsfrom sklearn.cluster import DBSCAN as DBSCAN_SKLEARNdef plot_clustering(X, y, core_pts_idx=None, title=None): if core_pts_idx is not None: core_pts_idx = np.array(list(core_pts_idx), dtype=int) core_sample_mask = np.zeros_like(y, dtype=bool) core_sample_mask[core_pts_idx] = True unique_labels = set(y) colors = [plt.cm.Spectral(item) for item in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: col = [0, 0, 0, 1] class_member_mask = (y == k) xy = X[class_member_mask &amp; core_sample_mask] plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=12, alpha=0.6) xy = X[class_member_mask &amp; ~core_sample_mask] plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6, alpha=0.6) else: plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6) if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout()# 构造数据集n_samples = 1500random_state = 170X, y = make_blobs(n_samples=n_samples, random_state=random_state)# 利用我自己手写的 DBSCAN 算法对数据集进行聚类dbscan_diy = DBSCAN(min_pts=20, eps=0.5)dbscan_diy.predict(X)n_clusters = len(set(dbscan_diy.pred_label)) - (1 if -1 in dbscan_diy.pred_label else 0)print("count of clusters generated: %s" % n_clusters)print("propotion of noise data for dbscan_diy: %.4f" % (np.sum(dbscan_diy.pred_label == -1) / n_samples))plt.subplot(1, 2, 1)plot_clustering(X, dbscan_diy.pred_label, dbscan_diy.core_points, title="DBSCAN(DIY) Results")# 利用 sklearn 实现的 DBSCAN 算法对数据集进行聚类dbscan_sklearn = DBSCAN_SKLEARN(min_samples=20, eps=0.5)dbscan_sklearn.fit(X)print("propotion of noise data for dbscan_sklearn: %.4f" % (np.sum(dbscan_sklearn.labels_ == -1) / n_samples))plt.subplot(1, 2, 2)plot_clustering(X, dbscan_sklearn.labels_, dbscan_sklearn.core_sample_indices_, title="DBSCAN(SKLEARN) Results")plt.show() 运行得到的输出和可视化结果如下所示：1234time used: 4.2602 secondscount of clusters generated: 3propotion of noise data for dbscan_diy: 0.1220propotion of noise data for dbscan_sklearn: 0.1220 上图中，大圆圈表示核心点、非黑色的小圆圈表示边缘点、黑色的小圆圈表示噪声点，它们的分布服从我们的直观感受。我们还可以看到，手写算法和 sklearn 实现的算法的产出时一模一样的（从可视化结果的对比以及噪声点所占的比例可以得出），这也验证了手写算法的正确性。 我们再来看一下 DBSCAN 算法对于非凸数据集的聚类效果，代码如下：123456789101112131415161718from sklearn.datasets import make_circles, make_moonsfrom sklearn.preprocessing import StandardScalern_samples = 1500noisy_circles, _ = make_circles(n_samples=n_samples, factor=.5, noise=.05)noisy_circles = StandardScaler().fit_transform(noisy_circles)noisy_moons, _ = make_moons(n_samples=n_samples, noise=.05)noisy_moons = StandardScaler().fit_transform(noisy_moons)dbscan = DBSCAN(min_pts=5, eps=0.22)dbscan.predict(noisy_circles)plt.subplot(1, 2, 1)plot_clustering(noisy_circles, dbscan.pred_label, title="Concentric Circles Dataset")dbscan.predict(noisy_moons)plt.subplot(1, 2, 2)plot_clustering(noisy_moons, dbscan.pred_label, title="Interleaved Moons DataSet")plt.show() 运行的结果如下图所示： 可以看到 DBSCAN 算法对非凸数据集的聚类效果非常好。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>密度聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（三）：高斯混合模型与 EM 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第三篇文章，其它相关文章请参见 聚类分析系列文章。 高斯混合模型高斯混合模型简介高斯混合模型（Gaussian Mixture Model，GMM）即是几个高斯分布的线性叠加，其可提供更加复杂但同时又便于分析的概率密度函数来对现实世界中的数据进行建模。具体地，高斯混合分布的概率密度函数如下所示：$$p({\bf x}) = \sum_{k = 1}^{K} \pi_{k} {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k)\tag {1}$$如果用该分布随机地产生样本数据，则每产生一个样本数据的操作如下：首先在 \( K \) 个类别中以 \( \pi_{k} \) 的概率随机选择一个类别 \( k \)，然后再依照该类别所对应的高斯分布 \( {\cal N}({\bf x}|{\bf \mu}_k, {\bf \Sigma}_k) \) 随机产生一个数据 \( {\bf x} \)。但最终生成数据集后，我们所观察到的仅仅只是 \( {\bf x} \)，而观察不到用于产生 \( {\bf x} \) 的类别信息。我们称这些观察不到的变量为隐变量（latent variables）。为描述方便，我们以随机向量 \( {\bf z} \in {\lbrace 0, 1 \rbrace}^{K} \) 来表示高斯混合模型中的类别变量，\( {\bf z} \) 中仅有一个元素的值为 \( 1 \)，而其它元素的值为 \( 0 \)，例如当 \( z_{k} = 1\) 时，表示当前数据是由高斯混合分布中的第 \( k \) 个类别所产生。 对应于上面高斯混合分布的概率密度函数，隐变量 \( {\bf z} \) 的概率质量函数为$$p(z_{k} = 1) = \pi_k\tag {2}$$其中 \( \lbrace \pi_{k} \rbrace \) 须满足$$\sum_{k = 1}^{K} \pi_k = 1 \text{ , } 0 \le \pi_k \le 1$$给定 \( {\bf z} \) 的值的情况下，\( {\bf x} \) 服从高斯分布$$p({\bf x} | z_{k} = 1) = {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k})\tag {3}$$因而可以得到 \( {\bf x} \) 的边缘概率分布为$$ p({\bf x}) = \sum_{\bf z} p({\bf z})p({\bf x} | {\bf z}) = \sum_{k =1}^{K} \pi_k {\cal N}({\bf x} | {\bf \mu}_{k}, {\bf \Sigma}_{k})\tag {4}$$该分布就是我们前面所看到的高斯混合分布。 最大似然估计问题假设有数据集 \( \lbrace {\bf x}_1, {\bf x}_2, …, {\bf x}_N \rbrace \)，其中样本的维度为 \( D \)，我们希望用高斯混合模型来对这个数据集来建模。为分析方便，我们可以以矩阵 \( {\bf X} = [ {\bf x}_1, {\bf x}_2, …, {\bf x}_N ]^{T} \in {\Bbb R}^{N \times D} \) 来表示数据集，以矩阵 \( {\bf Z} = [ {\bf z}_1, {\bf z}_2, …, {\bf z}_N ]^{T} \in {\Bbb R}^{N \times K} \) 来表示各个样本对应的隐变量。假设每个样本均是由某高斯混合分布独立同分布（i.i.d.）地产生的，则可以写出该数据集的对数似然函数为$$ L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \ln \lbrace \sum_{k = 1}^{K} \pi_k {\cal N}({\bf x}_{n} | {\bf \mu}_{k}, {\bf \Sigma}_{k}) \rbrace\tag {5}$$我们希望求解出使得以上对数似然函数最大的参数集 \( \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \)，从而我们就可以依照求解出的高斯混合模型来对数据集进行分析和阐释，例如对数据集进行聚类分析。但该似然函数的对数符号里面出现了求和项，这就使得对数运算不能直接作用于单个高斯概率密度函数（高斯分布属于指数分布族，而对数运算作用于指数分布族的概率密度函数可以抵消掉指数符号，使得问题的求解变得非常简单），从而使得直接求解该最大似然估计问题变得十分复杂。事实上，该问题也没有闭式解。 另一个值得注意的地方是，在求解高斯混合模型的最大似然估计问题时，可能会得到一个使 \( L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) \) 发散的解。具体地，观察公式 \( (5) \)，如果某一个高斯分量的期望向量正好取到了某一个样本点的值，则当该高斯分量的协方差矩阵为奇异矩阵（行列式为 \( 0 \)）的时候，会导致求和项中的该项的值为无穷大，从而也使得 \( L({\bf \pi}, {\bf \mu}, {\bf \Sigma}) \) 的值为无穷大；这样确实使得式 \( (5) \) 最大了，但求解出的高斯混合模型中的某一个高斯分量却直接坍缩到了某一个样本点，这显然是与我们的初衷不相符合的。这也体现了最大似然估计方法容易导致过拟合现象的特点，如果我们采用最大后验概率估计的方法，则可避免出现该问题；此外，如果我们采用迭代算法来求解该最大似然估计问题（如梯度下降法或之后要讲到的 EM 算法）的话，可以在迭代的过程中用一些启发式的方法加以干预来避免出现高斯分量坍缩的问题（将趋于坍缩的高斯分量的期望设置为不与任何样本点相同的值，将其协方差矩阵设置为一个具有较大的行列式值的矩阵）。 EM 算法求解高斯混合模型EM （Expectation-Maximization）算法是一类非常优秀且强大的用于求解含隐变量的模型的最大似然参数估计问题的算法。我们将在这一部分启发式地推导出用于求解高斯混合模型的最大似然参数估计问题的 EM 算法。 我们对对数似然函数 \( (5) \) 关于各参数 \( {\bf \pi} \)， \( {\bf \mu} \)， \( {\bf \Sigma} \) 分别求偏导，并将其置为 \( 0 \) 可以得到一系列的方程，而使得式 \( (5) \) 最大的解也一定满足这些方程。 首先令式 \( (5) \) 关于 \( {\bf \mu}_k \) 的偏导为 \( 0 \) 可得以下方程：$$- \sum_{n = 1}^{N} \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} {\bf \Sigma}_k ({\bf x}_n - {\bf \mu}_k) = 0\tag {6}$$注意到，上式中含有项$$\gamma (z_{nk}) = \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = p(z_{nk} = 1 | {\bf x}_n)\tag {7}$$该项具有重要的物理意义，它为给定样本点 \( {\bf x}_n \) 后隐变量 \( {\bf z}_n \) 的后验概率，可直观理解某个高斯分量对产生该样本点所负有的“责任”（resposibility）；GMM 聚类就是利用 \( \gamma (z_{nk}) \) 的值来做软分配的。 因而，我们可以由式 \( (6) \) 和式 \( (7) \) 写出$${\bf \mu}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) {\bf x}_n\tag {8}$$其中 \( N_k = \sum_{n = 1}^{N} \gamma(z_{nk}) \)，我们可以将 \( N_{k} \) 解释为被分配给类别 \( k \) 的有效样本数量，而 \( {\bf \mu}_{k} \) 即为所有样本点的加权算术平均值，每个样本点的权重等于第 \( k\) 个高斯分量对产生该样本点所负有的“责任”。 我们再将式 \( (5) \) 对 \( {\bf \Sigma}_k \) 的偏导数置为 \( 0 \) 可求得$${\bf \Sigma}_k = \frac{1} {N_k} \sum_{n = 1}^{N} \gamma(z_{nk}) ({\bf x}_n - {\bf \mu}_k)({\bf x}_n - {\bf \mu}_k)^{\text T}\tag {9}$$可以看到上式和单个高斯分布的最大似然参数估计问题求出来的协方差矩阵的解的形式是一样的，只是关于每个样本点做了加权，而加权值仍然是 \( \gamma(z_{nk}) \)。 最后我们再来推导 \( \pi_k \) 的最大似然解须满足的条件。由于 \( \pi_k \) 有归一化的约束，我们可以利用 Lagrange 乘数法 来求解（将 \( \ln p({\bf X} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) + \lambda (\sum_{k = 1}^{K} \pi_k - 1) \) 关于 \( \pi_k \) 的偏导数置 \( 0 \)），最后可求得$$\pi_k = \frac{N_k} {N}\tag {10}$$关于类别 \( k \) 的先验概率的估计值可以理解为所有样本点中被分配给第 \( k \) 个类别的有效样本点的个数占总样本数量的比例。 注意到，式 \( (8) \) 至 \( (10) \) 即是高斯混合模型的最大似然参数估计问题的解所需满足的条件，但它们并不是 \( \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \) 的闭式解，因为这些式子中给出的表达式中都含有 \( \gamma (z_{nk}) \)，而 \( \gamma (z_{nk}) \) 反过来又以一种非常复杂的方式依赖于所要求解的参数（见式 \( (7) \)）。 尽管如此，以上的这些公式仍然提供了一种用迭代的方式来解决最大似然估计问题的思路。 先将参数 \( \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \) 固定为一个初始值，再按式 \( (7) \) 计算出隐变量的后验概率 \( \gamma (z_{nk}) \) （E 步）；然后再固定 \( \gamma (z_{nk}) \)，按式 \( (8) \) 至 \( (10) \) 分别更新参数 \( \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \) 的值（M 步），依次交替迭代，直至似然函数收敛或所求解的参数收敛为止。 实际上，上述描述即是用于求解高斯混合模型的最大似然参数估计问题的 EM 算法，该算法可以求得一个局部最优解，因为其每一步迭代都会增加似然函数的值（稍后讲述一般 EM 算法的时候会证明）。 我们仍然借用 PRML 教材中的例子来阐述 EM 算法在求解上述问题时的迭代过程。下图中的数据集依旧来源于经过标准化处理的 Old Faithful 数据集，我们选用含有两个高斯分量的高斯混合模型，下图中的圆圈或椭圆圈代表单个高斯分量（圆圈代表该高斯分量的概率密度函数的偏离期望一个标准差处的等高线），以蓝色和红色来区分不同的高斯分量；图（a）为各参数的初始化的可视化呈现，在这里我们为两个高斯分量选取了相同的协方差矩阵，且该协方差矩阵正比于单位矩阵；图（b）为 EM 算法的 E 步，即更新后验概率 \( \gamma (z_{nk}) \)，在图中则体现为将每一个样本点染上有可能产生它的高斯分量的颜色（例如，某一个样本点的由蓝色的高斯分量产生的概率为 \( p_1 \) ，由红色的高斯分量产生的概率为 \( p_2 \)，则我们将其染上 \( p_1 \) 比例的蓝色，染上 \( p_2 \) 比例的红色）；图（c）为 EM 算法的 M 步，即更新 GMM 模型的各参数，在图中表现为椭圆圈的变化；图 （d）～（f）分别是后续的迭代步骤，到第 20 次迭代的时候，算法已经基本上收敛。 一般 EM 算法前面我们只是启发式地推导了用于求解 GMM 的最大似然估计问题的 EM 算法，但这仅仅只是 EM 算法的一个特例。实际上，EM 算法可用于求解各种含有隐变量的模型的最大似然参数估计问题或者最大后验概率参数估计问题。在这里，我们将以一种更正式的形式来推导这类适用范围广泛的 EM 算法。 假设观测到的变量集为 \( {\bf X} \)，隐变量集为 \( {\bf Z} \)，模型中所涉及到的参数集为 \( {\bf \theta} \)，我们的目的是最大化关于 \( {\bf X} \) 的似然函数$$p({\bf X} | {\bf \theta}) = \sum_{\bf Z} p({\bf X}, {\bf Z} | {\bf \theta})\tag {11}$$一般来讲，直接优化 \( p({\bf X} | {\bf \theta}) \) 是比较困难的，而优化完全数据集的似然函数 \( p({\bf X}, {\bf Z} | {\bf \theta}) \) 的难度则会大大减小，EM 算法就是基于这样的思路。 首先我们引入一个关于隐变量 \( {\bf Z} \) 的分布 \( q({\bf Z}) \)，然后我们可以将对数似然函数 \( \ln p({\bf X} | {\bf \theta}) \) 分解为如下$$\ln p({\bf X} | {\bf \theta}) = {\cal L}(q, {\bf \theta}) + {\text KL}(q || p)\tag {12}$$其中$${\cal L}(q, {\bf \theta}) = \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{p({\bf X}, {\bf Z} | {\bf \theta})} {q({\bf Z})} \rbrace\tag {13}$$$${\text KL}(q || p) = - \sum_{\bf Z} q({\bf Z}) \ln \lbrace \frac{ p({\bf Z} | {\bf X}, {\bf \theta})} {q({\bf Z})} \rbrace\tag {14}$$其中 \( {\cal L}(q, {\bf \theta}) \) 为关于概率分布 \( q({\bf Z}) \) 的泛函，且为关于参数集 \( {\bf \theta} \) 的函数，另外，\( {\cal L}(q, {\bf \theta}) \) 的表达式中包含了关于完全数据集的似然函数 \( p({\bf X}, {\bf Z} | {\bf \theta}) \)，这是我们需要好好加以利用的；\( {\text KL}(q || p) \) 为概率分布 \( q({\bf Z}) \) 与隐变量的后验概率分布 \( p({\bf Z} | {\bf X}, {\bf \theta}) \) 间的 KL 散度，它的值一般大于 \( 0 \)，只有在两个概率分布完全相同的情况下才等于 \( 0 \)，因而其一般被用来衡量两个概率分布之间的差异。 利用 \( {\text KL}(q || p) \ge 0 \) 的性质，我们可以得到 \( {\cal L}(q, {\bf \theta}) \le \ln p({\bf X} | {\bf \theta}) \)，即 \( {\cal L}(q, {\bf \theta}) \) 是对数似然函数 \( \ln p({\bf X} | {\bf \theta}) \) 的一个下界。 \( \ln p({\bf X} | {\bf \theta}) \) 与 \( {\cal L}(q, {\bf \theta}) \) 及 \( {\text KL}(q || p) \) 的关系可用下图中的图（a）来表示。 有了以上的分解之后，下面我们来推导 EM 算法的迭代过程。 假设当前迭代步骤的参数的值为 \( {\bf \theta}^{\text {old}} \)，我们先固定 \( {\bf \theta}^{\text {old}} \) 的值，来求 \( {\cal L}(q, {\bf \theta}^{\text {old}}) \) 关于概率分布 \( q({\bf Z}) \) 的最大值。可以看到，\( \ln p({\bf X} | {\bf \theta} ^{\text {old}}) \) 现在是一个定值，所以当 \( {\text KL}(q || p) \) 等于 \( 0 \) 时， \( {\cal L}(q, {\bf \theta}^{\text {old}}) \) 最大，如上图中的图（b）所示。此时由 \( {\text KL}(q || p) = 0 \) 可以推出，\( q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \)。 现在再固定 \( q({\bf Z}) \)，来求 \( {\cal L}(q, {\bf \theta}) \) 关于 \( {\bf \theta} \) 的最大值，假设求得的最佳 \( {\bf \theta} \) 的值为 \( {\bf \theta} ^{\text {new}} \)，此时 \( {\cal L}(q, {\bf \theta} ^{\text {new}}) \) 相比 \( {\cal L}(q, {\bf \theta}^{\text {old}}) \) 的值增大了，而由于 \( {\bf \theta} \) 值的改变又使得当前 \( {\text KL}(q || p) \) 的值大于或等于 \( 0 \) （当算法收敛时保持 \( 0 \) 的值不变），所以根据式 \( (14) \)，对数似然函数 \( \ln p({\bf X} | {\bf \theta}) \) 的值在本次迭代过程中肯定会有所增长（当算法收敛时保持不变），此步迭代过程如上图中的图（c）所示。 更具体地来说，以上的第一个迭代步骤被称之为 E 步（Expectation），即求解隐变量的后验概率函数 \( p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \)，我们将 \( q({\bf Z}) = p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \) 带入 \( {\cal L}(q, {\bf \theta}) \) 中，可得$$\begin{aligned}{\cal L}(q, {\bf \theta}) &amp;= \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta}) - \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \\&amp; = {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) + {\text {const}}\end{aligned}\tag {15}$$我们只对上式中的第一项 \( {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) \) 感兴趣（第二项为常数），可以看到它是完全数据集的对数似然函数的关于隐变量的后验概率分布的期望值，这也是 EM 算法中 “E” （Expectation）的来源。 第二个迭代步骤被称为 M 步（Maximization），是因为要对式 \( (15) \) 中的 \( {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) \) 求解关于 \( {\bf \theta} \) 的最大值，由于 \( {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) \) 的形式相较于对数似然函数 \( \ln p({\bf X} | {\bf \theta}) \) 来说简化了很多，因而对其求解最大值也是非常方便的，且一般都存在闭式解。 最后还是来总结一下 EM 算法的运行过程： 选择一个初始参数集 \( {\bf \theta}^{\text {old}} \)； E 步，计算隐变量的后验概率函数 \( p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \)； M 步，按下式计算 \( {\bf \theta}^{\text {new}} \) 的值$${\bf \theta}^{\text {new}} = \rm {arg} \rm {max}_{\bf \theta} {\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}})\tag {16}$$其中$${\cal Q} ({\bf \theta}, {\bf \theta}^{\text {old}}) = \sum_{\bf Z} p({\bf Z} | {\bf X}, {\bf \theta}^{\text {old}}) \ln p({\bf X}, {\bf Z} | {\bf \theta})\tag {17}$$ 检查是否满足收敛条件（如前后两次迭代后对数似然函数的差值小于一个阈值），若满足，则结束迭代；若不满足，则令 \( {\bf \theta}^{\text {old}} = {\bf \theta}^{\text {new}} \) ，回到第 2 步继续迭代。 再探高斯混合模型在给出了适用于一般模型的 EM 算法之后，我们再来从一般 EM 算法的迭代步骤推导出适用于高斯混合模型的 EM 算法的迭代步骤（式 \( (7) \) 至 \( (10) \) ）。 推导过程在高斯混合模型中，参数集 \( {\bf \theta} = \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \)，完整数据集 \( \lbrace {\bf X}, {\bf Z} \rbrace \) 的似然函数为$$p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} {\pi_k}^{z_{nk}} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)^{z_{nk}}\tag {18}$$对其取对数可得$$\ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} z_{nk} \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace\tag {19}$$按照 EM 算法的迭代步骤，我们先求解隐变量 \( {\bf Z} \) 的后验概率函数，其具有如下形式$$p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) \propto p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) = \prod_{n = 1}^{N} \prod_{k = 1}^{K} ({\pi_k} {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k))^{z_{nk}}\tag {20}$$再来推导完全数据集的对数似然函数在 \( p({\bf Z} | {\bf X}, {\bf \pi}, {\bf \mu}, {\bf \Sigma}) \) 下的期望$${\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} {\Bbb E}[z_{nk}] \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace\tag {21}$$而 \( {\Bbb E}[z_{nk}] \) 的值可以根据式 \( (20) \) 求出，由于 \( z_{nk} \) 只可能取 \( 1 \) 或 \( 0 \)，而取 \( 0 \) 时对期望没有贡献，故有$${\Bbb E}[z_{nk}] = p(z_{nk} = 1 | {\bf x}_n, {\bf \pi}, {\bf \mu}_k, {\bf \Sigma}_k) = \frac{\pi_k {\cal N}({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k)} { \sum_{j} \pi_j {\cal N} ({\bf x}_n | {\bf \mu}_j, {\bf \Sigma}_j)} = \gamma (z_{nk})\tag {22}$$将上式代入公式 \( (22) \) 中，可得$${\Bbb E}_{\bf Z}[ \ln p({\bf X}, {\bf Z} | {\bf \pi}, {\bf \mu}, {\bf \Sigma}) ] = \sum_{n = 1}^{N} \sum_{k = 1}^{K} \gamma (z_{nk}) \lbrace \ln \pi_k + \ln {\cal N} ({\bf x}_n | {\bf \mu}_k, {\bf \Sigma}_k) \rbrace\tag {23}$$接下来我们就可以对该式关于参数 \( \lbrace {\bf \pi}, {\bf \mu}, {\bf \Sigma} \rbrace \) 求解最大值了，可以验证，各参数的更新方程就是式 \( (8) \) 至 \( (10) \)。 与 k-means 算法的关系敏感的人很容易就发现求解 GMM 的最大似然参数估计问题的 EM 算法和 k-means 算法非常相似，比如二者的每一步迭代都分为两个步骤、二者的每一步迭代都会使目标函数减小或是似然函数增大、二者都对初始值敏感等等。实际上，k-means 算法是“用于求解 GMM 的 EM 算法”的特例。 首先，k-means 算法对数据集的建模是一个简化版的高斯混合模型，该模型仍然含有 \( K \) 个高斯分量，但 k-means 算法做了如下假设： 假设每个高斯分量的先验概率相等，即 \( \pi_k = 1 / K \); 假设每个高斯分量的协方差矩阵均为 \( \epsilon {\bf I} \)。 所以某一个高斯分量的概率密度函数为$$p({\bf x} | {\bf \mu}_k, {\bf \Sigma}_k) = \frac {1} {(2\pi\epsilon)^{D/2}} \exp \lbrace -\frac {\| {\bf x} - {\bf \mu}_k \|^{2}} {2\epsilon} \rbrace\tag {24}$$故根据 EM 算法，可求得隐变量的后验概率函数为$$\gamma(z_{nk}) = \frac{\pi_k \exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \pi_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace } = \frac{\exp \lbrace -\| {\bf x} - {\bf \mu}_k \|^{2} /2\epsilon \rbrace } {\sum_j \exp \lbrace -\| {\bf x} - {\bf \mu}_j \|^{2} /2\epsilon \rbrace }\tag {25}$$在这里，k-means 算法做了第三个重要的改变，它使用硬分配策略来将每一个样本分配给该样本点对应的 \( \gamma(z_{nk}) \) 的值最大的那个高斯分量，即有$$ r_{nk} = \begin{cases} 1, &amp; \text {if \( k = \rm {arg} \rm {min}_{j} \| {\bf x}_n - {\bf \mu}_j \|^{2} \) } \\ 0, &amp; \text {otherwise} \end{cases}\tag {26}$$由于在 k-means 算法里面只有每个高斯分量的期望对其有意义，因而后续也只对 \( {\bf \mu}_k \) 求优，将式 \( (8) \) 中的 \( \gamma(z_{nk}) \) 替换为 \( r_{nk} \)，即可得到 \( {\bf \mu}_k \) 的更新方法，与 k-means 算法中对中心点的更新方法一致。 现在我们再来理解 k-means 算法对数据集的假设就非常容易了：由于假设每个高斯分量的先验概率相等以及每个高斯分量的协方差矩阵都一致且正比于单位阵，所以 k-means 算法期待数据集具有球状的 cluster、期待每个 cluster 中的样本数量相近、期待每个 cluster 的密度相近。 实现 GMM 聚类前面我们看到，在将数据集建模为高斯混合模型，并利用 EM 算法求解出了该模型的参数后，我们可以顺势利用 \( \gamma(z_{nk}) \) 的值来对数据集进行聚类。\( \gamma(z_{nk}) \) 给出了样本 \( {\bf x}_n \) 是由 cluster \( k \) 产生的置信程度，最简单的 GMM 聚类即是将样本 \( {\bf x}_n \) 分配给 \( \gamma(z_{nk}) \) 值最大的 cluster。在这一部分，我们先手写一个简单的 GMM 聚类算法；然后再使用 scikit-learn 中的 GaussianMixture 类来展示 GMM 聚类算法对不同类型的数据集的聚类效果。 利用 python 实现 GMM 聚类首先我们手写了一个 GMM 聚类算法，并将其封装成了一个类，代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import numpy as npimport matplotlib.pyplot as pltimport timefrom sklearn.datasets import make_blobsclass GMMClust(): def __init__(self, n_components=2, max_iter=100, tol=1e-10): self.data_set = None self.n_components = n_components self.pred_label = None self.gamma = None self.component_prob = None self.means = None self.covars = None self.max_iter = max_iter self.tol = tol # 计算高斯分布的概率密度函数 @staticmethod def cal_gaussian_prob(x, mean, covar, delta=1e-10): n_dim = x.shape[0] covar = covar + delta * np.eye(n_dim) prob = np.exp(-0.5 * np.dot((x - mean).reshape(1, n_dim), np.dot(np.linalg.inv(covar), (x - mean).reshape(n_dim, 1)))) prob /= np.sqrt(np.linalg.det(covar) * ((2 * np.pi) ** n_dim)) return prob # 计算每一个样本点的似然函数 def cal_sample_likelihood(self, i): sample_likelihood = sum(self.component_prob[k] * self.cal_gaussian_prob(self.data_set[i], self.means[k], self.covars[k]) for k in range(self.n_components)) return sample_likelihood def predict(self, data_set): self.data_set = data_set self.n_samples, self.n_features = self.data_set.shape self.pred_label = np.zeros(self.n_samples, dtype=int) self.gamma = np.zeros((self.n_samples, self.n_components)) start_time = time.time() # 初始化各参数 self.component_prob = [1.0 / self.n_components] * self.n_components self.means = np.random.rand(self.n_components, self.n_features) for i in range(self.n_features): dim_min = np.min(self.data_set[:, i]) dim_max = np.max(self.data_set[:, i]) self.means[:, i] = dim_min + (dim_max - dim_min) * self.means[:, i] self.covars = np.zeros((self.n_components, self.n_features, self.n_features)) for i in range(self.n_components): self.covars[i] = np.eye(self.n_features) # 开始迭代 pre_L = 0 iter_cnt = 0 while iter_cnt &lt; self.max_iter: iter_cnt += 1 crt_L = 0 # E 步 for i in range(self.n_samples): sample_likelihood = self.cal_sample_likelihood(i) crt_L += np.log(sample_likelihood) for k in range(self.n_components): self.gamma[i, k] = self.component_prob[k] * \ self.cal_gaussian_prob(self.data_set[i], self.means[k], self.covars[k]) / sample_likelihood # M 步 effective_num = np.sum(self.gamma, axis=0) for k in range(self.n_components): self.means[k] = sum(self.gamma[i, k] * self.data_set[i] for i in range(self.n_samples)) self.means[k] /= effective_num[k] self.covars[k] = sum(self.gamma[i, k] * np.outer(self.data_set[i] - self.means[k], self.data_set[i] - self.means[k]) for i in range(self.n_samples)) self.covars[k] /= effective_num[k] self.component_prob[k] = effective_num[k] / self.n_samples print("iteration %s, current value of the log likelihood: %.4f" % (iter_cnt, crt_L)) if abs(crt_L - pre_L) &lt; self.tol: break pre_L = crt_L self.pred_label = np.argmax(self.gamma, axis=1) print("total iteration num: %s, final value of the log likelihood: %.4f, " "time used: %.4f seconds" % (iter_cnt, crt_L, time.time() - start_time)) # 可视化算法的聚类结果 def plot_clustering(self, kind, y=None, title=None): if kind == 1: y = self.pred_label plt.scatter(self.data_set[:, 0], self.data_set[:, 1], c=y, alpha=0.8) if kind == 1: plt.scatter(self.means[:, 0], self.means[:, 1], c='r', marker='x') if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout() 创建一个 GMMClust 类的实例即可对某数据集进行 GMM 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 predict 即可对给定的数据集进行 GMM 聚类；方法 plot_clustering 则可以可视化聚类的结果。利用 GMMClust 类进行 GMM 聚类的代码如下所示：12345678910111213141516171819# 生成数据集n_samples = 1500centers = [[0, 0], [5, 6], [8, 3.5]]cluster_std = [2, 1.0, 0.5]X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std)# 运行 GMM 聚类算法gmm_cluster = GMMClust(n_components=3)gmm_cluster.predict(X) for i in range(3): print("cluster %s" % i) print(" mean: %s, covariance: %s" %(gmm_cluster.means[i], gmm_cluster.covars[i])) # 可视化数据集的原始类别情况以及算法的聚类结果 plt.subplot(1, 2, 1) gmm_cluster.plot_clustering(kind=0, y=y, title='The Original Dataset') plt.subplot(1, 2, 2) gmm_cluster.plot_clustering(kind=1, title='GMM Clustering Result') plt.show() 以上代码首先由三个不同的球形高斯分布产生了一个数据集，之后我们对其进行 GMM 聚类，可得到如下的输出和可视化结果：123456789101112131415161718192021222324252627iteration 1, current value of the log likelihood: -15761.9757iteration 2, current value of the log likelihood: -6435.3937iteration 3, current value of the log likelihood: -6410.5633iteration 4, current value of the log likelihood: -6399.4306iteration 5, current value of the log likelihood: -6389.0317iteration 6, current value of the log likelihood: -6377.9131iteration 7, current value of the log likelihood: -6367.5704iteration 8, current value of the log likelihood: -6359.2076iteration 9, current value of the log likelihood: -6350.8678iteration 10, current value of the log likelihood: -6338.6458... ...iteration 35, current value of the log likelihood: -5859.0324iteration 36, current value of the log likelihood: -5859.0324iteration 37, current value of the log likelihood: -5859.0324iteration 38, current value of the log likelihood: -5859.0324iteration 39, current value of the log likelihood: -5859.0324iteration 40, current value of the log likelihood: -5859.0324total iteration num: 40, final value of the log likelihood: -5859.0324, time used: 18.3565 secondscluster 0 mean: [ 0.10120126 -0.04519941], covariance: [[3.49173063 0.08460269] [0.08460269 3.95599185]]cluster 1 mean: [5.03791461 5.9759609 ], covariance: [[ 1.0864461 -0.00345936] [-0.00345936 0.9630804 ]]cluster 2 mean: [7.99780506 3.51066619], covariance: [[0.23815215 0.01120954] [0.01120954 0.27281129]] 可以看到，对于给定的数据集， GMM 聚类的效果是非常好的，和数据集原本的 cluster 非常接近。其中一部分原因是由于我们产生数据集的模型就是一个高斯混合模型，但另一个更重要的原因可以归结为高斯混合模型是一个比较复杂、可以学习到数据中更为有用的信息的模型；因而一般情况下，GMM 聚类对于其它的数据集的聚类效果也比较好。但由于模型的复杂性，GMM 聚类要比 k-means 聚类迭代的步数要多一些，每一步迭代的计算复杂度也更大一些，因此我们一般不采用运行多次 GMM 聚类算法来应对初始化参数的问题，而是先对数据集运行一次 k-means 聚类算法，找出各个 cluster 的中心点，然后再以这些中心点来对 GMM 聚类算法进行初始化。 利用 sklearn 实现 GMM 聚类sklearn 中的 GaussianMixture 类可以用来进行 GMM 聚类，其中的 fit 函数接收一个数据集，并从该数据集中学习到高斯混合模型的参数；predict 函数则利用前面学习到的模型对给定的数据集进行 GMM 聚类。在这里，我们和前面利用 sklearn 实现 k-means 聚类的时候一样，来考察 GMM 距离在不同类型的数据集下的聚类结果。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npimport matplotlib.pyplot as pltfrom sklearn.mixture import GaussianMixturefrom sklearn.datasets import make_blobsplt.figure(figsize=(12, 12))n_samples = 1500random_state = 170# 产生一个理想的数据集X, y = make_blobs(n_samples=n_samples, random_state=random_state)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X)y_pred = gmm.predict(X)plt.subplot(221)plt.scatter(X[:, 0], X[:, 1], c=y_pred)plt.title("Normal Blobs")# 产生一个非球形分布的数据集transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]X_aniso = np.dot(X, transformation)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_aniso)y_pred = gmm.predict(X_aniso)plt.subplot(222)plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)plt.title("Anisotropicly Distributed Blobs")# 产生一个各 cluster 的密度不一致的数据集X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_varied)y_pred = gmm.predict(X_varied)plt.subplot(223)plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)plt.title("Unequal Density Blobs")# 产生一个各 cluster 的样本数目不一致的数据集X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))gmm = GaussianMixture(n_components=3, random_state=random_state)gmm.fit(X_filtered)y_pred = gmm.predict(X_filtered)plt.subplot(224)plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)plt.title("Unevenly Sized Blobs")plt.show() 运行结果如下图所示： 可以看到，GMM 聚类算法对不同的数据集的聚类效果都很不错，这主要归因于高斯混合模型强大的拟合能力。但对于非凸的或者形状很奇怪的 cluster，GMM 聚类算法的聚类效果会很差，这还是因为它假设数据是由高斯分布所产生，而高斯分布产生的数据组成的 cluster 都是超椭球形的。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>高斯混合模型</tag>
        <tag>GMM</tag>
        <tag>生成模型</tag>
        <tag>EM 算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（二）：k-means 算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Ak-means-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第二篇文章，其它相关文章请参见 聚类分析系列文章。 k-means 聚类算法k-means 算法是一种经典的针对数值型样本的聚类算法。如前面的博文 聚类分析（一）：层次聚类算法 中所述，k-means 算法是一种基于中心点模型的聚类算法，它所产生的每一个 cluster 都维持一个中心点（为属于该 cluster 的所有样本点的均值，k-means 算法由此得名），每一个样本点被分配给距离其最近的中心点所对应的 cluster。在该算法中，cluster 的数目 \( K \) 需要事先指定。我们可以很清楚地看到，在以上聚类场景中，一个最佳的目标是找到 \( K \) 个中心点以及每个样本点的分配方法，使得每个样本点距其被分配的 cluster 所对应的中心点的平方 Euclidean 距离之和最小。而 k-means 算法正是为实现这个目标所提出。 表示为求解特定的优化问题假定数据集为 \( \lbrace {\bf x}_1, {\bf x}_2, …, {\bf x}_N \rbrace \)，其包含 \( N \) 个样本点，每个样本点的维度为 \( D \)。我们的目的是将该数据集划分为 \( K \) 个 cluster，其中 \( K \) 是一个预先给定的值。假设每个 cluster 的中心点为向量 \( {\bf \mu}_k \in \Bbb{R}^{d} \)，其中 \( k = 1, …, K \)。如前面所述，我们的目的是找到中心点 \( \lbrace {\bf \mu}_k \rbrace \)，以及每个样本点所属的类别，以使每个样本点距其被分配的 cluster 所对应的中心点的平方 Euclidean 距离之和最小。 为方便用数学符号描述该优化问题，我们以变量 \( r_{nk} \in \lbrace 0, 1 \rbrace \) 来表示样本点 \( {\bf x}_n \) 是否被分配至第 \( k \) 个 cluster，若样本点 \( {\bf x}_n \) 被分配至第 \( k \) 个 cluster，则 \( r_{nk} = 1 \) 且 \( r_{nj} = 0 \) \( (j \neq k) \)。由此我们可以写出目标函数$$ J = \sum_{n = 1}^{N} \sum_{k = 1}^{K} r_{nk} \| {\bf x}_n - {\bf \mu}_k \|^{2} $$它表示的即是每个样本点与其被分配的 cluster 的中心点的平方 Euclidean 距离之和。整个优化问题用数学语言描述即是寻找优化变量 \( \lbrace r_{nk} \rbrace \) 和 \( \lbrace {\bf \mu}_k \rbrace \) 的值，以使得目标函数 \( J \) 最小。 我们可以看到，由于 \( \lbrace r_{nk} \rbrace \) 的定义域是非凸的，因而整个优化问题也是非凸的，从而寻找全局最优解变得十分困难，因此，我们转而寻找能得到局部最优解的算法。 k-means 算法即是一种简单高效的可以解决上述问题的迭代算法。k-means 算法是一种交替优化（alternative optimization）算法，其每一步迭代包括两个步骤，这两个步骤分别对一组变量求优而将另一组变量视为定值。具体地，首先我们为中心点 \( \lbrace {\bf \mu}_k \rbrace \) 选定初始值；然后在第一个迭代步骤中固定 \( \lbrace {\bf \mu}_k \rbrace \) 的值，对目标函数 \( J \) 根据 \( \lbrace r_{nk} \rbrace \) 求最小值；再在第二个迭代步骤中固定 \( \lbrace r_{nk} \rbrace \) 的值，对 \( J \) 根据 \( {\bf \mu}_k \) 求最小值；如此交替迭代，直至目标函数收敛。 考虑迭代过程中的两个优化问题。首先考虑固定 \( {\bf \mu}_k \) 求解 \( r_{nk} \) 的问题，可以看到 \( J \) 是关于 \( r_{nk} \) 的线性函数，因此我们很容易给出一个闭式解：\( J \) 包含 \( N \) 个独立的求和项，因此我们可以对每一个项单独求其最小值，显然，\( r_{nk} \) 的解为$$ r_{nk} = \begin{cases} 1, &amp; \text {if \( k = \rm {arg} \rm {min}_{j} \| {\bf x}_n - {\bf \mu}_j \|^{2} \) } \\ 0, &amp; \text {otherwise} \end{cases} $$从上式可以看出，此步迭代的含义即是将每个样本点分配给距离其最近的中心点所对应的 cluster。 再来考虑固定 \( r_{nk} \) 求解 \( {\bf \mu}_k \) 的问题，目标函数 \( J \) 是关于 \( {\bf \mu}_k \) 的二次函数，因此可以通过将 \( J \) 关于 \( {\bf \mu}_k \) 的导数置为 0 来求解 \( J \) 关于 \( {\bf \mu}_k \) 的最小值：$$ 2 \sum_{n = 1}^{N} r_{nk}({\bf x}_n - {\bf \mu}_k) = 0 $$容易求出 \( {\bf \mu}_k \) 的值为$$ {\bf \mu}_k = \frac {\sum_{n} r_{nk} {\bf x}_n} {\sum_{n} r_{nk} } $$该式表明，这一步迭代是将中心点更新为所有被分配至该 cluster 的样本点的均值。 k-means 算法的核心即是以上两个迭代步骤，由于每一步迭代均会减小或保持（不会增长）目标函数 \( J \) 的值，因而该算法最终一定会收敛，但可能会收敛至某个局部最优解而不是全局最优解。 虽然上面已经讲的很清楚了，但在这里我们还是总结一下 k-means 算法的过程： 初始化每个 cluster 的中心点。最终的聚类结果受初始化的影响很大，一般采用随机的方式生成中心点，对于比较有特点的数据集可采用一些启发式的方法选取中心点。由于 k-means 算法收敛于局部最优解的特性，在有些情况下我们会选取多组初始值，对其分别运行算法，最终选取目标函数值最小的一组方案作为聚类结果； 将每个样本点分配给距离其最近的中心点所对应的 cluster； 更新每个 cluster 的中心点为被分配给该 cluster 的所有样本点的均值； 交替进行 2～3 步，直至迭代到了最大的步数或者前后两次目标函数的值的差值小于一个阈值为止。 PRML 教材 中给出的 k-means 算法的运行示例非常好，这里拿过来借鉴一下，如下图所示。数据集为经过标准化处理（减去均值、对标准差归一化）后的 Old Faithful 数据集，记录的是黄石国家公园的 Old Faithful 热喷泉喷发的时长与此次喷发距离上次喷发的等待时间。我们选取 \( K = 2 \)，小图（a）为对中心点初始化，小图（b）至小图（i）为交替迭代过程，可以看到，经过短短的数次迭代，k-means 算法就已达到了收敛。 算法复杂度及其优缺点算法复杂度k-means 算法每次迭代均需要计算每个样本点到每个中心点的距离，一共要计算 \( O(NK) \) 次，而计算某一个样本点到某一个中心点的距离所需时间复杂度为 \( O(D) \)，其中 \( N \) 为样本点的个数，\( K \) 为指定的聚类个数，\( D \) 为样本点的维度；因此，一次迭代过程的时间复杂度为 \( O(NKD) \)，又由于迭代次数有限，所以 k-means 算法的时间复杂度为 \( O(NKD) \)。 实际实现中，一般采用高效的数据结构（如 kd 树）来结构化地存储对象之间的距离信息，因而可减小 k-means 算法运行的时间开销。 缺点k-means 算法虽简单易用，但其有一些很明显的缺点，总结如下： 由于其假设每个 cluster 的先验概率是一样的，这样就容易产生出大小（指包含的样本点的多少）相对均匀的 cluster；但事实上的数据的 cluster 有可能并不是如此。 由于其假设每一个 cluster 的分布形状都为球形（spherical），（“球形分布”表明一个 cluster 内的数据在每个维度上的方差都相同，且不同维度上的特征都不相关），这样其产生的聚类结果也趋向于球形的 cluster ，对于具有非凸的或者形状很特别的 cluster 的数据集，其聚类效果往往很差。 由于其假设不同的 cluster 具有相似的密度，因此对于具有密度差别较大的 cluster 的数据集，其聚类效果不好。 其对异常点（outliers）很敏感，这是由于其采用了平方 Euclidean 距离作为距离衡量准则。 cluster 的数目 \( K \) 需要预先指定，但由于很多情况下我们对数据也是知之甚少的，因而怎么选择一个合适的 \( K \) 值也是一个问题。一般确定 \( K \) 的值的方法有以下几种：a）选定一个 \( K \) 的区间，例如 2～10，对每一个 \( K \) 值分别运行多次 k-means 算法，取目标函数 \( J \) 的值最小的 \( K \) 作为聚类数目；b）利用 Elbow 方法 来确定 \( K \) 的值；c）利用 gap statistics 来确定 \( K \) 的值；d）根据问题的目的和对数据的粗略了解来确定 \( K \) 的值。 其对初始值敏感，不好的初始值往往会导致效果不好的聚类结果（收敛到不好的局部最优解）。一般采取选取多组初始值的方法或采用优化初始值选取的算法（如 k-means++ 算法 ）来克服此问题。 其仅适用于数值类型的样本。但其扩展算法 k-modes 算法 （专门针对离散型数据所设计） 和 k-medoid 算法（中心点只能在样本数据集中取得） 适用于离散类型的样本。 其中前面三个缺点都是基于 k-means 算法的假设，这些假设的来源是 k-means 算法仅仅用一个中心点来代表 cluster，而关于 cluster 的其他信息一概没有做限制，那么根据 Occam 剃刀原理 ，k-means 算法中的 cluster 应是最简单的那一种，即对应这三个假设。在博文 聚类分析（三）：高斯混合模型与 EM 算法 中，我们讲到 k-means 算法是 “EM 算法求解高斯混合模型的最大似然参数估计问题” 的特例的时候，会更正式地得出这些假设的由来。 优点尽管 k-means 算法有以上这些缺点，但一些好的地方还是让其应用广泛，其优点总结如下： 实现起来简单，总是可以收敛，算法复杂度低。 其产生的聚类结果容易阐释。 在实际应用中，数据集如果不满足以上部分假设条件，仍然有可能产生比较让人满意的聚类结果。 实现 k-means 聚类在这一部分，我们首先手写一个简单的 k-means 算法，然后用该算法来展示一下不同的初始化值对聚类结果的影响；然后再使用 scikit-learn 中的 KMeans 类来展示 k-means 算法对不同类型的数据集的聚类效果。 利用 python 实现 k-means 聚类首先我们手写一个 k-means 聚类算法，这里，我将该算法封装成了一个类，代码如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport matplotlib.pyplot as pltimport copyimport timefrom sklearn.datasets import make_blobsclass KMeansClust(): def __init__(self, n_clust=2, max_iter=50, tol=1e-10): self.data_set = None self.centers_his = [] self.pred_label = None self.pred_label_his = [] self.n_clust = n_clust self.max_iter = max_iter self.tol = tol def predict(self, data_set): self.data_set = data_set n_samples, n_features = self.data_set.shape self.pred_label = np.zeros(n_samples, dtype=int) start_time = time.time() # 初始化中心点 centers = np.random.rand(self.n_clust, n_features) for i in range(n_features): dim_min = np.min(self.data_set[:, i]) dim_max = np.max(self.data_set[:, i]) centers[:, i] = dim_min + (dim_max - dim_min) * centers[:, i] self.centers_his.append(copy.deepcopy(centers)) self.pred_label_his.append(copy.deepcopy(self.pred_label)) print("The initializing cluster centers are: %s" % centers) # 开始迭代 pre_J = 1e10 iter_cnt = 0 while iter_cnt &lt; self.max_iter: iter_cnt += 1 # E 步：将各个样本点分配给距其最近的中心点所对应的 cluster for i in range(n_samples): self.pred_label[i] = np.argmin(np.sum((self.data_set[i, :] - centers) ** 2, axis=1)) # M 步：更新中心点 for i in range(self.n_clust): centers[i] = np.mean(self.data_set[self.pred_label == i], axis=0) self.centers_his.append(copy.deepcopy(centers)) self.pred_label_his.append(copy.deepcopy(self.pred_label)) # 重新计算目标函数 J crt_J = np.sum((self.data_set - centers[self.pred_label]) ** 2) / n_samples print("iteration %s, current value of J: %.4f" % (iter_cnt, crt_J)) # 若前后两次迭代产生的目标函数的值变化不大，则结束迭代 if np.abs(pre_J - crt_J) &lt; self.tol: break pre_J = crt_J print("total iteration num: %s, final value of J: %.4f, time used: %.4f seconds" % (iter_cnt, crt_J, time.time() - start_time)) # 可视化算法每次迭代产生的结果 def plot_clustering(self, iter_cnt=-1, title=None): if iter_cnt &gt;= len(self.centers_his) or iter_cnt &lt; -1: raise Exception("iter_cnt is not valid!") plt.scatter(self.data_set[:, 0], self.data_set[:, 1], c=self.pred_label_his[iter_cnt], alpha=0.8) plt.scatter(self.centers_his[iter_cnt][:, 0], self.centers_his[iter_cnt][:, 1], c='r', marker='x') if title is not None: plt.title(title, size=14) plt.axis('on') plt.tight_layout() 创建一个 KMeansClust 类的实例即可进行 k-means 聚类，在创建实例的时候，会初始化一系列的参数，如聚类个数、最大迭代次数、终止迭代的条件等等；然后该实例调用自己的方法 predict 即可对给定的数据集进行 k-means 聚类；方法 plot_clustering 则可以可视化每一次迭代所产生的结果。利用 KMeansClust 类进行 k-means 聚类的代码如下所示：123456789101112131415161718if __name__ == '__main__': # 生成数据集 n_samples = 1500 centers = [[0, 0], [5, 6], [8, 3.5]] cluster_std = [2, 1.0, 0.5] X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std) # 运行 k-means 算法 kmeans_cluster = KMeansClust(n_clust=3) kmeans_cluster.predict(X) # 可视化中心点的初始化以及算法的聚类结果 plt.subplots(1, 2) plt.subplot(1, 2, 1) kmeans_cluster.plot_clustering(iter_cnt=0, title='initialization centers') plt.subplot(1, 2, 2) kmeans_cluster.plot_clustering(iter_cnt=-1, title='k-means clustering result') plt.show() 以上代码首先由三个不同的球形高斯分布产生了一个数据集，而后运行了 k-means 聚类方法，中心点的初始化是随机生成的。最终得到如下的输出和可视化结果：123456789101112131415161718192021222324The initializing cluster centers are: [[-6.12152378 2.14971475] [ 6.71575768 -5.41421872] [-1.30016464 -2.3824513 ]]iteration 1, current value of J: 12.5459iteration 2, current value of J: 7.3479iteration 3, current value of J: 5.2928iteration 4, current value of J: 5.1493iteration 5, current value of J: 5.1152iteration 6, current value of J: 5.1079iteration 7, current value of J: 5.1065iteration 8, current value of J: 5.1063iteration 9, current value of J: 5.1052iteration 10, current value of J: 5.0970iteration 11, current value of J: 5.0592iteration 12, current value of J: 4.9402iteration 13, current value of J: 4.5036iteration 14, current value of J: 3.6246iteration 15, current value of J: 3.2003iteration 16, current value of J: 3.1678iteration 17, current value of J: 3.1658iteration 18, current value of J: 3.1657iteration 19, current value of J: 3.1657total iteration num: 19, final value of J: 3.1657, time used: 0.3488 seconds 可以看到，这次算法产生的聚类结果比较好；但并不总是这样，例如某次运行该算法产生的聚类结果如下图所示，可以看出，这一次由于初始值的不同，该算法收敛到了一个不好的局部最优解。 利用 sklearn 实现 k-means 聚类sklearn 中的 KMeans 类可以用来进行 k-means 聚类，sklearn 对该模块进行了计算的优化以及中心点初始化的优化，因而其效果和效率肯定要比上面手写的 k-means 算法要好。在这里，我们直接采用 sklearn 官网的 demo 来展示 KMeans 类的用法，顺便看一下 k-means 算法在破坏了其假设条件的数据集下的运行结果。代码如下（直接照搬 sklearn 官网）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansfrom sklearn.datasets import make_blobsplt.figure(figsize=(12, 12))n_samples = 1500random_state = 170X, y = make_blobs(n_samples=n_samples, random_state=random_state)# 设定一个不合理的 K 值y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)plt.subplot(221)plt.scatter(X[:, 0], X[:, 1], c=y_pred)plt.title("Incorrect Number of Blobs")# 产生一个非球形分布的数据集transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]X_aniso = np.dot(X, transformation)y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)plt.subplot(222)plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)plt.title("Anisotropicly Distributed Blobs")# 产生一个各 cluster 的密度不一致的数据集X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)plt.subplot(223)plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)plt.title("Unequal Variance")# 产生一个各 cluster 的样本数目不一致的数据集X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:50]))y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)plt.subplot(224)plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)plt.title("Unevenly Sized Blobs")plt.show() 运行结果如下图所示： 上述代码分别产生了四个数据集，并分别对它们进行 k-means 聚类。第一个数据集符合所有 k-means 算法的假设条件，但是我们给定的 \( K \) 值与实际数据不符；第二个数据集破坏了球形分布的假设条件；第三个数据集破坏了各 cluster 的密度相近的假设条件；第四个数据集则破坏了各 cluster 内的样本数目相近的假设条件。可以看到，虽然有一些数据集破坏了 k-means 算法的某些假设条件（密度相近、数目相近），但算法的聚类结果仍然比较好；但如果数据集的分布偏离球形分布太远的话，最终的聚类结果会很差。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>k-means 算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析（一）：层次聚类算法]]></title>
    <url>%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章为讲述聚类算法的第一篇文章，其它相关文章请参见 聚类分析系列文章。 聚类算法综述聚类分析（clustering analysis）是将一组对象根据其特征分成不同的 cluster，使得同一 cluster 内的对象在某种意义上比不同的 cluster 之间的对象更为相似。 由于 “cluster” 没有一个明确的定义，因而会有基于不同的模型的聚类算法，其中被广泛运用的聚类算法有以下几类： 基于连通模型（connectivity-based）的聚类算法： 即本文将要讲述的层次聚类算法，其核心思想是按照对象之间的距离来聚类，两个离的近的对象要比两个离的远的对象更有可能属于同一 cluster。 基于中心点模型（centroid-based）的聚类算法： 在此类算法中，每个 cluster 都维持一个中心点（centorid），该中心点不一定属于给定的数据集。当预先指定聚类数目 k 时，此类算法需要解决一个优化问题，目标函数为所有的对象距其所属的 cluster 的中心点的距离的平方和，优化变量为每个 cluster 的中心点以及每个对象属于哪个 cluster；此优化问题被证明是 NP-hard 的，但有一些迭代算法可以找到近似解， k-means 算法 即是其中的一种。 基于分布模型（distribution-based）的聚类算法： 此类算法认为数据集中的数据是由一种混合概率模型所采样得到的，因而只要将可能属于同一概率分布所产生的数据归为同一 cluster 即可，最常被使用的此类算法为 高斯混合模型（GMM）聚类。 基于密度（density-based）的聚类算法： 在此类算法中，密度高的区域被归为一个 cluster，cluster 之间由密度低的区域隔开，密度低的区域中的点被认为是噪声 （noise），常用的密度聚类算法为 DBSCAN 和 OPTICS。 层次聚类综述层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 clusters，后面一层生成的 clusters 基于前面一层的结果。层次聚类算法一般分为两类： Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。本文主要关注此类算法。 Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。 下图直观的给出了层次聚类的思想以及以上两种聚类策略的异同。 另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。 树形图 树形图 （dendrogram）可以用来直观地表示层次聚类的成果。一个有 5 个点的树形图如下图所示，其中纵坐标高度表示不同的 cluster 之间的距离（“距离”的衡量准则可以多种多样，详见本文后面的定义），可以从这张图看到，\( x_1 \) 和 \( x_2 \) 的距离最近（为 1），因此将 \( x_1 \) 和 \( x_2 \) 合并为一个 cluster \( (x_1, x_2) \)，所以在树形图中首先将节点 \( x_1 \) 和 \( x_2 \) 连接，使其成为一个新的节点 \( (x_1, x_2) \) 的子节点，并将这个新的节点的高度置为 1；之后再在剩下的 4 个 cluster \( (x_1, x_2) \)， \( x_3 \)， \( x_4 \) 和 \( x_5 \) 中选取距离最近的两个 cluster 合并，\( x_4 \) 和 \( x_5 \) 的距离最近（为 2），因此将 \( x_4 \) 和 \( x_5 \) 合并为一个 cluster \( (x_4, x_5) \)，体现在树形图上，是将节点 \( x_4 \) 和 \( x_5 \) 连接，使其成为一个新的节点 \( (x_4, x_5) \) 的子节点，并将此新节点的高度置为 2；….依此模式进行树形图的生成，直至最终只剩下一个 cluster \( ((x_1, x_2), x_3), (x_4, x_5)) \)。 可以直观地看到，如果我们想得到一个聚类结果，使任意的两个 cluster 之间的距离都不大于 \( h \)，我们只要在树形图上作一条水平的直线对其进行切割，使其纵坐标等于 \( h \)，即可获得对应的聚类结果。例如，在下面的树形图中，设 \( h=2.5 \)，即可得到 3 个 cluster \( (x_1, x_2) \)， \( x_3 \) 和 \( (x_4, x_5) \)。 对象之间的距离衡量衡量两个对象之间的距离的方式有多种，对于数值类型（Numerical）的数据，常用的距离衡量准则有 Euclidean 距离、Manhattan 距离、Chebyshev 距离、Minkowski 距离等等。对于 \( d \) 维空间的两个对象 \({\bf x} =[ x_1, x_2, …, x_d]^{T} \) 和 \({\bf y} = [y_1, y_2, …, y_d]^{T}\)，其在不同距离准则下的距离计算方法如下表所示: 距离准则 距离计算方法 Euclidean 距离 \( d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{2} ]^{\frac{1}{2}} = [({\bf x} - {\bf y})^{T} ({\bf x} - {\bf y})]^{\frac{1}{2}} \) Manhattan 距离 \( d({\bf x},{\bf y}) = \sum_{j=1}^{d} \mid{x_j-y_j}\mid \) Chebyshev 距离 \( d({\bf x},{\bf y}) = \max_{1\leq{j}\leq{d}} \mid{x_j-y_j}\mid \) Minkowski 距离 \( d({\bf x},{\bf y}) = [\sum_{j=1}^{d} (x_j-y_j)^{p} ]^{\frac{1}{p}}, p\geq{1} \) Minkowski 距离就是 \( \rm{L}\it{p}\) 范数（\( p\geq{1} \))，而 Manhattan 距离、Euclidean 距离、Chebyshev 距离分别对应 \( p = 1, 2, \infty \) 时的情形。 另一种常用的距离是 Maholanobis 距离，其定义如下：$$ d_{mah}({\bf x}, {\bf y}) = \sqrt{({\bf x} - {\bf y})^{T}\Sigma^{-1} ({\bf x} - {\bf y})} $$其中 \( \Sigma \) 为数据集的协方差矩阵，为给出协方差矩阵的定义，我们先给出数据集的一些设定，假设数据集为 \( {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) \in \Bbb{R}^{d \times n}\)，\( {\bf x}_i \in \Bbb{R}^{d} \) 为第 \( i \) 个样本点，每个样本点的维度为 \( d \)，样本点的总数为 \( n \) 个；再假设样本点的平均值 \( m_{\bf x} = \frac{1}{n}\sum_{i=1}^{n} {\bf x}_i \) 为 \( {\bf 0} \) 向量（若不为 \( {\bf 0} \)，我们总可以将每个样本都减去数据集的平均值以得到符合要求的数据集），则协方差矩阵 \( \Sigma \in \Bbb{R}^{d \times d} \) 可被定义为$$ \Sigma = \frac{1}{n} {\bf X}{\bf X}^{T} $$Maholanobis 距离不同于 Minkowski 距离，后者衡量的是两个对象之间的绝对距离，其值不受数据集的整体分布情况的影响；而 Maholanobis 距离则衡量的是将两个对象置于整个数据集这个大环境中的一个相异程度，两个绝对距离较大的对象在一个分布比较分散的数据集中的 Maholanobis 距离有可能会比两个绝对距离较小的对象在一个分布比较密集的数据集中的 Maholanobis 距离更小。更细致地来讲，Maholanobis 距离是这样计算得出的：先对数据进行主成分分析，提取出互不相干的特征，然后再将要计算的对象在这些特征上进行投影得到一个新的数据，再在这些新的数据之间计算一个加权的 Euclidean 距离，每个特征上的权重与该特征在数据集上的方差成反比。由这些去相干以及归一化的操作，我们可以看到，对数据进行任意的可逆变换，Maholanobis 距离都保持不变。 Cluster 之间的距离衡量除了需要衡量对象之间的距离之外，层次聚类算法还需要衡量 cluster 之间的距离，常见的 cluster 之间的衡量方法有 Single-link 方法、Complete-link 方法、UPGMA（Unweighted Pair Group Method using arithmetic Averages）方法、WPGMA（Weighted Pair Group Method using arithmetic Averages）方法、Centroid 方法（又称 UPGMC，Unweighted Pair Group Method using Centroids）、Median 方法（又称 WPGMC，weighted Pair Group Method using Centroids）、Ward 方法。前面四种方法是基于图的，因为在这些方法里面，cluster 是由样本点或一些子 cluster （这些样本点或子 cluster 之间的距离关系被记录下来，可认为是图的连通边）所表示的；后三种方法是基于几何方法的（因而其对象间的距离计算方式一般选用 Euclidean 距离），因为它们都是用一个中心点来代表一个 cluster 。假设 \( C_i \) 和 \( C_j \) 为两个 cluster，则前四种方法定义的 \( C_i \) 和 \( C_j \) 之间的距离如下表所示： 方法 定义 Single-link \( D(C_i, C_j) = \min_{ {\bf x} \in C_i, {\bf y} \in C_j } d({\bf x}, {\bf y}) \) Complete-link \( D(C_i, C_j) = \max_{ {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) \) UPGMA \( D(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y} \in C_j} d({\bf x}, {\bf y}) \) WPGMA omitting 其中 Single-link 定义两个 cluster 之间的距离为两个 cluster 之间距离最近的两个对象间的距离，这样在聚类的过程中就可能出现链式效应，即有可能聚出长条形状的 cluster；而 Complete-link 则定义两个 cluster 之间的距离为两个 cluster 之间距离最远的两个对象间的距离，这样虽然避免了链式效应，但其对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类；UPGMA 正好是 Single-link 和 Complete-link 的一个折中，其定义两个 cluster 之间的距离为两个 cluster 之间两个对象间的距离的平均值；而 WPGMA 则计算的是两个 cluster 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 cluster 对距离的计算的影响在同一层次上，而不受 cluster 大小的影响（其计算方法这里没有给出，因为在运行层次聚类算法时，我们并不会直接通过样本点之间的距离之间计算两个 cluster 之间的距离，而是通过已有的 cluster 之间的距离来计算合并后的新的 cluster 和剩余 cluster 之间的距离，这种计算方法将由下一部分中的 Lance-Williams 方法给出）。 Centroid/UPGMC 方法给每一个 cluster 计算一个质心，两个 cluster 之间的距离即为对应的两个质心之间的距离，一般计算方法如下：$$ D_{\rm{UPGMC}}(C_i, C_j) = \frac{1}{\mid C_i \mid \mid C_j \mid} \sum_ { {\bf x} \in C_i, {\bf y}\in C_j} d({\bf x}, {\bf y}) - \frac{1}{2{\mid C_i \mid }^{2}} \sum_ { {\bf x}, {\bf y}\in C_i} d( {\bf x}, {\bf y}) - \frac{1}{2{\mid \it{C_j} \mid }^{2}} \sum_ { {\bf x}, {\bf y} \in C_j} d( {\bf x}, {\bf y}) $$当上式中的 \( d(.,.) \) 为平方 Euclidean 距离时，\( D_{\rm{UPGMC}}(C_i, C_j) \) 为 \( C_i \) 和 \( C_j \) 的中心点（每个 cluster 内所有样本点之间的平均值）之间的平方 Euclidean 距离。 Median/WPGMC 方法为每个 cluster 计算质心时，引入了权重。 Ward 方法提出的动机是最小化每次合并时的信息损失，具体地，其对每一个 cluster 定义了一个 ESS （Error Sum of Squares）量作为衡量信息损失的准则，cluster \( C \) 的 \( {\rm ESS} \) 定义如下：$$ {\rm ESS} ( C ) = \sum_{ {\bf x} \in C} ({\bf x} - m_{\bf x})^{T} ({\bf x} - m_{\bf x}) $$其中 \( m_{\bf x} \) 为 \( C \) 中样本点的均值。可以看到 \( {\rm ESS} \) 衡量的是一个 cluster 内的样本点的聚合程度，样本点越聚合，\( {\rm ESS} \) 的值越小。Ward 方法则是希望找到一种合并方式，使得合并后产生的新的一系列的 cluster 的 \( {\rm ESS} \) 之和相对于合并前的 cluster 的 \( {\rm ESS} \) 之和的增长最小。 Agglomerative 层次聚类算法这里总结有关 Agglomerative 层次聚类算法的内容，包括最简单的算法以及用于迭代计算两个 cluster 之间的距离的 Lance-Williams 方法。 Lance-Williams 方法在 Agglomerative 层次聚类算法中，一个迭代过程通常包含将两个 cluster 合并为一个新的 cluster，然后再计算这个新的 cluster 与其他当前未被合并的 cluster 之间的距离，而 Lance-Williams 方法提供了一个通项公式，使得其对不同的 cluster 距离衡量方法都适用。具体地，对于三个 cluster \( C_k \)，\( C_i \) 和 \( C_j \)， Lance-Williams 给出的 \( C_k \) 与 \( C_i \) 和 \( C_j \) 合并后的新 cluster 之间的距离的计算方法如下式所示：$$ D(C_k, C_i \cup C_j) = \alpha_i D(C_k, C_i) + \alpha_j D(C_k, C_j) + \beta D(C_i, C_j) + \gamma \mid D(C_k, C_i) - D(C_k, C_j) \mid $$其中，\( \alpha_i \)，\( \alpha_j \)，\( \beta \)，\( \gamma \) 均为参数，随 cluster 之间的距离计算方法的不同而不同，具体总结为下表（注：\( n_i \) 为 cluster \( C_i \) 中的样本点的个数)： 方法 参数 \( \alpha_i \) 参数 \( \alpha_j \) 参数 \( \beta \) 参数 \( \gamma \) Single-link \( 1/2 \) \( 1/2 \) \( 0 \) \( -1/2 \) Complete-link \( 1/2 \) \( 1/2 \) \( 0 \) \( 1/2 \) UPGMA \( n_i/(n_i + n_j) \) \( n_j/(n_i + n_j) \) \( 0 \) \( 0 \) WPGMA \( 1/2 \) \( 1/2 \) \( 0 \) \( 0 \) UPGMC \( n_i/(n_i + n_j) \) \( n_j/(n_i + n_j) \) \( n_{i}n_{j}/(n_i + n_j)^{2} \) \( 0 \) WPGMC \( 1/2 \) \( 1/2 \) \( 1/4 \) \( 0 \) Ward \( (n_k + n_i)/(n_i + n_j + n_k) \) \( (n_k + n_j)/(n_i + n_j + n_k) \) \( n_k/(n_i + n_j + n_k) \) \( 0 \) 其中 Ward 方法的参数仅适用于当样本点之间的距离衡量准则为平方 Euclidean 距离时，其他方法的参数适用范围则没有限制。 Naive 算法给定数据集 \( {\bf{X}} = ({\bf x}_1, {\bf x}_2, …, {\bf x}_n) \)，Agglomerative 层次聚类最简单的实现方法分为以下几步： 初始时每个样本为一个 cluster，计算距离矩阵 \( \bf D \)，其中元素 \( D_{ij} \) 为样本点 \( {\bf x}_i \) 和 \( {\bf x}_j \) 之间的距离； 遍历距离矩阵 \( \bf D \)，找出其中的最小距离（对角线上的除外），并由此得到拥有最小距离的两个 cluster 的编号，将这两个 cluster 合并为一个新的 cluster 并依据 Lance-Williams 方法更新距离矩阵 \( \bf D \) （删除这两个 cluster 对应的行和列，并把由新 cluster 所算出来的距离向量插入 \( \bf D \) 中），存储本次合并的相关信息； 重复 2 的过程，直至最终只剩下一个 cluster 。 当然，其中的一些细节这里都没有给出，比如，我们需要设计一些合适的数据结构来存储层次聚类的过程，以便于我们可以根据距离阈值或期待的聚类个数得到对应的聚类结果。 可以看到，该 Naive 算法的时间复杂度为 \( O(n^{3}) \) （由于每次合并两个 cluster 时都要遍历大小为 \( O(n^{2}) \) 的距离矩阵来搜索最小距离，而这样的操作需要进行 \( n - 1 \) 次），空间复杂度为 \( O(n^{2}) \) （由于要存储距离矩阵）。 当然，还有一些更高效的算法，它们用到了特殊设计的数据结构或者一些图论算法，是的时间复杂度降低到 \( O(n^{2} ) \) 或更低，例如 SLINK 算法（Single-link 方法），CLINK 算法（Complete-link 方法），BIRCH 算法（适用于 Euclidean 距离准则）等等。 利用 Scipy 实现层次聚类在这里我们将利用 SciPy（python 中的一个用于数值分析和科学计算的第三方包，功能强大，NumPy+SciPy+matplotlib 基本等同于 Matlab）中的层次聚类模块来做一些相关的实验。 生成实验样本集首先，我们需要导入相关的模块，代码如下所示：12345# python 3.6&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from scipy.cluster.hierarchy import linkage, dendrogram, fcluster&gt;&gt;&gt; from sklearn.datasets.samples_generator import make_blobs&gt;&gt;&gt; import matplotlib.pyplot as plt 其中 make_blobs 函数是 python 中的一个第三方机器学习库 scikit-learn 所提供的一个生成指定个数的高斯 cluster 的函数，非常适合用于聚类算法的简单实验，我们用该函数生成实验样本集，生成的样本集 X 的维度为 n*d。1234567&gt;&gt;&gt; centers = [[1, 1], [-1, -1], [1, -1]] # 定义 3 个中心点# 生成 n=750 个样本，每个样本的特征个数为 d=2，并返回每个样本的真实类别&gt;&gt;&gt; X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0） &gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c='b')&gt;&gt;&gt; plt.title('The dataset')&gt;&gt;&gt; plt.show() 样本的分布如下图所示。 进行 Agglomerative 层次聚类SciPy 里面进行层次聚类非常简单，直接调用 linkage 函数，一行代码即可搞定。1&gt;&gt;&gt; Z = linkage(X, method='ward', metric='euclidean') 以上即进行了一次 cluster 间距离衡量方法为 Ward、样本间距离衡量准则为 Euclidean 距离的 Agglomerative 层次聚类；其中 method 参数可以为 &#39;single&#39;、 &#39;complete&#39; 、&#39;average&#39;、 &#39;weighted&#39;、 &#39;centroid&#39;、 &#39;median&#39;、 &#39;ward&#39; 中的一种，分别对应我们前面讲到的 Single-link、Complete-link、UPGMA、WPGMA、UPGMC、WPGMC、Ward 方法；而样本间的距离衡量准则也可以由 metric 参数调整。 linkage 函数的返回值 Z 为一个维度 (n-1)*4 的矩阵， 记录的是层次聚类每一次的合并信息，里面的 4 个值分别对应合并的两个 cluster 的序号、两个 cluster 之间的距离以及本次合并后产生的新的 cluster 所包含的样本点的个数；具体地，对于第 i 次迭代（对应 Z 的第 i 行），序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 合并产生新的 cluster n + i, Z[i, 2] 为序号为 Z[i, 0] 和序号为 Z[i, 1] 的 cluster 之间的距离，合并后的 cluster 包含 Z[i, 3] 个样本点。 例如本次实验中 Z 记录到的前 25 次合并的信息如下所示：12345678910111213141516171819202122232425262728&gt;&gt;&gt; print(Z.shape)(749, 4)&gt;&gt;&gt; print(Z[: 25])[[253. 491. 0.00185 2. ] [452. 696. 0.00283 2. ] [ 70. 334. 0.00374 2. ] [237. 709. 0.00378 2. ] [244. 589. 0.00423 2. ] [141. 550. 0.00424 2. ] [195. 672. 0.00431 2. ] [ 71. 102. 0.00496 2. ] [307. 476. 0.00536 2. ] [351. 552. 0.00571 2. ] [ 62. 715. 0.00607 2. ] [ 98. 433. 0.0065 2. ] [255. 572. 0.00671 2. ] [437. 699. 0.00685 2. ] [ 55. 498. 0.00765 2. ] [143. 734. 0.00823 2. ] [182. 646. 0.00843 2. ] [ 45. 250. 0.0087 2. ] [298. 728. 0.00954 2. ] [580. 619. 0.01033 2. ] [179. 183. 0.01062 2. ] [101. 668. 0.01079 2. ] [131. 544. 0.01125 2. ] [655. 726. 0.01141 2. ] [503. 756. 0.01265 3. ]] 从上面的信息可以看到，在第 6 次合并中，样本点 141 与样本点 550 进行了合并，生成新 cluster 756；在第 25 次合并中，样本点 503 与 cluster 756 进行了合并，生成新的 cluster 770。我们可以将样本点 141、550 和 503 的特征信息打印出来，来看看它们是否确实很接近。1234&gt;&gt;&gt; print(X[[141, 550, 503]])[[ 1.27098 -0.97927] [ 1.27515 -0.98006] [ 1.37274 1.13599]] 看数字，这三个样本点确实很接近，从而也表明了层次聚类算法的核心思想：每次合并的都是当前 cluster 中相隔最近的。 画出树形图SciPy 中给出了根据层次聚类的结果 Z 绘制树形图的函数 dendrogram，我们由此画出本次实验中的最后 20 次的合并过程。123456&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; dendrogram(Z, truncate_mode='lastp', p=20, show_leaf_counts=False, leaf_rotation=90, leaf_font_size=15, show_contracted=True)&gt;&gt;&gt; plt.title('Dendrogram for the Agglomerative Clustering')&gt;&gt;&gt; plt.xlabel('sample index')&gt;&gt;&gt; plt.ylabel('distance')&gt;&gt;&gt; plt.show() 得到的树形图如下所示。 可以看到，该树形图的最后两次合并相比之前合并过程的合并距离要大得多，由此可以说明最后两次合并是不合理的；因而对于本数据集，该算法可以很好地区分出 3 个 cluster（和实际相符），分别在上图中由三种颜色所表示。 获取聚类结果在得到了层次聚类的过程信息 Z 后，我们可以使用 fcluster 函数来获取聚类结果。可以从两个维度来得到距离的结果，一个是指定临界距离 d，得到在该距离以下的未合并的所有 cluster 作为聚类结果；另一个是指定 cluster 的数量 k，函数会返回最后的 k 个 cluster 作为聚类结果。使用哪个维度由参数 criterion 决定，对应的临界距离或聚类的数量则由参数 t 所记录。fcluster 函数的结果为一个一维数组，记录每个样本的类别信息。 对应的代码与返回结果如下所示。123456789101112131415161718# 根据临界距离返回聚类结果&gt;&gt;&gt; d = 15&gt;&gt;&gt; labels_1 = fcluster(Z, t=d, criterion='distance')&gt;&gt;&gt; print(labels_1[: 100]) # 打印聚类结果[2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; print(len(set(labels_1))) # 看看在该临界距离下有几个 cluster3 # 根据聚类数目返回聚类结果&gt;&gt;&gt; k = 3&gt;&gt;&gt; labels_2 = fcluster(Z, t=k, criterion='maxclust')&gt;&gt;&gt; print(labels_2[: 100]) [2 1 2 3 2 1 1 3 2 2 1 1 1 3 1 2 1 1 3 3 3 3 3 3 1 1 3 2 2 3 2 1 1 2 1 2 3 2 2 3 3 1 1 1 1 1 2 3 2 1 3 3 1 1 3 3 1 2 3 1 3 3 3 3 3 2 3 3 2 2 2 3 2 2 3 1 2 1 2 3 1 1 2 2 2 2 1 3 1 3 3 2 1 2 1 2 1 1 2 2]&gt;&gt;&gt; list(labels_1) == list(labels_2) # 看看两种不同维度下得到的聚类结果是否一致True 下面将聚类的结果可视化，相同的类的样本点用同一种颜色表示。1234&gt;&gt;&gt; plt.figure(figsize=(10, 8))&gt;&gt;&gt; plt.title('The Result of the Agglomerative Clustering')&gt;&gt;&gt; plt.scatter(X[:, 0], X[:, 1], c=labels_2, cmap='prism')&gt;&gt;&gt; plt.show() 可视化结果如下图所示。 上图的聚类结果和实际的数据分布基本一致，但有几点值得注意，一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。 比较不同方法下的聚类结果最后，我们对同一份样本集进行了 cluster 间距离衡量准则分别为 Single-link、Complete-link、UPGMA（Average）和 Ward 的 Agglomerative 层次聚类，取聚类数目为 3，程序如下：1234567891011121314151617181920212223242526272829303132333435363738from time import timeimport numpy as npfrom scipy.cluster.hierarchy import linkage, fclusterfrom sklearn.datasets.samples_generator import make_blobsfrom sklearn.metrics.cluster import adjusted_mutual_info_scoreimport matplotlib.pyplot as plt# 生成样本点centers = [[1, 1], [-1, -1], [1, -1]]X, labels = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)# 可视化聚类结果def plot_clustering(X, labels, title=None): plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='prism') if title is not None: plt.title(title, size=17) plt.axis('off') plt.tight_layout()# 进行 Agglomerative 层次聚类linkage_method_list = ['single', 'complete', 'average', 'ward']plt.figure(figsize=(10, 8))ncols, nrows = 2, int(np.ceil(len(linkage_method_list) / 2))plt.subplots(nrows=nrows, ncols=ncols)for i, linkage_method in enumerate(linkage_method_list): print('method %s:' % linkage_method) start_time = time() Z = linkage(X, method=linkage_method) labels_pred = fcluster(Z, t=3, criterion='maxclust') print('Adjust mutual information: %.3f' % adjusted_mutual_info_score(labels, labels_pred)) print('time used: %.3f seconds' % (time() - start_time)) plt.subplot(nrows, ncols, i + 1) plot_clustering(X, labels_pred, '%s linkage' % linkage_method)plt.show() 可以得到 4 种方法下的聚类结果如下图所示。 在上面的过程中，我们还为每一种聚类产生的结果计算了一个用于评估聚类结果与样本的真实类之间的相近程度的 AMI()（Adjust Mutual Information）量，该量越接近于 1 则说明聚类算法产生的类越接近于真实情况。程序的打印结果如下： 123456789101112method single:Adjust mutual information: 0.001time used: 0.008 secondsmethod complete:Adjust mutual information: 0.838time used: 0.013 secondsmethod average:Adjust mutual information: 0.945time used: 0.019 secondsmethod ward:Adjust mutual information: 0.956time used: 0.015 seconds 从上面的图和 AMI 量的表现来看，Single-link 方法下的层次聚类结果最差，它几乎将所有的点都聚为一个 cluster，而其他两个 cluster 则都仅包含个别稍微有点偏离中心的样本点，这充分体现了 Single-link 方法下的“链式效应”，也体现了 Agglomerative 算法的一个特点，即“赢者通吃”（rich getting richer）： Agglomerative 算法倾向于聚出不均匀的类，尺寸大的类倾向于变得更大，对于 Single-link 和 UPGMA（Average） 方法尤其如此。由于本次实验的样本集较为理想，因此除了 Single-link 之外的其他方法都表现地还可以，但当样本集变复杂时，上述“赢者通吃” 的特点会显现出来。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>非监督学习</tag>
        <tag>层次聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的 Unicode String 和 Byte String]]></title>
    <url>%2FPython-%E4%B8%AD%E7%9A%84-Unicode-string-%E5%92%8C-Byte-string%2F</url>
    <content type="text"><![CDATA[python 2.x 和 python 3.x 字符串类型的区别python 2.x 中字符编码的坑是历史遗留问题，到 python 3.x 已经得到了很好的解决，在这里简要梳理一下二者处理字符串的思路。 python 2.x str 类型：处理 binary 数据和 ASCII 文本数据。 unicode 类型：处理非 ASCII 文本数据。 python 3.x bytes 类型：处理 binary 数据，同 str 类型一样是一个序列类型，其中每个元素均为一个 byte（本质上是一个取值为 0~255 的整型对象），用于处理二进制文件或数据（如图像，音频等）。 str 类型：处理 unicode 文本数据（包含 ASCII 文本数据）。 bytearray 类型：bytes 类型的变种，但是此类型是 mutable 的。 Unicode 简介包括 ASCII 码、latin-1 编码 和 utf-8 编码 等在内的码都被认为是 unicode 码。 编码和解码的概念 编码（encoding）：将字符串映射为一串原始的字节。 解码（decoding）：将一串原始的字节翻译成字符串。 ASCII码 编码长度为 1 个 byte. 编码范围为 0x00~0x7F，只包含一些常见的字符。 latin-1码 编码长度为 1 个 byte. 编码范围为 0x00~0xFF，能支持更多的字符（如 accent character），兼容 ASCII 码。 utf-8码 编码长度可变，为 1~4 个 byte。 当编码长度为 1 个 byte 时，等同于 ASCII 码，取值为 0x00 ~ 0x7F；当编码长度大于 1 个 byte 时，每个 byte 的取值为 0x80 ~ 0xFF。 其它编码 utf-16，编码长度为定长 2 个 byte。 utf-32，编码长度为定长 4 个 byte。 Unicode 字符串的存储方式在内存中的存储方式unicode 字符串中的字符在内存中以一种与编码方式无关的方式存储：unicode code point，它是一个数字，范围为 0~1,114,111，可以唯一确定一个字符。在表示 unicode 字符串时可以以 unicode code point 的方式表示， 例如在下面的例子 中，a 和 b 表示的是同一字符串（其中 &#39;\uNNNN&#39; 即为 unicode code point，N 为一个十六进制位，十六进制位的个数为 4~6 位；当 unicode code point 的取值在 0~255 范围内时，也可以 &#39;\xNN&#39; 的形式表示）：12345678# python 2.7&gt;&gt;&gt; a = u'\u5a1c\u5854\u838e'&gt;&gt;&gt; b = u'娜塔莎'&gt;&gt;&gt; print a, b娜塔莎 娜塔莎&gt;&gt;&gt; c = u'\xe4'&gt;&gt;&gt; print cä 在文件等外部媒介中的存储方式unicode 字符串在文件等外部媒介中须按照指定的编码方式将字符串转换为原始字节串存储。 字符表示python 3.x在 python 3.x 中，str 类型即可满足日常的字符需求（不论是 ASCII 字符还是国际字符），如下例所示：123456# python 3.6&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; len(a)12 可以看到，python 3.x 中得到的 a 的长度为 12（包含空格），没有任何问题；我们可以对 a 进行编码，将其转换为 bytes 类型：123456# python 3.6&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; bb'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)bytes 从上面可以看出，bytes 类型的对象中的某个字节的取值在 0x00 ~ 0x7F 时，控制台的输出会显示出其对应的 ASCII 码字符，但其本质上是一个原始字节，不应与任何字符等同。 同理，我们也可以将一个 bytes 类型的对象译码为一个 str 类型的对象：1234# python 3.6&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; a'Natasha, 娜塔莎' python 2.x在 python 2.x 中，如果还是用 str 类型来表示国际字符，就会有问题：12345678910# python 2.7&gt;&gt;&gt; a = 'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)str&gt;&gt;&gt; a'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; len(a)18&gt;&gt;&gt; print aNatasha, 娜塔莎 可以看到，python 2.x 中虽然定义了一个 ASCII 字符和中文字符混合的 str 字符串，但实际上 a 在内存中存储为一串字节序列，且长度也是该字节序列的长度，很明显与我们的定义字符串的初衷不符合。值得注意的是，这里 a 正好是字符串 &#39;Natasha, 娜塔莎&#39; 的 utf-8 编码的结果，且将 a 打印出来的结果和我们的定义初衷相符合，这其实与控制台的默认编码方式有关，这里控制台的默认编码方式正好是 utf-8，获取控制台的默认编码方式的方式如下:123456# python 2.7&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdin.encoding # 控制台的输入编码，可解释前例中 a 在内存中的表现形式'utf-8'&gt;&gt;&gt; sys.stdout.encoding # 控制台的输出编码，可解释前例中打印 a 的显示结果'utf-8' 另外，sys.getdefaultencoding()函数也会得到一种编码方式，得到的结果是系统的默认编码方式，在 python 2.x 中，该函数总是返回 &#39;ascii&#39;, 这表明在对字符串编译码时不指定编码方式时所采用的编码方式为ASCII 编码；除此之外，在 python 2.x 中，ASCII 编码方式还会被用作隐式转换，例如 json.dumps() 函数在默认情况下总是返回一串字节串，不论输入的数据结构里面的字符串是 unicode 类型还是 str 类型。在 python 3.x 中，隐式转换已经被禁止（也可以说，python 3.x 用不到隐式转换：>）。 切回正题，在 python 2.x 表示国际字符的正确方式应该是定义一个 unicode 类型字符串，如下所示：1234567891011121314# python 2.7&gt;&gt;&gt; a = u'Natasha, 娜塔莎'&gt;&gt;&gt; type(a)unicode&gt;&gt;&gt; len(a)12&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; b'Natasha, \xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e'&gt;&gt;&gt; type(b)str&gt;&gt;&gt; a = b.decode('utf-8')&gt;&gt;&gt; au'Natasha, \u5a1c\u5854\u838e' 另外，我们可以对 unicode 类型字符串进行编码操作，对 str 类型字符串进行译码操作。 文本文件操作python 3.x在 python 3.x 中，文本文件的读写过程中的编解码过程可以通过指定 open 函数的参数 encoding 的值来自动进行（python 3.x 中的默认情况下文件的编码方式可以由函数 sys.getfilesystemencoding()得到，如：12345678910111213# python 3.6&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)3&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.read()'娜塔莎'&gt;&gt;&gt; f.close() 当然，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：12345678910# python 3.6&gt;&gt;&gt; a = '娜塔莎'&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'wb')&gt;&gt;&gt; f.write(b)9&gt;&gt;&gt; f.close()&gt;&gt;&gt; f.read().decode('utf-8')'娜塔莎'&gt;&gt;&gt; f.close() python 2.x在 python 2.x 中，open 函数只支持读写二进制文件或者文件中的字符大小为 1 个 Byte 的文件，写入的数据为字节，读取出来的数据类型为 str；codecs.open 函数则支持自动读写 unicode 文本文件，如：12345678910# python 2.7&gt;&gt;&gt; import codecs&gt;&gt;&gt; a = u'安德烈'&gt;&gt;&gt; f = codecs.open('data.txt', 'w', encoding='utf-8')&gt;&gt;&gt; f.write(a)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = codecs.open('data.txt', 'r', encoding='utf-8') &gt;&gt;&gt; print f.read()安德烈&gt;&gt;&gt; f.close() 类似地，也可以先手动将字符串编码为字节串，然后再以二进制模式的方式写入文件，再以二进制模式的方式读取文件，最后再手动将读取出来的数据解码为字符串，如：123456789# python 2.7&gt;&gt;&gt; b = a.encode('utf-8')&gt;&gt;&gt; f = open('data.txt', 'w')&gt;&gt;&gt; f.write(b)&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open('data.txt', 'r')&gt;&gt;&gt; print f.read().decode('utf-8')安德烈&gt;&gt;&gt; f.close() 总之，在 python 2.x 中读写文件注意两点，一是从文件读取到数据之后的第一件事就是将其按照合适的编码方式译码，二是当所有操作完成需要写入文件时，一定要将要写入的字符串按照合适的编码方式编码。 python 2.x 中的 json.dumps() 操作json 作为一种广为各大平台所采用的数据交换格式，在 python 中更是被广泛使用，然而，在 python 2.x 中，有些地方需要注意。 对于数据结构中的字符串类型为 str、 但实际上定义的是一个国际字符串的情况，json.dumps() 的结果如下：12345678# python 2.7&gt;&gt;&gt; a = &#123;'Natasha': '娜塔莎'&#125;&gt;&gt;&gt; a_json_1 = json.dumps(a)&gt;&gt;&gt; a_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; a_json_2 = json.dumps(a, ensure_ascii=False)&gt;&gt;&gt; a_json_2'&#123;"Natasha": "\xe5\xa8\x9c\xe5\xa1\x94\xe8\x8e\x8e"&#125;' 可以看到，在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其会将 a 中的中文字符映射为其对应的 unicode code point 的形式，但是却是以 ASCII 字符存储的（即 &#39;\\u5a1c&#39; 对应 6 个字符而非 1 个）；当 ensure_ascii 为 False 时，json.dumps() 操作的返回值类型仍然为 str，其会将中文字符映射为其对应的某种 unicode 编码（这里为 utf-8）后的字节串，所以我们将 a_json_2 译码就可以得到我们想要的 json：12345# python 2.7&gt;&gt;&gt; a_json_2.decode('utf-8')u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print a_json_2.decode('utf-8')&#123;"Natasha": "娜塔莎"&#125; 对于数据结构中的字符串类型为 unicode 的情况，json.dumps() 的结果如下：12345678910# python 2.7&gt;&gt;&gt; u = &#123;u'Natasha': u'娜塔莎'&#125;&gt;&gt;&gt; u_json_1 = json.dumps(u)&gt;&gt;&gt; u_json_1'&#123;"Natasha": "\\u5a1c\\u5854\\u838e"&#125;'&gt;&gt;&gt; u_json_2 = json.dumps(u, ensure_ascii=False)&gt;&gt;&gt; u_json_2u'&#123;"Natasha": "\u5a1c\u5854\u838e"&#125;'&gt;&gt;&gt; print u_json_2&#123;"Natasha": "娜塔莎"&#125; 在这种情形下，当 ensure_ascii 为 True 时，json.dumps() 操作返回值的类型为 str，其得到的结果和前面对 a 操作返回的结果完全一样；而当ensure_ascii 为 False 时，json.dumps() 操作的返回值类型变为 unicode，原始数据结构中的中文字符在返回值中完整地保留了下来。 对于数据结构中的字符串类型既有 unicode 又有 str 的情形，运用 json.dumps() 时将 ensure_ascii 设为 False 的情况又会完全不同。 当数据结构中的 ASCII 字符串为 str 类型，国际字符串为 unicode 类型时（如 u = {&#39;Natasha&#39;: u&#39;娜塔莎&#39;}），json.dumps() 的返回值是正常的、符合预期的 unicode 字符串；当数据结构中有国际字符串为 str 类型，又存在其他字符串为 unicode 类型时（如 u = {u&#39;Natasha&#39;: &#39;娜塔莎&#39;} 或 u = {u&#39;娜塔莉娅&#39;: &#39;娜塔莎&#39;}），json.dumps() 会抛出异常 UnicodeDecodeError，这是因为系统会将数据结构中 str 类型字符串都转换为 unicode 类型，而系统的默认编译码方式为 ascii 编码，因而对 str 类型的国际字符串进行 ascii 译码就必然会出错。]]></content>
      <categories>
        <category>python学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>unicode</tag>
      </tags>
  </entry>
</search>
